{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a secret for you. It's something you probably already know but everyone is too afraid to say:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nobody likes writing ticket descriptions üôÑ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, you will be building your own GPT-style model that generates realistic-sounding tickets so you won't have to spend any more time writing all those long descriptions out for your hard-working TAs (you can just generate one here and paste it across!) üòÆ‚Äçüí®\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/06-DL/smarter_not_harder.jpg\" width = \"300px\">\n",
    "\n",
    "This notebook will cover both theory and practice. Really take the time to understand __why__ we're doing what we're doing rather than just implementing the challenges. \n",
    "\n",
    "üëâüëâüëâ This is the biggest challenge of the day and for good reason - we'll be solidifying all of the core concepts from the lecture, so don't stress if it takes you most of the day. üëà üëà üëà \n",
    "\n",
    "We're going to go through each step of this slowly and methodically, but the broad strokes look like this:\n",
    "\n",
    "### 1Ô∏è‚É£ Data Preprocessing üìä\n",
    "\n",
    "Read in our tickets data, clean it and split it into training, testing, and validation tensorflow datasets. üßπüîÄ\n",
    "\n",
    "### 2Ô∏è‚É£ Vocabulary Creation üìö\n",
    "\n",
    "To start, we'll be using a simplistic TextVectorization layer from TensorFlow with a custom text standardization function to turn our models into tokens. We're also going to make sure we can translate between our tokens and our original text nice and smoothly. üóÇÔ∏è\n",
    "\n",
    "### 3Ô∏è‚É£ Model Creation and Training üî®\n",
    "\n",
    "Define the architecture of the text generation model using a Transformer-based approach. This is where a lot of the heavy lifting will get done üèóÔ∏èüß†\n",
    "\n",
    "### 4Ô∏è‚É£ Text Generation üìùüîÆ\n",
    "\n",
    "Finally, we'll define a callback function to generate sample text at the end of each epoch. Let your model's creativity shine! ‚ú®üåü\n",
    "\n",
    "\n",
    "### üéâ5Ô∏è‚É£ üéâ  Freestyle \n",
    "By now you'll be a pro at NLP, so in this section, you'' have the opportunity to take the code you've written, tidy it up and and then you can start playing around with your own datasets! Get ready to explore and have some fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enough preamble! Let's get cracking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need this library a little later so for now, run the cell below to install the `keras_nlp` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1Ô∏è‚É£ Data Preprocessing üìä¬∂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to download this ```tickets.txt``` file from this [link](https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/tickets.txt) and put it into a ```data/``` folder. Then load the txt file into a variable - the variable will just be one long string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  123k  100  123k    0     0   640k      0 --:--:-- --:--:-- --:--:--  656k\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!curl https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/tickets.txt > data/tickets.txt\n",
    "with open(\"data/tickets.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 1000 characters of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm developing a sentiment analysis model for customer reviews, but I'm struggling with handling domain-specific language or sarcasm. What are some techniques like domain adaptation, transfer learning, or using pre-trained language models such as BERT or GPT-3 that can help me improve sentiment analysis performance on such challenging data? --- I'm facing challenges in detecting and handling outliers in my numerical data. How can I use techniques like the interquartile range (IQR), Z-score, or robust statistical methods to identify outliers and decide whether to remove them or treat them differently in my analysis or modeling pipeline? --- I'm working on a collaborative filtering-based recommendation system, and I need guidance on handling cold start problems when dealing with new users or items with limited interaction data. How can I leverage techniques like content-based filtering, popularity-based recommendations, or hybrid approaches to address the cold start issue? --- I'm encoun\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "text[:1000]\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ticket is broken up by some dashes. Split the text on the dashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "tickets = text.split(\" --- \")\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is to add \" EOS \" to the end of each of the strings we've created. This will let our model know that it's hit the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm developing a sentiment analysis model for customer reviews, but I'm struggling with handling domain-specific language or sarcasm. What are some techniques like domain adaptation, transfer learning, or using pre-trained language models such as BERT or GPT-3 that can help me improve sentiment analysis performance on such challenging data? EOS \""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "tickets = [sentence + \" EOS \" for sentence in tickets]\n",
    "\n",
    "tickets[0]\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many tickets you have and how long the longest ticket description (__in words - not characters__)  is. Save that maximum length as a variable `max_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "max_len = max([len(ticket.split()) for ticket in tickets])\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 372 tickets in the dataset\n",
      "The longest ticket description is 56 words (including the 'EOS' word)\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(tickets)} tickets in the dataset\")\n",
    "print(f\"The longest ticket description is {max_len} words (including the 'EOS' word)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we need to convert our nicely prepared collections of sentences into tokens. To do that we are going to use a simple [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer. \n",
    "\n",
    "We'll talk more about advanced Tokenizing techniques later but - for now - let's instantiate a tokenizer layer called `tokenize_layer` that \"standardizes\" all of our sentences to lower case (we won't worry about punctuation for the time being), outputs integers and has a maximum sentence length equal to our ```max_len```. Check the docs or docstrings for guidance on how to achieve these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# $CHALLENGIFY_BEGIN\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=\"lower\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_len,\n",
    "\n",
    ")\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've instantiated, we need to call the ```adapt()``` method of the layer to our sentences (i.e. pass in our our ```tickets``` variable into the `vectorize_layer.adapt()` function). When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a vocabulary from them. \n",
    "\n",
    "Then we can investigate our vocabulary using the [```get_vocabulary()```](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#get_vocabulary) method.\n",
    "\n",
    "Assign the list produced to a variable called `vocab` and take a look at the first 10 in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'or', 'like', 'and', \"i'm\", 'eos', 'can', 'techniques', 'to']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "vectorize_layer.adapt(tickets)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "vocab[:10]\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = \"### YOUR CODE HERE\"\n",
    "# $DELETE_BEGIN\n",
    "vocab_size = len(vocab)\n",
    "# $DELETE_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/markbotterill/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/markbotterill/code/lewagon_dev/data-solutions/06-Deep-Learning/05-Transformers/03-GPT-from-scratch/tests\n",
      "plugins: dash-2.11.1, asyncio-0.19.0, typeguard-2.13.3, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "test_vocab.py::TestVocab::test_vocab \u001b[32mPASSED\u001b[0m\u001b[32m                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/vocab.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed vocab step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('vocab',\n",
    "    vocab_size = vocab_size\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to create dictionary that will allow you to translate your tokens back to words:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_lookup = dict(zip(range(len(vocab)), vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try calling vectorizer layer on an example sentence of your choosing. (i.e. running `vectorize_layer(\"Try me\")`). Ensure you get a tensor out on the other side, filled with integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "sentence_as_tokens = vectorize_layer(\"Working with deep learning models is the best\")\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then ensure you can translate the tokens back into words with your dictionary\n",
    "<details >\n",
    "<summary>Click for hint üëá</summary>\n",
    "<br>\n",
    "The vectorizer outputs a tensor which you'll need to convert back to a list of numbers if you want to loop through them. Try using <code>.numpy().tolist()</code> on your tensor.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['working', 'with', 'deep', 'learning', 'models', 'is', 'the', 'best', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "translated_back = [index_lookup[token] for token in sentence_as_tokens.numpy().tolist()]\n",
    "translated_back\n",
    "print(translated_back)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may get some `[UNK]` tokens if you try putting in a word that isn't included in our rather small vocabularly but don't worry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you feel comfortable with the vectorizer, let's loop through all of the sentences and tokenize each one of them! Save this list as a variable ```all_tokenized``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(56,), dtype=int64, numpy=\n",
       "array([  5,  44,  12,  77,  62,  20,  11, 105, 477, 288,   5, 356,  29,\n",
       "        32, 751,  72,   2, 613,  41, 141, 196,   8,   3, 528, 804,  88,\n",
       "        93,   2,  18,  90,  72,  24, 131, 327, 440,   2, 714, 249,   7,\n",
       "        47,  46,  31,  77,  62,  92,  16, 131, 784,  63,   6,   0,   0,\n",
       "         0,   0,   0,   0])>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "all_tokenized = [vectorize_layer(sentence) for sentence in tickets]\n",
    "\n",
    "print(len(all_tokenized))\n",
    "\n",
    "all_tokenized[0]\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a ```list``` of 372 tensors with each tensor being 1-dimensional tensor that is 56 long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/markbotterill/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/markbotterill/code/lewagon_dev/data-solutions/06-Deep-Learning/05-Transformers/03-GPT-from-scratch/tests\n",
      "plugins: dash-2.11.1, asyncio-0.19.0, typeguard-2.13.3, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "test_tokenization.py::TestTokenization::test_len \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 33%]\u001b[0m\n",
      "test_tokenization.py::TestTokenization::test_seq_length \u001b[32mPASSED\u001b[0m\u001b[32m           [ 66%]\u001b[0m\n",
      "test_tokenization.py::TestTokenization::test_type \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/tokenization.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed tokenization step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('tokenization',\n",
    "    list_length = len(all_tokenized),\n",
    "    contains_tensor = tf.is_tensor(all_tokenized[0]),\n",
    "    tensor_shape = all_tokenized[0].shape[0]\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our sentences as sequences of tokens, let's think hard about exactly what our X and y are going to be. 372 sentences does not seem like a lot of information to train our model on - it isn't and this is partly so that we can have faster training times for demonstration purposes. But we can also split our sentences down into smaller X, y pairs. \n",
    "\n",
    "Our model is supposed to predict the next word in a sentence, only knowing the words that it has up until that point. Below you can see the most obvious next-word training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/gpt_scratch_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are many other sets of Xs and ys that we can get out of our sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/quick_brown_examples.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a sentence of 8 words, we have created 7 (ie. n - 1) X-y pairs! Is there a way we can think about implementing this more quickly? Let's consider replicating our sentence a total of 7 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/quick_brown_examples_X.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in just the parts underlined in red - what would be great would be if we could take a mask and ignore anything that wasn't desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/quick_brown_examples_masking.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what `tf.linalg.band()` (you saw this in the warm-up exercise!) is going to do for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's think about our ys - we could take one from each row of our tensors, but there's a more efficient way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/quick_brown_examples_y.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our ys will just be our sentence `sequence[1:]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take this dummy tensor and perform the follow operations on it:\n",
    "1) Create a tensor of shape 55x56 that is just repeats of the original sequence with ```tf.tile()```. First, you'll need to use `tf.expand_dims` here so that you can `tile` in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tensor = tf.range(0,56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Mask out the upper triangle of the tensor with ```tf.linalg.band_part()```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create our corresponding ys from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When those steps work, fill out the function that will `return X, y` (where X is a tensor of shape (55,56) and y is of shape (55,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y_creator(sequence_tensor):\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "    tiled_sequence = tf.tile(tf.expand_dims(sequence_tensor, 0), [max_len - 1, 1])\n",
    "    X_s = tf.linalg.band_part(tiled_sequence, -1, 0)\n",
    "    y_s = sequence_tensor[1:]\n",
    "    # $CHALLENGIFY_END\n",
    "    return X_s, y_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to run the cell below without error: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_y_creator(all_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/markbotterill/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/markbotterill/code/lewagon_dev/data-solutions/06-Deep-Learning/05-Transformers/03-GPT-from-scratch/tests\n",
      "plugins: dash-2.11.1, asyncio-0.19.0, typeguard-2.13.3, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "test_xy_creater.py::TestXyCreater::test_X_shape \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 33%]\u001b[0m\n",
      "test_xy_creater.py::TestXyCreater::test_list_length \u001b[32mPASSED\u001b[0m\u001b[32m               [ 66%]\u001b[0m\n",
      "test_xy_creater.py::TestXyCreater::test_type \u001b[32mPASSED\u001b[0m\u001b[32m                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 2.14s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/xy_creater.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed xy_creater step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "\n",
    "result = ChallengeResult('xy_creater',\n",
    "    list_length = len(X_y_creator(all_tokenized[0])),\n",
    "    X_shape = X_y_creator(all_tokenized[0])[0].shape,\n",
    "    y_shape = X_y_creator(all_tokenized[0])[1].shape\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this function to each element in our ```all_tokenized``` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = []\n",
    "ys = []\n",
    "for sequence in all_tokenized:\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "    X, y = X_y_creator(sequence)\n",
    "    Xs.append(X)\n",
    "    ys.append(y)\n",
    "    # $CHALLENGIFY_END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to tidy up a little - we've been doing things quite inefficiently with all of our ```for``` loops but that's so we can understand every step of the process and apply things methodically. We have a list of tensors for our `X`s and a list of `y`s for our `y`s so let's use `tf.concat` to create our 20460 training examples (that got big quickly!). Call this new variable `X` with shape `(20460, 56)` shape and do the same concat process for your ys and save it in a variable `y` `(shape (20460,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "X = tf.concat(Xs, axis = 0)\n",
    "y = tf.concat(ys, axis=0)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one thing we haven't considered yet! A lot of our y values are just 0s because there is so much padding. Let's quickly create a boolean mask to figure out where our `ys` are __not__ zero and then only keep those examples from both our X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "y != 0\n",
    "\n",
    "X = X[y != 0]\n",
    "\n",
    "y = y[y != 0]\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! We've just dropped almost 5000 useless training exampels. Check your X and y against the tests below to make sure you've ended up with the right shapes and value for your X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/markbotterill/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/markbotterill/code/lewagon_dev/data-solutions/06-Deep-Learning/05-Transformers/03-GPT-from-scratch/tests\n",
      "plugins: dash-2.11.1, asyncio-0.19.0, typeguard-2.13.3, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 5 items\n",
      "\n",
      "test_final_shapes.py::TestFinalShapes::test_X_shape \u001b[32mPASSED\u001b[0m\u001b[32m               [ 20%]\u001b[0m\n",
      "test_final_shapes.py::TestFinalShapes::test_sample_X_values \u001b[32mPASSED\u001b[0m\u001b[32m       [ 40%]\u001b[0m\n",
      "test_final_shapes.py::TestFinalShapes::test_y_shape \u001b[32mPASSED\u001b[0m\u001b[32m               [ 60%]\u001b[0m\n",
      "test_final_shapes.py::TestFinalShapes::test_y_value \u001b[32mPASSED\u001b[0m\u001b[32m               [ 80%]\u001b[0m\n",
      "test_final_shapes.py::TestFinalShapes::test_zeroes \u001b[32mPASSED\u001b[0m\u001b[32m                [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 1.98s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/final_shapes.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed final_shapes step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('final_shapes',\n",
    "    X_shape = X.shape,\n",
    "    X_value = X[500][7],\n",
    "    y_shape = y.shape,\n",
    "    y_value = y[356],\n",
    "    zeroes = tf.math.reduce_sum(tf.cast(y==0, \"int32\"))\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sorted! That was a lot of work - and you'll only have to do this once - but it's crucial you understand what is going into our model and what is being predicted from the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the tricky part - building our model! As you will recall from the lecture - GPT style models are what we call \"decoder-only\". GPT decoder-only models work by leveraging the Transformer architecture, specifically the decoder component, to generate output sequences based on input sequences. Let's take a look at this diagram that shows the architecture of GPT-2:\n",
    "\n",
    "<img src = \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/GPT2.png\" width=\"400px\">\n",
    "\n",
    "\n",
    "Focus your attention on the left side of the diagram and you can easily visualize the journey that our words (tokens) take along their path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Positional Encoding and Word Embeddings: \n",
    "\n",
    "Our input to the model is the lovely tokens we've just prepared. We now need to do two things to our input tokens:  \n",
    "\n",
    "1) Give them a regular token embedding (as we did yesterday in our NLP tasks) and also give them a positional embedding. As a reminder, a token embedding means taking a token and embedding its meaning across however many ```embedding_dimensions``` we choose. \n",
    "\n",
    "\n",
    "2) Use positional encoding to clues about where each word is in the sentence to the model, and this positional encoding is simply added to the input embeddings. \n",
    "\n",
    "This means the model understands __both__ the relative positions of the tokens in the sequence __as well as__ what each word \"means\". Fortunately, we can use a `TokenAndPositionEmbedding()` layer from the `keras_nlp` library to do both of these at once! See the diagram below for a reminder on this step from the lecture: \n",
    "\n",
    "<img src =https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/positional_encoding_sketch.png width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. The Transformer Block: \n",
    "\n",
    "Our embedded vectors now enter the Transformer block which is the __heart of the GPT architecture__. The attention mechanism here allows the model to attend to different positions in the input sequence when making predictions. The model learns the importance of each input token (now represented with its embeddings) by calculating attention weights, which reflect the token's relevance to other tokens in the sequence. \n",
    "\n",
    "This happens in a few steps - first we project our embedded vectors into Query, Key, and Value vectors. This can get a little more complex for multi-headed attention as you can see [here](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853) but once we have these matrices, we perform scaled dot-product attention by simply following this formula!\n",
    "\n",
    "<img src = \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/key-query-value.png\">\n",
    "\n",
    "Once we've done that, and we have updated our embeddings, we pass it through the final layers of the Transformer Block which just add some Dropout and LayerNormalization. \n",
    "\n",
    "The upshot of all of this is that we end up with updated vectors on the other side of our Transformer Block - each vector will still be 512 long, but they'll contain more information about their importance with respect to the task at hand (in our case predicting the next word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Making a prediction: \n",
    "\n",
    "We then need to think very carefully about what our output is going to be. \n",
    "\n",
    "Remember - our `X` is all of the sentence up to a point and our `y` is the next word. What does this mean for prediction? Well essentially we have a massive classification problem in front of us. We need to pick the next word correctly, so how many choices do we have? \n",
    "\n",
    "Answer: as many words as we have in our vocabulary! Our output will be a Dense layer with as many neurons as we have words in our vocabulary. It will have a \"softmax\" activation function - which just means that it will essentially be predicting probabilities across all of our neurons that add to one. We want the next word to have a value as close to 1 as possible.\n",
    "\n",
    "View the process below:\n",
    "\n",
    "<img src = \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/prediction_png_flow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this like for us in terms of code? Well here is our define model function with a few holes in it:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "from keras_nlp.layers import TokenAndPositionEmbedding\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_model(max_sequence_length, vocab_size, embedding_dimension):\n",
    "    \n",
    "    # 1. First up we define a layer that just grabs the inputs to our model\n",
    "    # We use the standard Input() layer and we know that each X going into\n",
    "    # our model is going to be 56 long!\n",
    "    inputs = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "\n",
    "    # 2. Next we give our tokens Positional and Regular Embeddings which is done \n",
    "    # by a nicely built layer that takes these arguments! \n",
    "    x = TokenAndPositionEmbedding(vocab_size, \n",
    "                                  max_sequence_length, \n",
    "                                  embedding_dimension, \n",
    "                                  mask_zero = True)(inputs)\n",
    "    \n",
    "    # 3. This part we are going to define in a moment - don't worry \n",
    "    # about it for now - we'll come back to it!\n",
    "    x = TransformerBlock(num_heads=4, \n",
    "                         embed_dim=embedding_dimension, \n",
    "                         ff_dim=embedding_dimension * 4)(x)\n",
    "\n",
    "    \n",
    "    # 4. This is just a regular Dropout layer that you've \n",
    "    # seen earlier in the week that helps our model avoid overfitting\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    \n",
    "    # 5. At this point in the model we'll have tensors with \n",
    "    # shape (batch_size, sequence_length, embedding_dimension) but \n",
    "    # we want to squish it down so we use GlobalAveragePooling1d. All \n",
    "    # this does is average elements across our sequence and \n",
    "    # squish them into a (batch_size, embedding_dimension) tensor. \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    \n",
    "    # 6. Finally we just need to have our \"classification\" layer \n",
    "    # which needs to be as large as our vocabulary size\n",
    "    outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "    \n",
    "    \n",
    "    # Now we just just stick our model together with the Functional API\n",
    "    # and compile with \"adam\" for our optimizer and \n",
    "    # \"sparse_categorical_crossentropy\" to compute the loss \n",
    "    # between our predicted labels and our actual labels. \n",
    "    # We'll talk about perplexity a little later.\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [33], line 17\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(max_sequence_length, vocab_size, embedding_dimension)\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m TokenAndPositionEmbedding(vocab_size, max_sequence_length, embedding_dimension, mask_zero \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)(inputs)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. This part we are going to define in a moment - don't worry \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# about it for now - we'll come back to it!\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerBlock\u001b[49m(num_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, embed_dim \u001b[38;5;241m=\u001b[39m embedding_dimension, ff_dim \u001b[38;5;241m=\u001b[39m embedding_dimension \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m)(x)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 4. This is just a regular Dropout layer that you've \u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# seen earlier in the week that helps our model avoid overfitting\u001b[39;00m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.4\u001b[39m)(x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerBlock' is not defined"
     ]
    }
   ],
   "source": [
    "create_model(50, 50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the cell above you will get an error! Why? Well, because we haven't defined many of our layers yet and we still need to implement out TransformerBlock- this is the part where all the magic happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To code our Transformer Block - we'll need to code our attention mechanism first. Here's a reminder on what that looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/key-query-value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this layer is doing is projecting our embedded inputs into three matrices which are  - queries, keys, and values - and using the interaction between all three to give us better embeddings for our words. Let's break it down step by step:\n",
    "\n",
    "1. We multiply the Q matrix with a transposed version of the K matrix.\n",
    "2. We divide this by the square root of our model dimension\n",
    "3. We take the Softmax of final dimension of the matrix the to get the scaled scores\n",
    "4. We multiply these softmax scores by the V matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coded_attention(query, key, value):\n",
    "        # Step 1: Matrix multiply the query with the transpose of the key \n",
    "        # $CHALLENGIFY_BEGIN\n",
    "\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        \n",
    "        # $CHALLENGIFY_END\n",
    "        \n",
    "        # Step 2: Divide this matrix by the square root of the hidden dimension\n",
    "        # In our case this dimension will be 512 (with the square root being 22.6). \n",
    "        # You will have to use tf.cast(22.6, tf.float32) so that the two matrices can interact \n",
    "        \n",
    "        # $CHALLENGIFY_BEGIN\n",
    "\n",
    "        divider = tf.cast(22.6, tf.float32)\n",
    "        scaled_score = score / divider\n",
    "        # $CHALLENGIFY_END\n",
    "        \n",
    "        # Step 3:\n",
    "        # Compute the softmax_scores - use tf.nn.softmax(scaled_score, axis = ?) \n",
    "        # Think about what dimension we should be using our softmax along -\n",
    "        # it'll need to be our last dimension!\n",
    "        # $CHALLENGIFY_BEGIN\n",
    "\n",
    "        softmax_scores = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        # $CHALLENGIFY_END\n",
    "        \n",
    "        # Step 4: \n",
    "        # Matrix multiply the weights matrix with the value matrix\n",
    "        # and set this to be your \"output\"\n",
    "        # $CHALLENGIFY_BEGIN\n",
    "\n",
    "        output = tf.matmul(softmax_scores, value)\n",
    "        # $CHALLENGIFY_END\n",
    "\n",
    "        \n",
    "        # Return *both* the output and the softmax_scores as a tuple\n",
    "        return output, softmax_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your function with some dummy tensors:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy tensor for query\n",
    "example_query = tf.constant([[0.1, 0.2],\n",
    "                     [0.3, 0.4]])\n",
    "\n",
    "# Dummy tensor for key\n",
    "example_key = tf.constant([[0.5, 0.6],\n",
    "                   [0.7, 0.8]])\n",
    "\n",
    "# Dummy tensor for value\n",
    "example_value = tf.constant([[0.9, 1.0],\n",
    "                     [1.1, 1.2]])\n",
    "# Test your function\n",
    "coded_attention(example_query, example_key, example_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "\n",
    "example_query = tf.constant([[0.1, 0.2],\n",
    "                     [0.3, 0.4]])\n",
    "\n",
    "\n",
    "example_key = tf.constant([[0.5, 0.6],\n",
    "                   [0.7, 0.8]])\n",
    "\n",
    "\n",
    "example_value = tf.constant([[0.9, 1.0],\n",
    "                     [1.1, 1.2]])\n",
    "output = coded_attention(example_query, example_key, example_value)\n",
    "\n",
    "result = ChallengeResult('attention',\n",
    "    len_output = len(output),\n",
    "    output_shape = output[0].shape,\n",
    "    output_value = output[0][-1]\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's run, we can fold our hand-coded attention into our larger MultiAttentionHead and the - even larger - TransformerBlock. \n",
    "\n",
    "__If you want to to go through and understand each step of the block below, do so later__, but for now you can run the cell below to define the architecture then move on down - we're almost there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Sequential\n",
    "import keras_nlp\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        # Your coded attention function goes here\n",
    "        return coded_attention(query, key, value)\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, 56, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Here, we \"project\" into long query, key, value vectors\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # We rearrange the projections for each head\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        # We perform attention on our QKV for our heads\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), \n",
    "             layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_output = self.attention(inputs)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do now is put it all together. The function below will stitch everything together for us! It's really only 5 layers (although - as we've just seen - one of them is quite complicated!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_length, vocab_size, embedding_dimension):\n",
    "       \n",
    "    inputs = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    x = TokenAndPositionEmbedding(vocab_size, \n",
    "                                  max_sequence_length, \n",
    "                                  embedding_dimension, \n",
    "                                  mask_zero = True)(inputs)\n",
    "    x = TransformerBlock(num_heads=4, \n",
    "                         embed_dim=embedding_dimension, \n",
    "                         ff_dim=embedding_dimension)(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to instantiate our model with:\n",
    "- `max_sequence_length` = Our longest sequence length\n",
    "- `vocab_size` = The number of unique words we had in our vocabulary\n",
    "- `embedding_dimension` = 512 (512 should work well for a dataset of this size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "model = create_model(56, 1150, 512)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at your model summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 56)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 56, 512)           617472    \n",
      " ng_1 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_block (Transfo  (None, 56, 512)           1577984   \n",
      " rmerBlock)                                                      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 56, 512)           0         \n",
      "                                                                 \n",
      " tf.math.reduce_mean (TFOpL  (None, 512)               0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1150)              589950    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2785406 (10.63 MB)\n",
      "Trainable params: 2785406 (10.63 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "model.summary()\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/markbotterill/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/markbotterill/code/lewagon_dev/data-solutions/06-Deep-Learning/05-Transformers/03-GPT-from-scratch/tests\n",
      "plugins: dash-2.11.1, asyncio-0.19.0, typeguard-2.13.3, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "test_model.py::TestModel::test_params \u001b[32mPASSED\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/model.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed model step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "model = create_model(56, 1150, 512)\n",
    "\n",
    "result = ChallengeResult('model',\n",
    "    params = model.count_params()\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left to do now is fit your model with our `X` and `y`. A batch size of 32 should be good and 20 epochs. Now you can just sip on some tea, read an explanation on perplexity below, and wait while our model works its magic! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before fitting, we need to expand the dims of y to make it work\n",
    "# with the perplexity measure when using the latest version of tf.\n",
    "# This changes the shape from (n_samples, ) to (n_samples, 1).\n",
    "y = tf.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "533/533 [==============================] - 49s 79ms/step - loss: 4.0605 - perplexity: 58.0046 - accuracy: 0.2233\n",
      "Epoch 2/25\n",
      "533/533 [==============================] - 19s 36ms/step - loss: 2.0654 - perplexity: 7.8887 - accuracy: 0.5263\n",
      "Epoch 3/25\n",
      "533/533 [==============================] - 17s 33ms/step - loss: 1.3556 - perplexity: 3.8791 - accuracy: 0.6600\n",
      "Epoch 4/25\n",
      "533/533 [==============================] - 17s 32ms/step - loss: 0.9811 - perplexity: 2.6675 - accuracy: 0.7410\n",
      "Epoch 5/25\n",
      "533/533 [==============================] - 17s 32ms/step - loss: 0.7819 - perplexity: 2.1855 - accuracy: 0.7890\n",
      "Epoch 6/25\n",
      "533/533 [==============================] - 17s 31ms/step - loss: 0.6348 - perplexity: 1.8866 - accuracy: 0.8228\n",
      "Epoch 7/25\n",
      "533/533 [==============================] - 17s 33ms/step - loss: 0.5317 - perplexity: 1.7018 - accuracy: 0.8461\n",
      "Epoch 8/25\n",
      "533/533 [==============================] - 17s 32ms/step - loss: 0.4622 - perplexity: 1.5876 - accuracy: 0.8660\n",
      "Epoch 9/25\n",
      "533/533 [==============================] - 17s 31ms/step - loss: 0.4075 - perplexity: 1.5031 - accuracy: 0.8806\n",
      "Epoch 10/25\n",
      "533/533 [==============================] - 17s 33ms/step - loss: 0.3816 - perplexity: 1.4646 - accuracy: 0.8859\n",
      "Epoch 11/25\n",
      "533/533 [==============================] - 16s 30ms/step - loss: 0.3681 - perplexity: 1.4450 - accuracy: 0.8898\n",
      "Epoch 12/25\n",
      "533/533 [==============================] - 17s 32ms/step - loss: 0.3318 - perplexity: 1.3935 - accuracy: 0.9006\n",
      "Epoch 13/25\n",
      "533/533 [==============================] - 17s 32ms/step - loss: 0.3050 - perplexity: 1.3566 - accuracy: 0.9073\n",
      "Epoch 14/25\n",
      "533/533 [==============================] - 17s 31ms/step - loss: 0.2900 - perplexity: 1.3364 - accuracy: 0.9138\n",
      "Epoch 15/25\n",
      "533/533 [==============================] - 17s 31ms/step - loss: 0.2546 - perplexity: 1.2900 - accuracy: 0.9228\n",
      "Epoch 16/25\n",
      "533/533 [==============================] - 17s 33ms/step - loss: 0.2586 - perplexity: 1.2951 - accuracy: 0.9201\n",
      "Epoch 17/25\n",
      "533/533 [==============================] - 16s 31ms/step - loss: 0.2769 - perplexity: 1.3190 - accuracy: 0.9135\n",
      "Epoch 18/25\n",
      "533/533 [==============================] - 17s 31ms/step - loss: 0.2584 - perplexity: 1.2948 - accuracy: 0.9200\n",
      "Epoch 19/25\n",
      "533/533 [==============================] - 16s 31ms/step - loss: 0.2309 - perplexity: 1.2597 - accuracy: 0.9274\n",
      "Epoch 20/25\n",
      "533/533 [==============================] - 17s 31ms/step - loss: 0.2538 - perplexity: 1.2889 - accuracy: 0.9241\n",
      "Epoch 21/25\n",
      "533/533 [==============================] - 16s 31ms/step - loss: 0.2094 - perplexity: 1.2330 - accuracy: 0.9327\n",
      "Epoch 22/25\n",
      "533/533 [==============================] - 17s 32ms/step - loss: 0.2254 - perplexity: 1.2529 - accuracy: 0.9317\n",
      "Epoch 23/25\n",
      "533/533 [==============================] - 17s 32ms/step - loss: 0.2328 - perplexity: 1.2622 - accuracy: 0.9253\n",
      "Epoch 24/25\n",
      "533/533 [==============================] - 17s 31ms/step - loss: 0.1904 - perplexity: 1.2097 - accuracy: 0.9394\n",
      "Epoch 25/25\n",
      "533/533 [==============================] - 17s 31ms/step - loss: 0.1949 - perplexity: 1.2152 - accuracy: 0.9366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d5c23bb0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "model.fit(X, y, batch_size=32, epochs = 25)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> \n",
    "    <summary> Click HERE for a Perplexity Explanation</summary>\n",
    "<br>\n",
    "Perplexity is a metric commonly used in the context of Language Models to evaluate the quality and performance of the model in predicting the next word or token in a sequence of words. It measures how well the language model assigns probabilities to a given sequence of words.\n",
    "\n",
    "Mathematically, perplexity is calculated using the concept of cross-entropy. The perplexity score is the exponential of the average cross-entropy per word in a given dataset. The formula for perplexity is as follows:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = \\exp\\left(-\\frac{{\\sum_{{i=1}}^{{N}} \\log(p(w_i))}}{{N}}\\right)\n",
    "$$\n",
    "\n",
    "where $N$ represents the total number of words in the dataset, and $(p(w_i))$ is the probability assigned by the language model to the $(i)$-th word in the sequence.\n",
    "\n",
    "The formula involves taking the logarithm of the model's predicted probabilities for each word in the sequence and summing them. Dividing this sum by the total number of words $(N)$ and then taking the exponential gives the perplexity score.\n",
    "\n",
    "A lower perplexity value indicates that the language model is more confident and accurate in its predictions, as it assigns higher probabilities to the true words in the dataset. Conversely, a higher perplexity score suggests that the model is more uncertain and less accurate in its predictions.\n",
    "\n",
    "Perplexity is often used to compare different language models or to track the progress of a model during training. Lower perplexity values are generally desired, indicating better language modeling performance.</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll remember that GPT-style models work simply by taking an input sequence, predicting the next word, adding it to our existing input sequence and then predicting again and again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to write a generate function that takes as its input the starter_string. \n",
    "\n",
    "Then - in a for loop of ``range(max_len - len(starter_string.split()))``:\n",
    "\n",
    "1. We use our ```vectorize_layer``` to convert it to a ```token_tensor```\n",
    "2. Use ```model.predict(token_tensor)``` to get out a tensor of size vocab_size out of our model (you'll need to use ```tf.expand_dims(token_tensor, 0)``` so that it looks like the right input size for our model!\n",
    "3. Use ```tf.argmax()``` to find the index of the most probable (largest number) word out (this is effectively a \"greedy\" algorithm as we just take the most likely word at any given step.\n",
    "4. Use our ```index_lookup``` dictionary from earlier to convert that index from a number back into a word\n",
    "5. Add that word to our input string and repeat UNLESS\n",
    "\n",
    "Two catches: \n",
    "\n",
    "1) If we predict the word \"eos\" that means our model has predicted the end of the sentence so we ```return``` our ```starter_string``` in its current form. \n",
    "\n",
    "2) If the length of our string (when split on whitespace) is 55 then we also ```return``` our ```starter_string``` in its current form. \n",
    "\n",
    "N.B. Make sure you add whitespace when you add the word to your sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(starter_string):\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "\n",
    "    for x in range(max_len - len(starter_string.split())):\n",
    "        tokens = vectorize_layer(starter_string)\n",
    "        token_expanded = tf.expand_dims(tokens, 0)\n",
    "        pred = model.predict(token_expanded)\n",
    "        index_pred = pred.argmax()\n",
    "        word = index_lookup[index_pred]\n",
    "        print(word)\n",
    "        if word == \"eos\":\n",
    "            return starter_string\n",
    "        if len(starter_string.split())==55:\n",
    "            return starter_string\n",
    "        else:\n",
    "            starter_string += f\" {word} \"\n",
    "    # $CHALLENGIFY_END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "to\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "detect\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "anomalies\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "or\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "outliers\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "in\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "my\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "irregular\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "time\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "series\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "data\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "with\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "prediction\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "or\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "credit\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "data.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "what\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "are\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "some\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "techniques\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "like\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "domain\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "adaptation,\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "transfer\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "learning,\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "or\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "using\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "pre-trained\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "language\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "models\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "such\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "as\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "bert\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "or\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "gpt-3\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "that\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "can\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "help\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "me\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "improve\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "sentiment\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "analysis\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "performance\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "on\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "such\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "challenging\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "data?\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "eos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I need to  detect  anomalies  or  outliers  in  my  irregular  time  series  data  with  prediction  or  credit  data.  what  are  some  techniques  like  domain  adaptation,  transfer  learning,  or  using  pre-trained  language  models  such  as  bert  or  gpt-3  that  can  help  me  improve  sentiment  analysis  performance  on  such  challenging  data? '"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"I need\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!!! You've just built your own GPT model from first principles üí™\n",
    "\n",
    "Naturally there are __much__ more efficient ways to do what we've just done - you will almost never end up writing your own Transformer block, attention mechanism or even full GPT model from scratch. HuggingFace abstracts so much of the difficult coding away from us, which is why fine-tuning existing models is much more effective. \n",
    "\n",
    "The dataset we've been working with has been very, very small and we have done very simplistic tokenization too (each word is currently assigned its own unique token and punctuation is kept in as well, but if you'd like to try working with some more messy data, you can tidy up your code into Python files and then repeat your steps on the real data found in these 10000 StackOverflow answers [here](https://wagon-public-datasets.s3.amazonaws.com/answers.csv) and build a more complex generator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
