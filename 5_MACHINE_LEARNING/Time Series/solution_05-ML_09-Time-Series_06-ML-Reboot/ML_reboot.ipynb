{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Machine Learning Reboot Challenge 🚀\n",
    "Get ready to apply your knowledge in a hands-on experience with our Airbnb data.\n",
    "\n",
    "We'll work through:\n",
    "\n",
    "- 🧹 Data Cleaning\n",
    "- 🚦 Train-Test Split\n",
    "- 📈 Linear Regression\n",
    "- 🌐 Random Forest Regressor\n",
    "- 🔁 Cross-Validation\n",
    "- 🎯 Logistic Regression\n",
    "- 🎯 K-Means Clustering\n",
    "\n",
    "At the end of each task, you'll answer questions 📝 to test your understanding.\n",
    "\n",
    "Let's dive in and bring these concepts to life! 🏊‍♀️🏊‍♂️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Understand 👏 The 👏 Data 👏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, we're going to be working through some real life AirBnb data. We'll be using data from multiple cities, so we're going to focus on making sure our code is nice and reusable (that way we don't have to write out all of our steps for data manipulation each time we work with another csv!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up: [Asheville, North Carolina](https://wagon-public-datasets.s3.amazonaws.com/data-science-images/05-ML/Reboot-2/asheville_airbnb.csv). Download the csv from the link provided. We're going to be doing a linear regression, trying to predict the `price` of an AirBnb given all the other info we have about it! Load the DataFrame and take a look at the distribution of the dependent variable. Also check out its minimum and maximum values. \n",
    "\n",
    "N.B. Practice reading in your data cleanly and take care to set an `index_col` when you load up your DataFrame. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data\n",
    "! curl \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/05-ML/Reboot-2/asheville_airbnb.csv\" > \"data/asheville_airbnb.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/asheville_airbnb.csv\", index_col = 0)\n",
    "\n",
    "# $CHALLENGIFY_BEGIN\n",
    "df[\"price\"] = df[\"price\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "df[\"price\"].min()\n",
    "\n",
    "df[\"price\"].max()\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks quite skewed! We're going to focus on AirBnb listings priced above 50 dollars and less than 1500 dollars. Create a DataFrame named `reduced` that reflects this change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "reduced = df[(df[\"price\"] > 50) & (df[\"price\"] < 1500)].copy()\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the asserts throughout the notebook to make sure you're on the right track!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(reduced.shape == (2746, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at all of our columns, pick out only ones that you think might help us with our linear regression task (along with our price column!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "reduced.columns\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only once you've done some investigation yourself,<details>\n",
    "<summary>click here for a hint 👆</summary>\n",
    "\n",
    "\n",
    "\n",
    "Here we've compiled a list for you that should serve as a good starting point, but you're welcome to pick some of your own.\n",
    "\n",
    "```\n",
    "    interesting_cols = [\n",
    "    'price',\n",
    "    'room_type',\n",
    "    'accommodates',\n",
    "    'bathrooms_text',\n",
    "    'bedrooms',\n",
    "    'beds',\n",
    "    'minimum_nights',\n",
    "    'number_of_reviews',\n",
    "    'review_scores_rating',\n",
    "    'instant_bookable']\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "\n",
    "interesting_cols = [\n",
    "    'price',\n",
    "    'room_type',\n",
    "    'accommodates',\n",
    "    'bathrooms_text',\n",
    "    'bedrooms',\n",
    "    'beds',\n",
    "    'minimum_nights',\n",
    "    'number_of_reviews',\n",
    "    'review_scores_rating',\n",
    "    'instant_bookable'\n",
    "]\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "\n",
    "relevant = reduced[interesting_cols].copy()\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1:__\n",
    "What are the primary differences between feature selection and feature engineering in the context of data preprocessing for machine learning models?\n",
    "\n",
    " A) Feature selection involves creating new features from existing ones, while feature engineering involves selecting the most relevant features. <br>\n",
    " B) Feature selection involves transforming numerical features into categorical ones, while feature engineering involves encoding categorical features.<br>\n",
    "C) Feature selection focuses on reducing the dimensionality of the dataset by choosing only relevant features, while feature engineering involves creating new features from existing ones.<br>\n",
    "D) Feature selection is only applicable to linear models, while feature engineering is used for non-linear models.\n",
    "\n",
    "Save your answer as a string (either \"A\", \"B\", \"C\" or \"D\") in the variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_1 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ What would be a __useful__ example of feature engineering for this data?\n",
    "\n",
    "A) Adding together `bedrooms` and `beds` to make a combined `all_bed_info` columns <br>\n",
    "B) Using the `latitude` and `longitude` points for each listing to create a `distance_from_downtown` feature<br>\n",
    "C) Using `price` divded by `bedrooms` to create a new `price_per_room` feature<br>\n",
    "D) Converting our `review_scores_rating` into a categorical variable\n",
    "\n",
    "Save your answer as a string (either \"A\", \"B\", \"C\" or \"D\") in the variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_2 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your null and missing values. Proportionally speaking, how much of the dataset do they represent? Do we have a solid imputation strategy or can we just drop them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "relevant.isna().sum()\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your decision and proceed to the next test cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "relevant.dropna(inplace = True)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cell\n",
    "assert(relevant.shape == (2409, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to take everything we have here and ensure that it's ready to be passed to our model. That means it has to be expressed as a number! So let's extract information from our `string` columns (`instant_bookable` and `bathrooms_text` we're looking at you 👀 - we may need some `regex` here to help us) and One Hot Encode our `room_type` (`pd.get_dummies()` is a very useful function for helping us do this). We will provide you with the cleaning function here but you will have to apply it to the DataFrame yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_number(text):\n",
    "    if text and type(text)==str:\n",
    "        match = re.search(r'\\d+(\\.\\d+)?', text)\n",
    "        return float(match.group()) if match else None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "relevant['bathrooms_text'] = relevant['bathrooms_text'].apply(extract_number)\n",
    "\n",
    "relevant['instant_bookable'] = relevant['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "relevant = pd.get_dummies(relevant, columns=['room_type'])\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the next test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(\"object\" not in list(relevant.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3:__ What does the regular expression re.search(r'\\d+(\\.\\d+)?', text) in the cell above do when applied to a given text?\n",
    "\n",
    "A) It matches any sequence of digits. <br>\n",
    "B) It matches any floating-point number in the text.<br>\n",
    "C) It matches any integer or decimal number in the text.<br>\n",
    "D) It matches any sequence of characters that starts with a digit.<br>\n",
    "\n",
    "Save your answer as a string (either \"A\", \"B\", \"C\" or \"D\") in the variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_3 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, all sorted! You've done all of your cleaning steps! Before we proceed, we're going to wrap up everything we've just done into one cleaning function. Why? Because it'll make working with other datasets so much easier! Down the line. Copy and paste your code chunks from the cells above into one large function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cleaner(df):\n",
    "    # Converts a messy DataFrame into one\n",
    "    # that contains only the relevant columns\n",
    "    # with no null values and only numerical data.\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "def df_cleaner(df):\n",
    "    \n",
    "    copy = df.copy()\n",
    "    \n",
    "    copy[\"price\"] = copy[\"price\"].str.replace(\"$\", \"\", regex = True).str.replace(\",\", \"\", regex = True).astype(float)\n",
    "    \n",
    "    reduced = copy[(copy[\"price\"] > 50) & (copy[\"price\"] < 1500)].copy()\n",
    "    \n",
    "    relevant = reduced[interesting_cols].copy()\n",
    "    \n",
    "    relevant['bathrooms_text'] = relevant['bathrooms_text'].apply(extract_number)\n",
    "\n",
    "    relevant['instant_bookable'] = relevant['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "    relevant = pd.get_dummies(relevant, columns=['room_type'])\n",
    "    \n",
    "    clean_df = relevant.dropna()\n",
    "    \n",
    "    return clean_df\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've coded your function, run the cell below to test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"data/asheville_airbnb.csv\", index_col = 0)\n",
    "df_cleaner(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this test cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('data/asheville_airbnb.csv')\n",
    "assert(df_cleaner(new_df).shape == (2408, 13))\n",
    "assert(\"object\" not in df_cleaner(new_df).dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model, we need to create our X and y then split up our data with a train test split! We'll do an `80/20` split with a random state of `42`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df_cleaner(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "X = clean_df.drop(\"price\", axis = 1)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "y = clean_df[\"price\"]\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# $CHALLENGIFY_BEGIN\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__: What is the primary purpose of performing a train-test split in data science when building a machine learning model?\n",
    "\n",
    "A) To divide the dataset into multiple subsets for parallel processing.<br>\n",
    "B) To divide the dataset into two parts: one for training the model and one for testing its performance.<br>\n",
    "C) To merge two datasets for increased model accuracy.<br>\n",
    "D) To ensure the model has access to the entire dataset during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_4 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to scale our `X_train` - to keep it simple let's use MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5:__ What is the purpose of applying Min-Max Scaler to features in data preprocessing for machine learning?\n",
    "\n",
    "A) To eliminate outliers from the dataset.<br>\n",
    "B) To reduce the dimensionality of the data.<br>\n",
    "C) To standardize the features to have a mean of 0 and a standard deviation of 1.<br>\n",
    "D) To scale the features to a specific range, usually between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_5 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6:__ Perhaps more importantly - __why__ do we scale our data in ML?\n",
    "\n",
    "A) To reduce the number of features in the dataset for faster processing.<br>\n",
    "B) To ensure that the model always converges to the global optimum.<br>\n",
    "C) To avoid numerical instability and speed up the optimization process.<br>\n",
    "D) To eliminate outliers and anomalies from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_6 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to model! Fit a simple Linear Regression model from `sklearn` to your training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up - calculate our baseline Mean Squared Error for a linear regression model? Think through what our simplest possible guess will be. Then calculate the MSE of guessing that every time for our test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 7:__ What is the simplest and most common baseline guess for a machine learning regression model?\n",
    "\n",
    "\n",
    "A) Median of the target variable<br>\n",
    "B) Maximum value of the target variable<br>\n",
    "C) Mean of the target variable<br>\n",
    "D) Minimum value of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_7 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "price_mean = y_train.mean()\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "baseline = ((y_test - price_mean) ** 2).mean()\n",
    "baseline\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(round(baseline) == 16467)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instantiate a Linear Regression model from `sklearn` and fit it to your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_scaled, y_train)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on your test set and __DO NOT FORGET TO SCALE YOUR X_TEST!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "y_pred = linear_model.predict(X_test_scaled)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the MSE between your predictions (use `sklearn.metrics` to expedite things) and your real answers. Assign it to the variable `mse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "mse\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(mse < 9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the R-squared of your model prediction? Save it in a variable `r_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "r_2 = linear_model.score(X_test_scaled, y_test)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(r_2 > 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8:__ What does the coefficient of determination, R-squared, measure in the context of regression models?\n",
    "\n",
    "A) The percentage of variance in the dependent variable explained by the independent variable/s.<br>\n",
    "B) The percentage of variance in the independent variable explained by the dependent variable/s.<br>\n",
    "C) The percentage of correct predictions made by the regression model.<br>\n",
    "D) The percentage of outliers in the dataset that affect the regression model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_8 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Random Forest Regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad first attempt - let's try quickly implementing a different model - a RandomForestRegressor - to see if we get better results. Instantiate a vanilla (no hyperparam tuning) RandomForestRegressor with a `random state` of 42. Evaluate your model in the same way as before with `rf_mse` and `rf_r_2` variables storing your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "rf_preds = rf.predict(X_test_scaled)\n",
    "\n",
    "rf_mse = mean_squared_error(y_test, rf_preds)\n",
    "\n",
    "rf_r_2 = linear_model.score(X_test_scaled, y_test)\n",
    "# $CHALLENGIFY_END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rf_mse < 8000\n",
    "assert rf_r_2 > 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 9:__ How does a Random Forest algorithm work in simple terms?\n",
    "\n",
    "A) It creates multiple decision trees and combines their predictions to make more accurate and robust predictions.<br>\n",
    "B) It randomly selects features from the dataset to build a single powerful decision tree.<br>\n",
    "C) It uses a random process to shuffle the data and find the best fit for the target variable.<br>\n",
    "D) It relies on the randomness of the data to make predictions without using any decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_9 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://wagon-public-datasets.s3.amazonaws.com/data-science-images/05-ML/Reboot-2/new_york.csv) is the dataset for New York's listings. Use your cleaning function to preprocess it, then see how the two different models perform against each other and against a new NYC baseline MSE! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://wagon-public-datasets.s3.amazonaws.com/data-science-images/05-ML/Reboot-2/new_york.csv > data/new_york.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to use cross validation to see which one performs better first. We're going to try our `LinearRegression`, our `RandomForestRegressor` and also a `KNNRegressor`.\n",
    "\n",
    "Remember each of the steps - this should be muscle memory now:\n",
    "\n",
    "- Load the DataFrame\n",
    "- Clean it (using the function we wrote above)\n",
    "- Create the X and y\n",
    "- Train test split (use `random_state = 42` for this notebook)\n",
    "- Calculate baseline MSE\n",
    "- Scale the X_train\n",
    "- Cross validate each model, scoring with \"mse\"\n",
    "- Fit the best model on the train for real\n",
    "- Predict on the test\n",
    "- Calculate the test MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (don't forget index_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_df = pd.read_csv(\"data/new_york.csv\", index_col = 0)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "clean_ny = df_cleaner(ny_df)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_X = clean_ny.drop(\"price\", axis = 1)\n",
    "ny_y = clean_ny[\"price\"]\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split (with random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_X_train, ny_X_test, ny_y_train, ny_y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_mean = ny_y_train.mean()\n",
    "ny_baseline = ((ny_y_test - ny_mean)**2).mean()\n",
    "ny_baseline\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_scaler = MinMaxScaler()\n",
    "ny_X_scaled = ny_scaler.fit_transform(ny_X_train)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cross validation models for all three models (remember random_state of 42 for Random Forest)\n",
    "# You will also have to think carefully about how you get an \"mse\" out of your cross validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_linear_model = LinearRegression()\n",
    "ny_knn_regressor = KNeighborsRegressor()\n",
    "ny_rf_model = RandomForestRegressor(random_state = 42)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "from sklearn.model_selection import cross_val_score\n",
    "ny_linear_score = cross_val_score(ny_linear_model, ny_X_scaled, ny_y_train, cv = 5, scoring = \"neg_mean_squared_error\")\n",
    "ny_neighbor_score = cross_val_score(ny_knn_regressor, ny_X_scaled, ny_y_train, cv = 5, scoring = \"neg_mean_squared_error\")\n",
    "ny_rf_score = cross_val_score(ny_rf_model, ny_X_scaled, ny_y_train, cv = 5, scoring = \"neg_mean_squared_error\")\n",
    "# $CHALLENGIFY_END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ny_linear_score.mean(), ny_neighbor_score.mean(), ny_rf_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 10:__ Why do we perform cross-validation in machine learning?\n",
    "\n",
    "A) To evaluate the model's performance on the training data.<br>\n",
    "B) To estimate the model's performance on unseen data and assess its generalization ability.<br>\n",
    "C) To increase the size of the training data for better model training.<br>\n",
    "D) To reduce overfitting and prevent the model from memorizing the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_10 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our best model for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_rf_model.fit(ny_X_scaled, ny_y_train)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on scaled X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_X_test_scaled = ny_scaler.transform(ny_X_test)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_rf_pred = ny_rf_model.predict(ny_X_test_scaled)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_rf_mse = mean_squared_error(ny_y_test, ny_rf_pred)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "ny_rf_mse\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we've been doing such a good job at AirBnB that an airline company has heard of our data science talents 👀\n",
    "\n",
    "They have assigned us a task! Use [this data](https://wagon-public-datasets.s3.amazonaws.com/data-science-images/05-ML/Reboot-2/Invistico_Airline.csv) to figure out what is making their customers satisfied or not given a whole host of other features. They have given us their dataset and told us the goal is simple:\n",
    "\n",
    "- Show which features are having the largest impact on customer satisfaction\n",
    "\n",
    "The only other clue they have given us is there are some null values in our `'Arrival Delay in Minutes'` column and that they would like us to fill that column with the `median` for our `.fillna()` strategy. The rest is up to you now, you'll have to do all of the feature preprocessing and transformation yourself and fit your own Logistic Regression (from `sklearn` model). Then you'll answer a few questions on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember your process for exploreing your data:\n",
    "1) Take the time to investigate your target <br>\n",
    "2) See if there is a class imbalance<br>\n",
    "3) Check your independent variables <br>\n",
    "4) Make sure everything is nicely scaled (just using your training data split)<br>\n",
    "5) Fit on the train<br>\n",
    "6) Predict on the __scaled__ test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/05-ML/Reboot-2/Invistico_Airline.csv\" > data/Invistico_Airline.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "airline_df = pd.read_csv(\"data/Invistico_Airline.csv\")\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you've given the data a first look - fill missing [\"Arrival \n",
    "# Delay in Mins\"] values with the median value as we have been instructed\n",
    "# $CHALLENGIFY_BEGIN\n",
    "airline_df['Arrival Delay in Minutes'].fillna(airline_df['Arrival Delay in Minutes'].median(), inplace=True)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our target \"Satisfaction\" is a 0 or 1, not a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "airline_df[\"satisfaction\"] = (airline_df[\"satisfaction\"] == \"satisfied\").astype(int)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables into dummy/indicator \n",
    "# variables (i.e., one-hot encoding w/ pd.get_dummies())\n",
    "# $CHALLENGIFY_BEGIN\n",
    "airline_df = pd.get_dummies(airline_df, drop_first=True)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "X, y = airline_df.drop(\"satisfaction\", axis = 1), airline_df[\"satisfaction\"]\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split (use random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale your X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logitistic regression model on your train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "log_model = LogisticRegression(max_iter = 1000)\n",
    "log_model.fit(X_train_scaled, y_train)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale your X_test and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "log_preds = log_model.predict(X_test_scaled)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two variables - accuracy and conf_matrix - for your accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# $CHALLENGIFY_BEGIN\n",
    "accuracy = accuracy_score(y_test, log_preds)\n",
    "conf_matrix = confusion_matrix(y_test, log_preds)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test to make sure you're on the right track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(accuracy > 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot your confusion matrix visually with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Satisfied', 'Satisfied'], \n",
    "            yticklabels=['Not Satisfied', 'Satisfied'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate your `precision` and `recall` by hand and save them as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "precision, recall = 0.8531885073580939, 0.851339067198098\n",
    "\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(round(precision, 2) == 0.85)\n",
    "assert(round(recall, 2) == 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 11:__ In binary classification, what is the main difference between precision and recall?\n",
    "\n",
    "A) Precision measures the ability of a model to correctly identify positive instances, while recall measures the ability to correctly identify negative instances.<br>\n",
    "B) Precision measures the ability of a model to correctly identify negative instances, while recall measures the ability to correctly identify positive instances.<br>\n",
    "C) Precision measures the overall accuracy of the model, while recall measures the model's ability to handle imbalanced datasets.<br>\n",
    "D) Precision is the ratio of true positives to the sum of true positives and false positives, while recall is the ratio of true positives to the sum of true positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_11 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to look at your model's coefficients - run the code below to make a DataFrame that shows the coefficient for each feature if you'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "coefficients = log_model.coef_[0]\n",
    "column_names = X.columns\n",
    "coef_df = pd.DataFrame({'Feature': column_names, 'Coefficient': coefficients})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "coef_df.sort_values(by = \"Coefficient\")\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 12:__ Based on the provided DataFrame showing the coefficients of our model, what can we conclude about the impact of a __one minute increase__ in the \"Arrival Delay in Minutes\" feature on the model's results? Think carefully about all the steps you have taken to produce your model.\n",
    "\n",
    "A) One minute increase in \"Arrival Delay in Minutes\" has a considerable impact on the model's predictions.<br>\n",
    "B) One minute increase in \"Arrival Delay in Minutes\" has no impact on the model's predictions.<br>\n",
    "C) The impact of a one-minute increase in the Arrival Delay in Minutes feature on the model's results probably has a fairly small impact given that the shown coefficient was calculated based on our MinMax scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_12 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 13:__ In the provided DataFrame showing the coefficients of a logistic regression model, which of the following features appears to have the most significant impact on the model's predictions?\n",
    "\n",
    "A) Inflight wifi <br>\n",
    "B) Inflight entertainment<br>\n",
    "C) Cleanliness<br>\n",
    "D) On-board service\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_13 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) K-Means Clustering: Optimal number of NY Boroughs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame that only include the `latitude` and `longitude` columsn from the above NY AirBnB dataset. We are going to do a little unsupervised learning to see if we can replicate the New York city zone limits from our own data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/05-ML/Reboot-2/shutterstock-152208935.webp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have 5 key boroughs in New York City: Staten Island, Brooklyn, Queesion, the Bronx, and Manhattan! Is this the optimal number of boroughs? We're going to use the placement of our apartments listed on AirBnB to see if an unsupervised learning method will replicate the same kinds of boundaries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `locations` - a DataFrame with only the lat and longs from our AirBnB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "locations = ny_df[[\"latitude\", \"longitude\"]].copy()\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import KMeans and set your number of cluster to 5 then fit it on `locations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "from sklearn.cluster import KMeans\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "k_model = KMeans(n_clusters = 5)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "k_model.fit(locations)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your labels to your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "locations[\"labels\"] = k_model.labels_ \n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot your results with a different colour for each neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "sns.scatterplot(data = locations, x =\"longitude\", y = \"latitude\", hue = \"labels\")\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we approximate similar boundaries for the boroughs of New York from our sampled data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some different `n_neighbours` and use the elbow method to see if you find an optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 14:__ How does the K-means algorithm work in layman's terms?\n",
    "\n",
    "A) K-means algorithm finds the mean of all data points and groups them based on their distance to this mean.<br>\n",
    "B) K-means algorithm calculates the distance between each data point and its nearest neighbor to create clusters.<br>\n",
    "C) K-means algorithm randomly selects K data points as cluster centers, then assigns each data point to the nearest center and recalculates the center's position based on the data points in that cluster.<br>\n",
    "D) K-means algorithm sorts the data points in ascending order and assigns the first K points to the same cluster, then continues with the next K points until all data points are grouped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_14 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 15:__ K Means is an example of an unsupervised learning technique where we start out with no labels for our data. Which of the following is an example of unsupervised learning technique?\n",
    "\n",
    "A) Principal Component Analysis (PCA), a technique used for feature reduction and data dimensionality reduction.<br>\n",
    "B) Decision Tree, a model used for classification and regression tasks with labeled data.<br>\n",
    "C) Support Vector Machine (SVM), a model used for binary classification with labeled data.<br>\n",
    "D) Random Forest, an ensemble model combining multiple decision trees for classification and regression with labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_15 = \"Save answer letter here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST YOUR ANSWERS\n",
    "\n",
    "Make sure you have saved all of your answers as uppercase \"A\", \"B\", \"C\" or \"D\" and then run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $CHALLENGIFY_DELETE\n",
    "# Define correct answers as variables\n",
    "answer_1 = \"C\"  \n",
    "answer_2 = \"B\"  \n",
    "answer_3 = \"B\"  \n",
    "answer_4 = \"B\"  \n",
    "answer_5 = \"D\"  \n",
    "answer_6 = \"C\"  \n",
    "answer_7 = \"C\"  \n",
    "answer_8 = \"A\"  \n",
    "answer_9 = \"A\"  \n",
    "answer_10 = \"B\" \n",
    "answer_11 = \"A\" \n",
    "answer_12 = \"C\" \n",
    "answer_13 = \"B\" \n",
    "answer_14 = \"C\" \n",
    "answer_15 = \"A\" \n",
    "\n",
    "\n",
    "# $CHALLENGIFY_DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_answers import check_answers\n",
    "\n",
    "student_answers = {\n",
    "    \"question_1\": answer_1,  \n",
    "    \"question_2\": answer_2,  \n",
    "    \"question_3\": answer_3,  \n",
    "    \"question_4\": answer_4,  \n",
    "    \"question_5\": answer_5,  \n",
    "    \"question_6\": answer_6,  \n",
    "    \"question_7\": answer_7,  \n",
    "    \"question_8\": answer_8,  \n",
    "    \"question_9\": answer_9,  \n",
    "    \"question_10\": answer_10,\n",
    "    \"question_11\": answer_11,\n",
    "    \"question_12\": answer_12,\n",
    "    \"question_13\": answer_13,\n",
    "    \"question_14\": answer_14,\n",
    "    \"question_15\": answer_15,\n",
    "}\n",
    "\n",
    "check_answers(student_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
