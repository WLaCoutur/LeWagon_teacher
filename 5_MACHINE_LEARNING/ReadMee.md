Machine Learning
-
10 lectures
Welcome to the Machine Learning module! ü§ñ
ML Fundamentals
First things first, you will learn what we mean by Machine Learning, its difference from regular programming, and the two sets of tasks that you‚Äôll have to tackle as a data scientist - regression and classification.

We‚Äôll then introduce you to Scikit-learn, probably the most popular Machine Learning library for Python, and your best friend during the ML module.

You will then dive into ML‚Äôs fundamental concepts and will be introduced to the key steps of the implementation of ML algorithms. We want our models to be able to generalize on unseen data flawlessly, and we‚Äôll need specific techniques that will allow us to not overfit and validate the performance of our model.

Following those fundamental guidelines and techniques, you will train your first Machine Learning models with Scikit-learn: linear regression and logistic regression.

Data Preparation
This module is dedicated to the preprocessing of your dataset - how to have a clean, balanced dataset that is representative of your problem. You will discover how to deal with missing values, scale your features and encode your data into vectorized forms that can be used later in your models.

You will realize that the quality of your preprocessing will affect the performance of your models and that its importance should not be underestimated.

We‚Äôll also show you how to enrich your dataset via feature engineering, and how to evaluate the contribution of individual features to the performance of a model as a way to get rid of the ‚Äúnoise‚Äù.

Performance Metrics
This module is split into two sections, Regression and Classification metrics, which are different by nature. We will see here how to evaluate the performance of our models precisely, and how to choose the error metric appropriate to the task.

In an applied setting, knowing what to optimize is paramount to correctly covering business use cases and evaluating the feasibility and utility of the use of Machine Learning technologies. Metrics like precision, recall & f1 score, are very important to use to determine business impact.

Finally, we‚Äôll introduce you to your first distance-based algorithm, the K-nearest neighbors, a versatile model capable of solving both Classification and Regression tasks.

Under the Hood
Now that we have covered the fundamentals of Machine learning, let‚Äôs get a bit more theoretical. This module is dedicated to understanding the learning mechanism of algorithms, which consists of a solver minimizing a loss function.

We will cover the famous Gradient Descent, an iterative optimization solver, in depth, and will introduce other types of solvers. We will then dive into the different loss functions, their specificities, and how they influence the learning process of algorithms.

Understanding the optimization process will give you more control over the design of your models to have the best performance possible for the specific problem you want to solve.

Model Tuning
To conclude the first half of the Machine Learning module, you will learn how to fine-tune your models and push them to their limits.

First, you‚Äôll be introduced to the concept of regularization, a technique used to combat overfitting. We‚Äôll then speak about Grid and Random Search, two model hyperparameter optimization techniques.

Finally, we will see another powerful supervised learning method called SVM (Support Vector Machines).

Workflow
This module is dedicated to pipelines, the transition from notebooks to production.

A pipeline is a chain of operations of a Machine Learning project (preprocessing, training, predicting, etc.) combined into a single object.

Pipelines allow you to structure your code, make your workflow easy to read and understand, enforce the implementation and order of steps in your ML project, and make your work reproducible and deployable.

Ensemble Methods
Today, we get the final family of supervised learning models: ensemble methods. Ensemble methods consist of combining multiple weak models into a single powerful one via techniques called bagging, boosting, and stacking.

We will start by introducing the Decision Tree, a simple ML algorithm, and will show you how it can be enhanced by bagging and boosting.

You will also get to know the famous Gradient Boosting algorithm, one of the most powerful models out there, often winning Kaggle-type competitions.

Unsupervised Learning
Let‚Äôs dive into the other dimension of ML, unsupervised learning. We will cover algorithms that are used for exploratory analysis, dimensionality reduction, and other compression-type tasks.

We‚Äôll see the Principal Component Analysis (PCA), a powerful dimensionality reduction tool that will help you to deal with very large datasets.

We will then dive into the K-means algorithm, a clustering method used to discover coherent sub-groups within a dataset without supervision.

This day is also an opportunity to discover big Machine Learning applications through building a recommender system and an image compression program.

Time Series
Time to dive into one of the specific domains of Machine Learning: Time Series.

First, you will be introduced to the fundamental concepts of Time series: decomposition, stationarity, and autocorrelation.

We will then show you three state-of-the-art Machine Learning models: ARMA, ARIMA, and SARIMAX.

NLP
This day is dedicated to one of the most common types of data out there: text.

The first part of the lecture is dedicated to text preprocessing and representation techniques. By nature, text data is different from numerical data, and needs to be preprocessed and represented in a specific way for algorithms to be able to interpret it.

You will discover the NLTK, a popular NLP library with powerful tools that facilitate the crunching of text data.

We will then break down the Naive Bayes algorithm, a probabilistic model that has proved successful for text classification tasks such as authorship attribution.

Finally, we will also tackle word embeddings and document clustering with the LDA algorithm, which is used to explore large corpora of unstructured text documents.