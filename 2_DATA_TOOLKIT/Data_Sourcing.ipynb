{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Sourcing : De Débutant à Pro 🚀",
   "id": "afe6e8b7be2c1857"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Table des matières",
   "id": "e7c198fe78771564"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. [Setup et imports](#setup)\n",
    "2. [Partie 1 : Maîtriser les fichiers CSV](#csv)\n",
    "3. [Partie 2 : APIs - Collecter des données depuis le web](#api)\n",
    "4. [Partie 3 : Web Scraping avec BeautifulSoup](#scraping)\n",
    "5. [Partie 4 : Projet complet - Pipeline de données](#project)\n",
    "6. [Exercices et défis](#exercises)"
   ],
   "id": "63afdd14a8a4531f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Setup et imports {#setup}\n",
    "\n",
    "Commençons par installer et importer toutes les bibliothèques nécessaires."
   ],
   "id": "e0f50aa0470560c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:07:38.161549Z",
     "start_time": "2025-06-11T22:07:16.239754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Installation des packages nécessaires (à exécuter une seule fois)\n",
    "!pip install requests beautifulsoup4 pandas lxml\n",
    "\n",
    "# Imports essentiels\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Tous les packages sont importés avec succès!\")"
   ],
   "id": "a271a2740be8f1de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (2.32.4)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (4.13.4)\r\n",
      "Collecting pandas\r\n",
      "  Downloading pandas-2.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\r\n",
      "Collecting lxml\r\n",
      "  Downloading lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (2025.4.26)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from beautifulsoup4) (4.14.0)\r\n",
      "Collecting numpy>=1.26.0 (from pandas)\r\n",
      "  Downloading numpy-2.3.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Collecting pytz>=2020.1 (from pandas)\r\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting tzdata>=2022.7 (from pandas)\r\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Downloading pandas-2.3.0-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.7/10.7 MB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl (8.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.1/8.1 MB\u001B[0m \u001B[31m11.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading numpy-2.3.0-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.1/5.1 MB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\r\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\r\n",
      "Installing collected packages: pytz, tzdata, numpy, lxml, pandas\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5/5\u001B[0m [pandas]2m4/5\u001B[0m [pandas]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed lxml-5.4.0 numpy-2.3.0 pandas-2.3.0 pytz-2025.2 tzdata-2025.2\r\n",
      "✅ Tous les packages sont importés avec succès!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:09:41.917437Z",
     "start_time": "2025-06-11T22:09:41.405640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Installation des packages nécessaires (à exécuter une seule fois)\n",
    "!pip install requests beautifulsoup4 pandas lxml\n",
    "\n",
    "# Imports essentiels\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Tous les packages sont importés avec succès!\")"
   ],
   "id": "7f4529c3eaf8eecb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (2.32.4)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (4.13.4)\r\n",
      "Requirement already satisfied: pandas in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (2.3.0)\r\n",
      "Requirement already satisfied: lxml in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (5.4.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (2025.4.26)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from beautifulsoup4) (4.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from pandas) (2.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "✅ Tous les packages sont importés avec succès!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:10:03.166260Z",
     "start_time": "2025-06-11T22:10:03.162554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Créer la structure de dossiers pour notre projet\n",
    "folders = ['data', 'data/raw', 'data/processed', 'outputs', 'cache']\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"📁 Structure de dossiers créée:\")\n",
    "for folder in folders:\n",
    "    print(f\"  └── {folder}/\")"
   ],
   "id": "e72613283162a4f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Structure de dossiers créée:\n",
      "  └── data/\n",
      "  └── data/raw/\n",
      "  └── data/processed/\n",
      "  └── outputs/\n",
      "  └── cache/\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "3c67050af12fa072"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Partie 1 : Maîtriser les fichiers CSV {#csv}\n",
    "\n",
    "### 2.1 Télécharger un fichier CSV depuis internet"
   ],
   "id": "f40677b218152f97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:10:52.622557Z",
     "start_time": "2025-06-11T22:10:52.619309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Méthode 1 : Avec requests\n",
    "def download_csv(url, filename):\n",
    "    \"\"\"Télécharge un fichier CSV depuis une URL\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Vérifier les erreurs\n",
    "\n",
    "    filepath = f'data/raw/{filename}'\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    print(f\"✅ Fichier téléchargé: {filepath}\")\n",
    "    return filepath"
   ],
   "id": "63591f34b2c5e0fb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:11:12.662393Z",
     "start_time": "2025-06-11T22:11:11.976248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Télécharger un fichier d'exemple\n",
    "url = 'https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv'\n",
    "csv_file = download_csv(url, 'addresses.csv')"
   ],
   "id": "6916a62ca5786395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier téléchargé: data/raw/addresses.csv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:11:32.551195Z",
     "start_time": "2025-06-11T22:11:32.546887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Aperçu rapide du fichier\n",
    "print(\"📄 Aperçu du fichier CSV:\")\n",
    "print(\"-\" * 50)\n",
    "with open(csv_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:  # Afficher les 5 premières lignes\n",
    "            print(line.strip())\n",
    "print(\"-\" * 50)"
   ],
   "id": "e7402f78c263e28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Aperçu du fichier CSV:\n",
      "--------------------------------------------------\n",
      "John,Doe,120 jefferson st.,Riverside, NJ, 08075\n",
      "Jack,McGinnis,220 hobo Av.,Phila, PA,09119\n",
      "\"John \"\"Da Man\"\"\",Repici,120 Jefferson St.,Riverside, NJ,08075\n",
      "Stephen,Tyler,\"7452 Terrace \"\"At the Plaza\"\" road\",SomeTown,SD, 91234\n",
      ",Blankman,,SomeTown, SD, 00298\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Lire un CSV sans pandas (méthode native Python)",
   "id": "fbec9fb50ae34fbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Lecture basique avec le module csv\n",
    "print(\"🔍 Lecture avec csv.reader():\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "with open(csv_file, 'r') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i < 3:  # Afficher les 3 premières lignes\n",
    "            print(f\"Ligne {i+1}: {row}\")\n",
    "            print(f\"  → Prénom: {row[0]}\")\n",
    "            print(f\"  → Nom: {row[1]}\")\n",
    "            print(f\"  → Adresse: {row[2]}\")\n",
    "            print()"
   ],
   "id": "923ff383ebe5aa35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 CSV avec en-têtes",
   "id": "1d2953b76016980a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Télécharger un CSV avec en-têtes\n",
    "url_with_headers = 'https://people.sc.fsu.edu/~jburkardt/data/csv/biostats.csv'\n",
    "csv_with_headers = download_csv(url_with_headers, 'biostats.csv')\n",
    "\n",
    "# Lire avec DictReader\n",
    "print(\"📊 Lecture avec csv.DictReader():\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "with open(csv_with_headers, 'r') as f:\n",
    "    reader = csv.DictReader(f, skipinitialspace=True)\n",
    "\n",
    "    # Afficher les colonnes disponibles\n",
    "    print(f\"Colonnes: {reader.fieldnames}\\n\")\n",
    "\n",
    "    # Lire quelques lignes\n",
    "    for i, row in enumerate(reader):\n",
    "        if i < 3:\n",
    "            print(f\"Personne {i+1}:\")\n",
    "            print(f\"  Nom: {row['Name']}\")\n",
    "            print(f\"  Sexe: {row['Sex']}\")\n",
    "            print(f\"  Âge: {row['Age']} ans\")\n",
    "            print(f\"  Taille: {row['Height (in)']} pouces\")\n",
    "            print()"
   ],
   "id": "198b60f60087a600"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.4 Analyser et nettoyer les données CSV",
   "id": "77ad231edd98db03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_csv(filepath):\n",
    "    \"\"\"Analyse un fichier CSV et retourne des statistiques\"\"\"\n",
    "    stats = {\n",
    "        'total_rows': 0,\n",
    "        'total_columns': 0,\n",
    "        'empty_cells': 0,\n",
    "        'column_names': [],\n",
    "        'sample_data': []\n",
    "    }\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        reader = csv.DictReader(f, skipinitialspace=True)\n",
    "        stats['column_names'] = reader.fieldnames\n",
    "        stats['total_columns'] = len(reader.fieldnames)\n",
    "\n",
    "        for i, row in enumerate(reader):\n",
    "            stats['total_rows'] += 1\n",
    "\n",
    "            # Compter les cellules vides\n",
    "            for value in row.values():\n",
    "                if not value or value.strip() == '':\n",
    "                    stats['empty_cells'] += 1\n",
    "\n",
    "            # Garder quelques exemples\n",
    "            if i < 3:\n",
    "                stats['sample_data'].append(row)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Analyser notre CSV\n",
    "print(\"📈 Analyse du fichier CSV:\")\n",
    "print(\"=\" * 50)\n",
    "stats = analyze_csv(csv_with_headers)\n",
    "print(f\"Total de lignes: {stats['total_rows']}\")\n",
    "print(f\"Total de colonnes: {stats['total_columns']}\")\n",
    "print(f\"Cellules vides: {stats['empty_cells']}\")\n",
    "print(f\"Colonnes: {', '.join(stats['column_names'])}\")"
   ],
   "id": "1dde11488a5f4f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.5 Écrire un fichier CSV",
   "id": "974781cfc0d2838e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Créer des données à sauvegarder\n",
    "beatles = [\n",
    "    {'prénom': 'John', 'nom': 'Lennon', 'instrument': 'guitare', 'année_naissance': 1940},\n",
    "    {'prénom': 'Paul', 'nom': 'McCartney', 'instrument': 'basse', 'année_naissance': 1942},\n",
    "    {'prénom': 'George', 'nom': 'Harrison', 'instrument': 'guitare', 'année_naissance': 1943},\n",
    "    {'prénom': 'Ringo', 'nom': 'Starr', 'instrument': 'batterie', 'année_naissance': 1940}\n",
    "]\n",
    "\n",
    "# Écrire le CSV\n",
    "output_file = 'data/processed/beatles.csv'\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    fieldnames = ['prénom', 'nom', 'instrument', 'année_naissance']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for beatle in beatles:\n",
    "        writer.writerow(beatle)\n",
    "\n",
    "print(f\"✅ Fichier créé: {output_file}\")\n",
    "\n",
    "# Vérifier le résultat\n",
    "print(\"\\n📄 Contenu du fichier créé:\")\n",
    "print(\"-\" * 50)\n",
    "with open(output_file, 'r') as f:\n",
    "    print(f.read())"
   ],
   "id": "fc0d94e04f2e10e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.6 Exercice pratique : Fusionner plusieurs CSV",
   "id": "a1248fdd5c1b4b9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def merge_csv_files(file_list, output_file, key_column=None):\n",
    "    \"\"\"\n",
    "    Fusionne plusieurs fichiers CSV en un seul\n",
    "    Si key_column est spécifié, supprime les doublons basés sur cette colonne\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    all_fieldnames = set()\n",
    "\n",
    "    # Lire tous les fichiers\n",
    "    for filepath in file_list:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            all_fieldnames.update(reader.fieldnames)\n",
    "            all_data.extend(list(reader))\n",
    "\n",
    "    # Supprimer les doublons si nécessaire\n",
    "    if key_column and key_column in all_fieldnames:\n",
    "        seen = set()\n",
    "        unique_data = []\n",
    "        for row in all_data:\n",
    "            key = row.get(key_column)\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_data.append(row)\n",
    "        all_data = unique_data\n",
    "\n",
    "    # Écrire le fichier fusionné\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=sorted(all_fieldnames))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_data)\n",
    "\n",
    "    print(f\"✅ {len(file_list)} fichiers fusionnés → {output_file}\")\n",
    "    print(f\"   Total: {len(all_data)} lignes, {len(all_fieldnames)} colonnes\")\n",
    "\n",
    "    return all_data"
   ],
   "id": "d0524742d79c1d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test de fusion (créons d'abord des fichiers d'exemple)\n",
    "# Vous pouvez tester cette fonction avec vos propres fichiers CSV"
   ],
   "id": "c8a3679df8fd4158"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Partie 2 : APIs - Collecter des données depuis le web {#api}\n",
    "\n",
    "### 3.1 Comprendre les APIs REST"
   ],
   "id": "ab32096f3ac82a02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Anatomie d'une URL d'API\n",
    "def explain_api_url(url):\n",
    "    \"\"\"Décompose et explique une URL d'API\"\"\"\n",
    "    from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    params = parse_qs(parsed.query)\n",
    "\n",
    "    print(\"🔍 Analyse de l'URL:\")\n",
    "    print(f\"  📍 Protocole: {parsed.scheme}\")\n",
    "    print(f\"  🌐 Domaine: {parsed.netloc}\")\n",
    "    print(f\"  📂 Chemin: {parsed.path}\")\n",
    "    if params:\n",
    "        print(f\"  ❓ Paramètres:\")\n",
    "        for key, values in params.items():\n",
    "            print(f\"     - {key}: {', '.join(values)}\")\n",
    "\n",
    "# Exemple\n",
    "url_example = \"https://api.github.com/users/octocat?type=user&sort=created\"\n",
    "explain_api_url(url_example)"
   ],
   "id": "3d2a9266df2363a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Première requête API",
   "id": "3bec08fea4c0596e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# API simple sans authentification : GitHub\n",
    "def get_github_user(username):\n",
    "    \"\"\"Récupère les informations d'un utilisateur GitHub\"\"\"\n",
    "    url = f'https://api.github.com/users/{username}'\n",
    "\n",
    "    print(f\"🔄 Requête vers: {url}\")\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Vérifier le statut\n",
    "    print(f\"📊 Status code: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"❌ Erreur: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Tester avec un utilisateur\n",
    "user_data = get_github_user('torvalds')  # Linus Torvalds\n",
    "\n",
    "if user_data:\n",
    "    print(\"\\n👤 Informations utilisateur:\")\n",
    "    print(f\"  Nom: {user_data['name']}\")\n",
    "    print(f\"  Entreprise: {user_data['company']}\")\n",
    "    print(f\"  Bio: {user_data['bio'][:100]}...\" if user_data['bio'] else \"  Bio: N/A\")\n",
    "    print(f\"  Repos publics: {user_data['public_repos']}\")\n",
    "    print(f\"  Followers: {user_data['followers']}\")"
   ],
   "id": "d08400eaafe6e4ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "### 3.3 API avec paramètres",
   "id": "d958f6926495a943"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# API de livres Open Library\n",
    "def search_books(query, limit=5):\n",
    "    \"\"\"Recherche des livres via l'API Open Library\"\"\"\n",
    "    base_url = 'https://openlibrary.org/search.json'\n",
    "\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'limit': limit,\n",
    "        'fields': 'title,author_name,first_publish_year,isbn,number_of_pages_median'\n",
    "    }\n",
    "\n",
    "    print(f\"🔍 Recherche de livres: '{query}'\")\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['docs']\n",
    "    else:\n",
    "        print(f\"❌ Erreur: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Rechercher des livres\n",
    "books = search_books('Python programming', limit=3)\n",
    "\n",
    "print(f\"\\n📚 {len(books)} livres trouvés:\\n\")\n",
    "for i, book in enumerate(books, 1):\n",
    "    print(f\"{i}. {book.get('title', 'Sans titre')}\")\n",
    "    authors = book.get('author_name', ['Auteur inconnu'])\n",
    "    print(f\"   Auteur(s): {', '.join(authors[:2])}\")\n",
    "    print(f\"   Année: {book.get('first_publish_year', 'N/A')}\")\n",
    "    print(f\"   Pages: {book.get('number_of_pages_median', 'N/A')}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "### 3.4 Gestion avancée des APIs\n",
    "\n",
    "```python\n",
    "class APIClient:\n",
    "    \"\"\"Client API générique avec fonctionnalités avancées\"\"\"\n",
    "\n",
    "    def __init__(self, base_url, headers=None, timeout=10):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.headers = headers or {}\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "\n",
    "        # Statistiques\n",
    "        self.stats = {\n",
    "            'requests': 0,\n",
    "            'errors': 0,\n",
    "            'total_time': 0\n",
    "        }\n",
    "\n",
    "    def _make_request(self, method, endpoint, **kwargs):\n",
    "        \"\"\"Effectue une requête avec gestion d'erreurs\"\"\"\n",
    "        url = f\"{self.base_url}/{endpoint.lstrip('/')}\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.stats['requests'] += 1\n",
    "\n",
    "        try:\n",
    "            response = self.session.request(\n",
    "                method, url, timeout=self.timeout, **kwargs\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            self.stats['total_time'] += time.time() - start_time\n",
    "            return response\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.stats['errors'] += 1\n",
    "            print(f\"❌ Erreur: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get(self, endpoint, params=None):\n",
    "        \"\"\"Requête GET\"\"\"\n",
    "        return self._make_request('GET', endpoint, params=params)\n",
    "\n",
    "    def post(self, endpoint, data=None, json=None):\n",
    "        \"\"\"Requête POST\"\"\"\n",
    "        return self._make_request('POST', endpoint, data=data, json=json)\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Affiche les statistiques\"\"\"\n",
    "        print(\"\\n📊 Statistiques API:\")\n",
    "        print(f\"  Total requêtes: {self.stats['requests']}\")\n",
    "        print(f\"  Erreurs: {self.stats['errors']}\")\n",
    "        if self.stats['requests'] > 0:\n",
    "            avg_time = self.stats['total_time'] / self.stats['requests']\n",
    "            print(f\"  Temps moyen: {avg_time:.2f}s\")\n",
    "            success_rate = (1 - self.stats['errors']/self.stats['requests']) * 100\n",
    "            print(f\"  Taux de succès: {success_rate:.1f}%\")\n",
    "\n",
    "# Utilisation du client\n",
    "github_client = APIClient('https://api.github.com')\n",
    "\n",
    "# Faire plusieurs requêtes\n",
    "endpoints = ['users/github', 'users/microsoft', 'users/google']\n",
    "for endpoint in endpoints:\n",
    "    response = github_client.get(endpoint)\n",
    "    if response:\n",
    "        data = response.json()\n",
    "        print(f\"✅ {data['name']}: {data['public_repos']} repos\")\n",
    "\n",
    "github_client.get_stats()"
   ],
   "id": "267dc91f8cf49a02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 API avec pagination",
   "id": "31b0a811d38da1a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_all_pages(base_url, params=None, max_pages=5):\n",
    "    \"\"\"\n",
    "    Récupère toutes les pages d'une API paginée\n",
    "    Exemple avec l'API GitHub\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= max_pages:\n",
    "        # Ajouter la pagination aux paramètres\n",
    "        current_params = params.copy() if params else {}\n",
    "        current_params['page'] = page\n",
    "        current_params['per_page'] = 30  # GitHub default\n",
    "\n",
    "        print(f\"📄 Récupération page {page}...\")\n",
    "        response = requests.get(base_url, params=current_params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ Erreur page {page}: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # Si pas de données, on arrête\n",
    "        if not data:\n",
    "            print(f\"✅ Fin de la pagination à la page {page}\")\n",
    "            break\n",
    "\n",
    "        all_results.extend(data)\n",
    "        page += 1\n",
    "\n",
    "        # Respecter le rate limit\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Exemple : récupérer les repos d'une organisation\n",
    "org_repos = get_all_pages(\n",
    "    'https://api.github.com/orgs/python/repos',\n",
    "    params={'type': 'public', 'sort': 'stars'},\n",
    "    max_pages=3\n",
    ")\n",
    "\n",
    "print(f\"\\n📦 {len(org_repos)} repositories récupérés\")\n",
    "print(\"\\nTop 5 par étoiles:\")\n",
    "for i, repo in enumerate(org_repos[:5], 1):\n",
    "    print(f\"{i}. {repo['name']} ⭐ {repo['stargazers_count']}\")"
   ],
   "id": "2b1794514e81fb51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.6 Exercice : Créer un wrapper d'API complet",
   "id": "97d36a73cd2ef8b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BookAPI:\n",
    "    \"\"\"Wrapper pour l'API Open Library\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_url = 'https://openlibrary.org'\n",
    "        self.cache = {}\n",
    "\n",
    "    def search_books(self, query, **kwargs):\n",
    "        \"\"\"Recherche de livres\"\"\"\n",
    "        endpoint = f\"{self.base_url}/search.json\"\n",
    "        params = {'q': query, **kwargs}\n",
    "\n",
    "        # Simple cache en mémoire\n",
    "        cache_key = f\"search_{query}_{str(kwargs)}\"\n",
    "        if cache_key in self.cache:\n",
    "            print(\"📦 Résultat depuis le cache\")\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            self.cache[cache_key] = data\n",
    "            return data\n",
    "        return None\n",
    "\n",
    "    def get_book_by_isbn(self, isbn):\n",
    "        \"\"\"Récupère un livre par ISBN\"\"\"\n",
    "        endpoint = f\"{self.base_url}/api/books\"\n",
    "        params = {\n",
    "            'bibkeys': f'ISBN:{isbn}',\n",
    "            'format': 'json',\n",
    "            'jscmd': 'data'\n",
    "        }\n",
    "\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get(f'ISBN:{isbn}')\n",
    "        return None\n",
    "\n",
    "    def get_author(self, author_id):\n",
    "        \"\"\"Récupère les infos d'un auteur\"\"\"\n",
    "        endpoint = f\"{self.base_url}/authors/{author_id}.json\"\n",
    "        response = requests.get(endpoint)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        return None"
   ],
   "id": "27ba89e64b00147e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Utilisation\n",
    "book_api = BookAPI()\n",
    "\n",
    "# Rechercher\n",
    "results = book_api.search_books('Harry Potter', limit=1)\n",
    "if results and results['docs']:\n",
    "    book = results['docs'][0]\n",
    "    print(f\"📖 Trouvé: {book.get('title')}\")\n",
    "    print(f\"   Auteur: {book.get('author_name', ['Unknown'])[0]}\")\n",
    "\n",
    "    # Récupérer par ISBN si disponible\n",
    "    if 'isbn' in book and book['isbn']:\n",
    "        isbn = book['isbn'][0]\n",
    "        print(f\"\\n🔍 Recherche détails pour ISBN: {isbn}\")\n",
    "        details = book_api.get_book_by_isbn(isbn)\n",
    "        if details:\n",
    "            print(f\"   Éditeur: {details.get('publishers', [{}])[0].get('name', 'N/A')}\")\n",
    "            print(f\"   Pages: {details.get('number_of_pages', 'N/A')}\")"
   ],
   "id": "c5356783f9209c1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "7be95aa85310b29d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Partie 3 : Web Scraping avec BeautifulSoup {#scraping}\n",
    "\n",
    "### 4.1 Introduction au HTML et BeautifulSoup"
   ],
   "id": "fef05ac2181ca1cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# HTML d'exemple pour comprendre la structure\n",
    "html_example = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Ma Page d'Exemple</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 class=\"main-title\">Bienvenue sur ma page</h1>\n",
    "        <div class=\"content\">\n",
    "            <p id=\"intro\">Ceci est une introduction.</p>\n",
    "            <ul class=\"list\">\n",
    "                <li class=\"item\">Premier élément</li>\n",
    "                <li class=\"item\">Deuxième élément</li>\n",
    "                <li class=\"item special\">Élément spécial</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div class=\"footer\">\n",
    "            <p>© 2024 Mon Site</p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parser avec BeautifulSoup\n",
    "soup = BeautifulSoup(html_example, 'html.parser')\n",
    "\n",
    "print(\"🌳 Structure HTML parsée\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Naviguer dans le HTML\n",
    "print(f\"Titre de la page: {soup.title.string}\")\n",
    "print(f\"H1 principal: {soup.h1.string}\")\n",
    "print(f\"Paragraphe intro: {soup.find(id='intro').string}\")\n",
    "\n",
    "# Trouver tous les éléments de liste\n",
    "print(\"\\nÉléments de la liste:\")\n",
    "for li in soup.find_all('li'):\n",
    "    classes = li.get('class', [])\n",
    "    print(f\"  - {li.string} (classes: {', '.join(classes)})\")"
   ],
   "id": "6f68b795c42a3e12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Scraping d'un vrai site web",
   "id": "e21b51be964626c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_quotes():\n",
    "    \"\"\"Scrape des citations depuis quotes.toscrape.com\"\"\"\n",
    "    url = 'http://quotes.toscrape.com/'\n",
    "\n",
    "    print(f\"🕷️ Scraping de {url}\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    quotes_data = []\n",
    "\n",
    "    # Trouver toutes les citations\n",
    "    quotes = soup.find_all('div', class_='quote')\n",
    "\n",
    "    for quote in quotes:\n",
    "        # Extraire le texte\n",
    "        text = quote.find('span', class_='text').text\n",
    "\n",
    "        # Extraire l'auteur\n",
    "        author = quote.find('small', class_='author').text\n",
    "\n",
    "        # Extraire les tags\n",
    "        tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
    "\n",
    "        quotes_data.append({\n",
    "            'text': text,\n",
    "            'author': author,\n",
    "            'tags': tags\n",
    "        })\n",
    "\n",
    "    return quotes_data\n",
    "\n",
    "# Scraper les citations\n",
    "quotes = scrape_quotes()\n",
    "\n",
    "print(f\"\\n📜 {len(quotes)} citations trouvées:\\n\")\n",
    "for i, quote in enumerate(quotes[:3], 1):\n",
    "    print(f\"{i}. {quote['text'][:60]}...\")\n",
    "    print(f\"   - {quote['author']}\")\n",
    "    print(f\"   Tags: {', '.join(quote['tags'])}\")\n",
    "    print()"
   ],
   "id": "40544c34113235ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 Scraping avancé : IMDB",
   "id": "525edca28dccbf6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_imdb_movies(num_movies=10):\n",
    "    \"\"\"\n",
    "    Scrape les films populaires d'IMDB\n",
    "    Note: Dans un cas réel, respectez robots.txt et les conditions d'utilisation\n",
    "    \"\"\"\n",
    "    url = 'https://www.imdb.com/chart/moviemeter/'\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "\n",
    "    print(f\"🎬 Scraping IMDB: {url}\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"❌ Erreur: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    movies = []\n",
    "\n",
    "    # Chercher le tableau des films\n",
    "    movie_table = soup.find('tbody', class_='lister-list')\n",
    "    if not movie_table:\n",
    "        print(\"❌ Structure de la page a changé\")\n",
    "        return []\n",
    "\n",
    "    movie_rows = movie_table.find_all('tr')[:num_movies]\n",
    "\n",
    "    for row in movie_rows:\n",
    "        try:\n",
    "            # Titre\n",
    "            title_column = row.find('td', class_='titleColumn')\n",
    "            title = title_column.find('a').text.strip()\n",
    "\n",
    "            # Année\n",
    "            year = title_column.find('span', class_='secondaryInfo').text.strip('()')\n",
    "\n",
    "            # Note\n",
    "            rating_column = row.find('td', class_='ratingColumn')\n",
    "            rating = rating_column.find('strong')\n",
    "            rating = rating.text if rating else 'N/A'\n",
    "\n",
    "            # Rang\n",
    "            rank = row.find('td', class_='posterColumn').find('span')['data-value']\n",
    "\n",
    "            movies.append({\n",
    "                'rank': rank,\n",
    "                'title': title,\n",
    "                'year': year,\n",
    "                'rating': rating\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur lors du parsing d'un film: {e}\")\n",
    "            continue\n",
    "\n",
    "    return movies\n",
    "\n",
    "# Note: Cette fonction est à des fins éducatives\n",
    "# Toujours vérifier robots.txt et respecter les limites\n",
    "print(\"⚠️ Note: Ceci est un exemple éducatif.\")\n",
    "print(\"   En production, utilisez l'API IMDB ou respectez robots.txt\\n\")\n",
    "\n",
    "# Pour l'exemple, nous allons créer des données simulées\n",
    "mock_movies = [\n",
    "    {'rank': '1', 'title': 'The Shawshank Redemption', 'year': '1994', 'rating': '9.3'},\n",
    "    {'rank': '2', 'title': 'The Godfather', 'year': '1972', 'rating': '9.2'},\n",
    "    {'rank': '3', 'title': 'The Dark Knight', 'year': '2008', 'rating': '9.0'},\n",
    "]\n",
    "\n",
    "print(\"🎬 Top 3 films (données simulées):\")\n",
    "for movie in mock_movies:\n",
    "    print(f\"{movie['rank']}. {movie['title']} ({movie['year']}) - ⭐ {movie['rating']}\")"
   ],
   "id": "b0109c3a242a76b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.4 Techniques de navigation avancées",
   "id": "fb57c2996e8bb4c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# HTML complexe pour démonstration\n",
    "complex_html = \"\"\"\n",
    "<div class=\"container\">\n",
    "    <article class=\"post\" id=\"post1\">\n",
    "        <header>\n",
    "            <h2>Premier Article</h2>\n",
    "            <span class=\"author\">Par Alice</span>\n",
    "            <time>2024-01-15</time>\n",
    "        </header>\n",
    "        <div class=\"content\">\n",
    "            <p>Premier paragraphe de l'article.</p>\n",
    "            <p>Deuxième paragraphe avec <a href=\"/link1\">un lien</a>.</p>\n",
    "        </div>\n",
    "        <footer>\n",
    "            <span class=\"tags\">Python, Web Scraping</span>\n",
    "            <span class=\"comments\">5 commentaires</span>\n",
    "        </footer>\n",
    "    </article>\n",
    "\n",
    "    <article class=\"post\" id=\"post2\">\n",
    "        <header>\n",
    "            <h2>Deuxième Article</h2>\n",
    "            <span class=\"author\">Par Bob</span>\n",
    "            <time>2024-01-16</time>\n",
    "        </header>\n",
    "        <div class=\"content\">\n",
    "            <p>Contenu du deuxième article.</p>\n",
    "        </div>\n",
    "    </article>\n",
    "</div>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(complex_html, 'html.parser')\n",
    "\n",
    "print(\"🧭 Navigation avancée dans le HTML\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Naviguer avec les relations parent/enfant\n",
    "article = soup.find('article')\n",
    "header = article.find('header')\n",
    "print(f\"1. Titre de l'article: {header.h2.string}\")\n",
    "print(f\"   Parent du header: {header.parent.name}\")\n",
    "\n",
    "# 2. Naviguer entre siblings\n",
    "print(f\"\\n2. Siblings du header:\")\n",
    "for sibling in header.find_next_siblings():\n",
    "    print(f\"   - {sibling.name}: {sibling.get('class', [])}\")\n",
    "\n",
    "# 3. Recherche avec fonctions personnalisées\n",
    "def has_multiple_classes(tag):\n",
    "    \"\"\"Trouve les tags avec plusieurs classes\"\"\"\n",
    "    return tag.has_attr('class') and len(tag['class']) > 1\n",
    "\n",
    "tags_with_multiple_classes = soup.find_all(has_multiple_classes)\n",
    "print(f\"\\n3. Tags avec plusieurs classes: {len(tags_with_multiple_classes)}\")\n",
    "\n",
    "# 4. Extraction de données structurées\n",
    "def extract_article_data(article):\n",
    "    \"\"\"Extrait toutes les données d'un article\"\"\"\n",
    "    return {\n",
    "        'id': article.get('id'),\n",
    "        'title': article.find('h2').string,\n",
    "        'author': article.find('span', class_='author').text.replace('Par ', ''),\n",
    "        'date': article.find('time').string,\n",
    "        'content': ' '.join(p.get_text(strip=True) for p in article.find_all('p')),\n",
    "        'tags': article.find('span', class_='tags').text if article.find('span', class_='tags') else 'N/A'\n",
    "    }\n",
    "\n",
    "print(\"\\n4. Données extraites:\")\n",
    "for article in soup.find_all('article'):\n",
    "    data = extract_article_data(article)\n",
    "    print(f\"\\n   Article: {data['title']}\")\n",
    "    print(f\"   Auteur: {data['author']}\")\n",
    "    print(f\"   Date: {data['date']}\")"
   ],
   "id": "60694d3c6dcceeac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.5 Gestion des erreurs et robustesse",
   "id": "9936d90e6fdd47af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RobustScraper:\n",
    "    \"\"\"Scraper robuste avec gestion d'erreurs\"\"\"\n",
    "\n",
    "    def __init__(self, retry_count=3, delay=1):\n",
    "        self.retry_count = retry_count\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (compatible; Educational Bot)'\n",
    "        })\n",
    "\n",
    "    def fetch_page(self, url):\n",
    "        \"\"\"Récupère une page avec retry\"\"\"\n",
    "        for attempt in range(self.retry_count):\n",
    "            try:\n",
    "                print(f\"🔄 Tentative {attempt + 1}/{self.retry_count} pour {url}\")\n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"❌ Erreur: {e}\")\n",
    "                if attempt < self.retry_count - 1:\n",
    "                    print(f\"⏳ Attente {self.delay}s avant retry...\")\n",
    "                    time.sleep(self.delay)\n",
    "                else:\n",
    "                    print(\"❌ Échec après toutes les tentatives\")\n",
    "                    return None\n",
    "\n",
    "    def safe_extract(self, soup, selector, attribute=None, default='N/A'):\n",
    "        \"\"\"Extraction sécurisée d'éléments\"\"\"\n",
    "        try:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                if attribute:\n",
    "                    return element.get(attribute, default)\n",
    "                return element.get_text(strip=True)\n",
    "            return default\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur d'extraction pour {selector}: {e}\")\n",
    "            return default\n",
    "\n",
    "    def scrape_with_structure(self, url, structure):\n",
    "        \"\"\"\n",
    "        Scrape selon une structure définie\n",
    "        structure = {\n",
    "            'title': {'selector': 'h1', 'attribute': None},\n",
    "            'image': {'selector': 'img.main', 'attribute': 'src'}\n",
    "        }\n",
    "        \"\"\"\n",
    "        response = self.fetch_page(url)\n",
    "        if not response:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        data = {}\n",
    "\n",
    "        for field, config in structure.items():\n",
    "            data[field] = self.safe_extract(\n",
    "                soup,\n",
    "                config['selector'],\n",
    "                config.get('attribute'),\n",
    "                config.get('default', 'N/A')\n",
    "            )\n",
    "\n",
    "        return data\n",
    "\n",
    "# Utilisation\n",
    "scraper = RobustScraper()\n",
    "\n",
    "# Définir la structure à extraire\n",
    "structure = {\n",
    "    'title': {'selector': 'h1.main-title'},\n",
    "    'author': {'selector': 'span.author'},\n",
    "    'date': {'selector': 'time', 'attribute': 'datetime'},\n",
    "    'content': {'selector': 'div.content'}\n",
    "}\n",
    "\n",
    "# Test avec une URL (utiliser une vraie URL ici)\n",
    "# data = scraper.scrape_with_structure('https://example.com', structure)"
   ],
   "id": "12b312d3d6b96a3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "1607555da260d686"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Partie 4 : Projet complet - Pipeline de données {#project}\n",
    "\n",
    "### 5.1 Architecture du pipeline"
   ],
   "id": "bef30cc8fbd3701b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DataSource(ABC):\n",
    "    \"\"\"Classe abstraite pour les sources de données\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def validate(self, data):\n",
    "        pass\n",
    "\n",
    "class CSVSource(DataSource):\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"Extrait les données du CSV\"\"\"\n",
    "        data = []\n",
    "        with open(self.filepath, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                data.append(row)\n",
    "        return data\n",
    "\n",
    "    def validate(self, data):\n",
    "        \"\"\"Valide les données CSV\"\"\"\n",
    "        if not data:\n",
    "            return False\n",
    "        # Vérifier que toutes les lignes ont les mêmes clés\n",
    "        keys = set(data[0].keys())\n",
    "        return all(set(row.keys()) == keys for row in data)\n",
    "\n",
    "class APISource(DataSource):\n",
    "    def __init__(self, url, params=None):\n",
    "        self.url = url\n",
    "        self.params = params or {}\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"Extrait les données de l'API\"\"\"\n",
    "        response = requests.get(self.url, params=self.params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        return []\n",
    "\n",
    "    def validate(self, data):\n",
    "        \"\"\"Valide les données API\"\"\"\n",
    "        return isinstance(data, (list, dict)) and len(str(data)) > 0\n",
    "\n",
    "class WebSource(DataSource):\n",
    "    def __init__(self, url, extractor_func):\n",
    "        self.url = url\n",
    "        self.extractor_func = extractor_func\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"Extrait les données du site web\"\"\"\n",
    "        response = requests.get(self.url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return self.extractor_func(soup)\n",
    "        return []\n",
    "\n",
    "    def validate(self, data):\n",
    "        \"\"\"Valide les données web\"\"\"\n",
    "        return isinstance(data, list) and len(data) > 0\n",
    "\n",
    "print(\"✅ Classes de base définies\")"
   ],
   "id": "401f0abc83093925"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Pipeline de transformation",
   "id": "4ee9f80ddcb776a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"Pipeline complet de traitement de données\"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sources = []\n",
    "        self.transformations = []\n",
    "        self.data = []\n",
    "        self.metadata = {\n",
    "            'created': datetime.now(),\n",
    "            'sources_count': 0,\n",
    "            'records_count': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "\n",
    "    def add_source(self, source):\n",
    "        \"\"\"Ajoute une source de données\"\"\"\n",
    "        self.sources.append(source)\n",
    "        self.metadata['sources_count'] = len(self.sources)\n",
    "        return self\n",
    "\n",
    "    def add_transformation(self, func):\n",
    "        \"\"\"Ajoute une transformation\"\"\"\n",
    "        self.transformations.append(func)\n",
    "        return self\n",
    "\n",
    "    def extract_all(self):\n",
    "        \"\"\"Extrait données de toutes les sources\"\"\"\n",
    "        print(f\"\\n🚀 Démarrage pipeline: {self.name}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        all_data = []\n",
    "\n",
    "        for i, source in enumerate(self.sources, 1):\n",
    "            print(f\"\\n📥 Source {i}/{len(self.sources)}: {source.__class__.__name__}\")\n",
    "\n",
    "            try:\n",
    "                data = source.extract()\n",
    "\n",
    "                if source.validate(data):\n",
    "                    if isinstance(data, dict):\n",
    "                        data = [data]\n",
    "\n",
    "                    all_data.extend(data)\n",
    "                    print(f\"   ✅ {len(data)} enregistrements extraits\")\n",
    "                else:\n",
    "                    error = f\"Validation échouée pour {source.__class__.__name__}\"\n",
    "                    print(f\"   ❌ {error}\")\n",
    "                    self.metadata['errors'].append(error)\n",
    "\n",
    "            except Exception as e:\n",
    "                error = f\"Erreur extraction {source.__class__.__name__}: {str(e)}\"\n",
    "                print(f\"   ❌ {error}\")\n",
    "                self.metadata['errors'].append(error)\n",
    "\n",
    "        self.data = all_data\n",
    "        self.metadata['records_count'] = len(all_data)\n",
    "        return self\n",
    "\n",
    "    def transform_all(self):\n",
    "        \"\"\"Applique toutes les transformations\"\"\"\n",
    "        print(f\"\\n🔄 Application de {len(self.transformations)} transformations\")\n",
    "\n",
    "        for i, transform in enumerate(self.transformations, 1):\n",
    "            print(f\"   📝 Transformation {i}: {transform.__name__}\")\n",
    "            try:\n",
    "                self.data = transform(self.data)\n",
    "                print(f\"      ✅ Succès ({len(self.data)} enregistrements)\")\n",
    "            except Exception as e:\n",
    "                error = f\"Erreur transformation {transform.__name__}: {str(e)}\"\n",
    "                print(f\"      ❌ {error}\")\n",
    "                self.metadata['errors'].append(error)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def save(self, format='json', output_dir='outputs'):\n",
    "        \"\"\"Sauvegarde les données\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        if format == 'json':\n",
    "            filepath = f\"{output_dir}/{self.name}_{timestamp}.json\"\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'metadata': {k: str(v) for k, v in self.metadata.items()},\n",
    "                    'data': self.data\n",
    "                }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        elif format == 'csv':\n",
    "            filepath = f\"{output_dir}/{self.name}_{timestamp}.csv\"\n",
    "            if self.data:\n",
    "                with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=self.data[0].keys())\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(self.data)\n",
    "\n",
    "        print(f\"\\n💾 Données sauvegardées: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Résumé du pipeline\"\"\"\n",
    "        print(f\"\\n📊 Résumé du pipeline: {self.name}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Sources: {self.metadata['sources_count']}\")\n",
    "        print(f\"Enregistrements: {self.metadata['records_count']}\")\n",
    "        print(f\"Erreurs: {len(self.metadata['errors'])}\")\n",
    "\n",
    "        if self.metadata['errors']:\n",
    "            print(\"\\n⚠️ Erreurs rencontrées:\")\n",
    "            for error in self.metadata['errors']:\n",
    "                print(f\"   - {error}\")\n",
    "\n",
    "# Définir des transformations\n",
    "def clean_text(data):\n",
    "    \"\"\"Nettoie les champs texte\"\"\"\n",
    "    for record in data:\n",
    "        for key, value in record.items():\n",
    "            if isinstance(value, str):\n",
    "                record[key] = value.strip()\n",
    "    return data\n",
    "\n",
    "def add_timestamp(data):\n",
    "    \"\"\"Ajoute un timestamp à chaque enregistrement\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    for record in data:\n",
    "        record['processed_at'] = timestamp\n",
    "    return data\n",
    "\n",
    "def remove_duplicates(data):\n",
    "    \"\"\"Supprime les doublons basés sur un hash\"\"\"\n",
    "    seen = set()\n",
    "    unique_data = []\n",
    "\n",
    "    for record in data:\n",
    "        # Créer un hash unique pour chaque enregistrement\n",
    "        record_hash = hashlib.md5(\n",
    "            json.dumps(record, sort_keys=True).encode()\n",
    "        ).hexdigest()\n",
    "\n",
    "        if record_hash not in seen:\n",
    "            seen.add(record_hash)\n",
    "            unique_data.append(record)\n",
    "\n",
    "    print(f\"      Doublons supprimés: {len(data) - len(unique_data)}\")\n",
    "    return unique_data\n",
    "\n",
    "print(\"✅ Pipeline et transformations définis\")"
   ],
   "id": "bd4f94e27a92939b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.3 Exemple d'utilisation complète",
   "id": "998acdc397047cd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Créer un pipeline complet\n",
    "pipeline = DataPipeline(\"donnees_combinees\")\n",
    "\n",
    "# 1. Ajouter une source CSV\n",
    "csv_source = CSVSource('data/processed/beatles.csv')\n",
    "pipeline.add_source(csv_source)\n",
    "\n",
    "# 2. Ajouter une source API (exemple avec une API de test)\n",
    "api_source = APISource(\n",
    "    'https://jsonplaceholder.typicode.com/users',\n",
    "    params={'_limit': 5}\n",
    ")\n",
    "pipeline.add_source(api_source)\n",
    "\n",
    "# 3. Ajouter une source Web\n",
    "def extract_quotes_simple(soup):\n",
    "    \"\"\"Extracteur simple pour les citations\"\"\"\n",
    "    quotes = []\n",
    "    # Simuler l'extraction (en production, utiliser le vrai scraping)\n",
    "    quotes.append({\n",
    "        'text': 'La vie est belle',\n",
    "        'author': 'Anonyme',\n",
    "        'source': 'web'\n",
    "    })\n",
    "    return quotes\n",
    "\n",
    "web_source = WebSource('http://quotes.toscrape.com/', extract_quotes_simple)\n",
    "pipeline.add_source(web_source)\n",
    "\n",
    "# 4. Ajouter des transformations\n",
    "pipeline.add_transformation(clean_text)\n",
    "pipeline.add_transformation(add_timestamp)\n",
    "pipeline.add_transformation(remove_duplicates)\n",
    "\n",
    "# 5. Exécuter le pipeline\n",
    "pipeline.extract_all().transform_all()\n",
    "\n",
    "# 6. Sauvegarder les résultats\n",
    "json_file = pipeline.save(format='json')\n",
    "csv_file = pipeline.save(format='csv')\n",
    "\n",
    "# 7. Afficher le résumé\n",
    "pipeline.get_summary()\n",
    "\n",
    "# 8. Aperçu des données finales\n",
    "print(\"\\n👀 Aperçu des données finales:\")\n",
    "for i, record in enumerate(pipeline.data[:3], 1):\n",
    "    print(f\"\\nEnregistrement {i}:\")\n",
    "    for key, value in list(record.items())[:4]:  # Afficher 4 premiers champs\n",
    "        print(f\"  {key}: {value}\")"
   ],
   "id": "2a7a20e785e35863"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.4 Cache et optimisation",
   "id": "ec6b4523571027e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CachedDataSource:\n",
    "    \"\"\"Wrapper pour ajouter du cache à n'importe quelle source\"\"\"\n",
    "\n",
    "    def __init__(self, source, cache_dir='cache', expire_hours=24):\n",
    "        self.source = source\n",
    "        self.cache_dir = cache_dir\n",
    "        self.expire_hours = expire_hours\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    def _get_cache_key(self):\n",
    "        \"\"\"Génère une clé de cache unique\"\"\"\n",
    "        source_str = f\"{self.source.__class__.__name__}_{self.source.__dict__}\"\n",
    "        return hashlib.md5(source_str.encode()).hexdigest()\n",
    "\n",
    "    def _get_cache_path(self):\n",
    "        \"\"\"Chemin du fichier cache\"\"\"\n",
    "        return os.path.join(self.cache_dir, f\"{self._get_cache_key()}.pkl\")\n",
    "\n",
    "    def _is_cache_valid(self, cache_path):\n",
    "        \"\"\"Vérifie si le cache est encore valide\"\"\"\n",
    "        if not os.path.exists(cache_path):\n",
    "            return False\n",
    "\n",
    "        # Vérifier l'âge du cache\n",
    "        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))\n",
    "        age = datetime.now() - file_time\n",
    "        return age.total_seconds() < self.expire_hours * 3600\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"Extrait avec mise en cache\"\"\"\n",
    "        cache_path = self._get_cache_path()\n",
    "\n",
    "        # Vérifier le cache\n",
    "        if self._is_cache_valid(cache_path):\n",
    "            print(f\"   📦 Utilisation du cache: {os.path.basename(cache_path)}\")\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "\n",
    "        # Extraire les données\n",
    "        print(f\"   🔄 Extraction depuis la source...\")\n",
    "        data = self.source.extract()\n",
    "\n",
    "        # Sauvegarder en cache\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def validate(self, data):\n",
    "        \"\"\"Délègue la validation à la source originale\"\"\"\n",
    "        return self.source.validate(data)\n",
    "\n",
    "# Utilisation avec cache\n",
    "print(\"🧪 Test du système de cache\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Source API avec cache\n",
    "api_source = APISource('https://jsonplaceholder.typicode.com/posts', {'_limit': 10})\n",
    "cached_api = CachedDataSource(api_source, expire_hours=1)\n",
    "\n",
    "# Premier appel - pas de cache\n",
    "print(\"\\n1️⃣ Premier appel:\")\n",
    "data1 = cached_api.extract()\n",
    "print(f\"   Données récupérées: {len(data1)} éléments\")\n",
    "\n",
    "# Deuxième appel - depuis le cache\n",
    "print(\"\\n2️⃣ Deuxième appel:\")\n",
    "data2 = cached_api.extract()\n",
    "print(f\"   Données récupérées: {len(data2)} éléments\")\n",
    "\n",
    "print(\"\\n✅ Le cache fonctionne!\")"
   ],
   "id": "4e643e5928a4905c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "33e05953fce4315e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Exercices et défis {#exercises}\n",
    "\n",
    "### Défi 1 : Analyseur CSV avancé"
   ],
   "id": "5a54bde072585819"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: Implémenter cette classe\n",
    "class AdvancedCSVAnalyzer:\n",
    "    \"\"\"\n",
    "    Votre mission : Créer un analyseur CSV qui peut :\n",
    "    1. Détecter automatiquement le délimiteur (,;|\\t)\n",
    "    2. Détecter l'encodage du fichier\n",
    "    3. Gérer les fichiers avec ou sans en-têtes\n",
    "    4. Fournir des statistiques sur chaque colonne\n",
    "    5. Détecter les types de données\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        # TODO: Implémenter\n",
    "\n",
    "    def detect_delimiter(self):\n",
    "        \"\"\"Détecte automatiquement le délimiteur utilisé\"\"\"\n",
    "        # TODO: Lire quelques lignes et analyser\n",
    "        pass\n",
    "\n",
    "    def detect_encoding(self):\n",
    "        \"\"\"Détecte l'encodage du fichier\"\"\"\n",
    "        # TODO: Essayer différents encodages\n",
    "        pass\n",
    "\n",
    "    def analyze_columns(self):\n",
    "        \"\"\"Analyse chaque colonne (type, valeurs uniques, etc.)\"\"\"\n",
    "        # TODO: Parcourir et analyser\n",
    "        pass\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Génère un rapport complet sur le fichier\"\"\"\n",
    "        # TODO: Compiler toutes les analyses\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# analyzer = AdvancedCSVAnalyzer('data/votre_fichier.csv')\n",
    "# report = analyzer.generate_report()\n",
    "# print(report)"
   ],
   "id": "9774d0ea83a58129"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Défi 2 : Multi-API Aggregator",
   "id": "a29ecddad059626b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: Implémenter\n",
    "class MultiAPIAggregator:\n",
    "    \"\"\"\n",
    "    Votre mission : Créer un agrégateur qui peut :\n",
    "    1. Interroger plusieurs APIs en parallèle\n",
    "    2. Normaliser les formats de réponse différents\n",
    "    3. Fusionner les résultats intelligemment\n",
    "    4. Gérer les erreurs par API\n",
    "    5. Respecter les rate limits de chaque API\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.apis = {}\n",
    "        # TODO: Implémenter\n",
    "\n",
    "    def register_api(self, name, config):\n",
    "        \"\"\"Enregistre une nouvelle API\"\"\"\n",
    "        # config = {\n",
    "        #     'base_url': '...',\n",
    "        #     'rate_limit': 60,  # requêtes par minute\n",
    "        #     'normalizer': function,  # pour normaliser les réponses\n",
    "        # }\n",
    "        pass\n",
    "\n",
    "    def query_all(self, search_term):\n",
    "        \"\"\"Interroge toutes les APIs enregistrées\"\"\"\n",
    "        # TODO: Utiliser threading ou asyncio\n",
    "        pass\n",
    "\n",
    "    def merge_results(self, results):\n",
    "        \"\"\"Fusionne intelligemment les résultats\"\"\"\n",
    "        # TODO: Dédupliquer, scorer, trier\n",
    "        pass\n",
    "\n",
    "# Exemple d'utilisation attendu :\n",
    "# aggregator = MultiAPIAggregator()\n",
    "# aggregator.register_api('github', {...})\n",
    "# aggregator.register_api('gitlab', {...})\n",
    "# results = aggregator.query_all('python scraping')"
   ],
   "id": "c495600c7f7954df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Défi 3 : Smart Web Scraper",
   "id": "31bf82856375b688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: Implémenter\n",
    "class SmartWebScraper:\n",
    "    \"\"\"\n",
    "    Votre mission : Créer un scraper intelligent qui peut :\n",
    "    1. Détecter automatiquement la structure d'une page\n",
    "    2. Identifier les patterns de données (listes, tableaux, etc.)\n",
    "    3. S'adapter aux changements de structure\n",
    "    4. Générer des sélecteurs CSS optimaux\n",
    "    5. Exporter dans plusieurs formats\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        # TODO: Implémenter\n",
    "\n",
    "    def analyze_structure(self):\n",
    "        \"\"\"Analyse la structure de la page\"\"\"\n",
    "        # TODO: Identifier les patterns répétitifs\n",
    "        pass\n",
    "\n",
    "    def generate_selectors(self):\n",
    "        \"\"\"Génère automatiquement les meilleurs sélecteurs\"\"\"\n",
    "        # TODO: Trouver les sélecteurs les plus stables\n",
    "        pass\n",
    "\n",
    "    def extract_data(self):\n",
    "        \"\"\"Extrait les données identifiées\"\"\"\n",
    "        # TODO: Utiliser les sélecteurs générés\n",
    "        pass\n",
    "\n",
    "    def monitor_changes(self):\n",
    "        \"\"\"Monitore les changements de structure\"\"\"\n",
    "        # TODO: Comparer avec version précédente\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# scraper = SmartWebScraper('https://example.com/products')\n",
    "# scraper.analyze_structure()\n",
    "# data = scraper.extract_data()"
   ],
   "id": "695b5dadb7d66b45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Challenge Final : Pipeline de veille concurrentielle",
   "id": "16ac5c20863e9c83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "CHALLENGE FINAL : Créer un système complet de veille concurrentielle\n",
    "\n",
    "Objectif : Surveiller automatiquement les prix/produits de plusieurs sites\n",
    "\n",
    "Fonctionnalités requises :\n",
    "1. Configuration par fichier YAML/JSON\n",
    "2. Support de multiples sources (API, Web, CSV)\n",
    "3. Détection automatique des changements\n",
    "4. Alertes (email, webhook, etc.)\n",
    "5. Dashboard de visualisation\n",
    "6. Historique des données\n",
    "7. Export automatique\n",
    "\n",
    "Structure suggérée :\n",
    "competitive_intelligence/\n",
    "├── config/\n",
    "│   └── sources.yaml\n",
    "├── extractors/\n",
    "│   ├── api_extractor.py\n",
    "│   ├── web_extractor.py\n",
    "│   └── csv_extractor.py\n",
    "├── processors/\n",
    "│   ├── normalizer.py\n",
    "│   ├── comparator.py\n",
    "│   └── alerting.py\n",
    "├── storage/\n",
    "│   ├── database.py\n",
    "│   └── cache.py\n",
    "├── dashboard/\n",
    "│   └── visualizer.py\n",
    "└── main.py\n",
    "\n",
    "Bonus :\n",
    "- Interface web (Flask/Streamlit)\n",
    "- Scheduling automatique (cron)\n",
    "- Machine Learning pour prédictions\n",
    "- API REST pour accéder aux données\n",
    "\"\"\"\n",
    "\n",
    "# Commencez ici :\n",
    "class CompetitiveIntelligence:\n",
    "    def __init__(self, config_file):\n",
    "        self.config = self.load_config(config_file)\n",
    "        self.sources = []\n",
    "        self.processors = []\n",
    "        self.storage = None\n",
    "\n",
    "    def load_config(self, config_file):\n",
    "        \"\"\"Charge la configuration depuis un fichier\"\"\"\n",
    "        # TODO: Implémenter\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Lance le pipeline complet\"\"\"\n",
    "        # TODO: Implémenter\n",
    "        pass\n",
    "\n",
    "# Bonne chance ! 🚀"
   ],
   "id": "8fe98df3c449a2d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "ebc50a09456cbc38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📚 Ressources et conseils finaux\n",
    "\n",
    "### Meilleures pratiques à retenir\n",
    "\n",
    "1. **Toujours respecter les sites web**\n",
    "   - Vérifier robots.txt\n",
    "   - Ajouter des délais entre requêtes\n",
    "   - Utiliser un User-Agent descriptif\n",
    "\n",
    "2. **Gérer les erreurs gracieusement**\n",
    "   - Try/except partout\n",
    "   - Logging détaillé\n",
    "   - Retry avec backoff exponentiel\n",
    "\n",
    "3. **Optimiser les performances**\n",
    "   - Cache intelligent\n",
    "   - Requêtes asynchrones quand possible\n",
    "   - Batch processing\n",
    "\n",
    "4. **Maintenir la qualité des données**\n",
    "   - Validation à chaque étape\n",
    "   - Tests unitaires\n",
    "   - Monitoring continu\n",
    "\n",
    "### Prochaines étapes\n",
    "\n",
    "1. **Approfondir les bases de données**\n",
    "   - SQLite pour le stockage local\n",
    "   - PostgreSQL pour la production\n",
    "   - MongoDB pour les données non-structurées\n",
    "\n",
    "2. **Explorer les outils avancés**\n",
    "   - Scrapy pour le web scraping industriel\n",
    "   - Selenium pour les sites JavaScript\n",
    "   - Apache Airflow pour l'orchestration\n",
    "\n",
    "3. **Apprendre l'analyse de données**\n",
    "   - Pandas pour la manipulation\n",
    "   - Matplotlib/Seaborn pour la visualisation\n",
    "   - Scikit-learn pour le ML\n",
    "\n",
    "### Commandes utiles\n"
   ],
   "id": "b9d313ca50e71717"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Vérifier robots.txt\n",
    "curl https://example.com/robots.txt\n",
    "\n",
    "# Tester une API\n",
    "curl -X GET \"https://api.example.com/endpoint\" -H \"accept: application/json\"\n",
    "\n",
    "# Monitorer les requêtes\n",
    "netstat -an | grep :80\n",
    "\n",
    "# Analyser un fichier CSV\n",
    "head -n 10 file.csv | column -t -s ','"
   ],
   "id": "92d26c3a352a1364"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "df8f4f25bd9b862"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "🎉 **Félicitations !** Vous avez maintenant toutes les bases pour devenir un expert en Data Sourcing !\n",
    "\n",
    "N'oubliez pas : la pratique est la clé. Commencez par de petits projets et augmentez progressivement la complexité.\n",
    "\n",
    "Bon code ! 🐍✨"
   ],
   "id": "b02f520f0dfb461"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
