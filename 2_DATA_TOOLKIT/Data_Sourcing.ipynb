{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Sourcing : De DÃ©butant Ã  Pro ğŸš€",
   "id": "afe6e8b7be2c1857"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Table des matiÃ¨res",
   "id": "e7c198fe78771564"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. [Setup et imports](#setup)\n",
    "2. [Partie 1 : MaÃ®triser les fichiers CSV](#csv)\n",
    "3. [Partie 2 : APIs - Collecter des donnÃ©es depuis le web](#api)\n",
    "4. [Partie 3 : Web Scraping avec BeautifulSoup](#scraping)\n",
    "5. [Partie 4 : Projet complet - Pipeline de donnÃ©es](#project)\n",
    "6. [Exercices et dÃ©fis](#exercises)"
   ],
   "id": "63afdd14a8a4531f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Setup et imports {#setup}\n",
    "\n",
    "CommenÃ§ons par installer et importer toutes les bibliothÃ¨ques nÃ©cessaires."
   ],
   "id": "e0f50aa0470560c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:07:38.161549Z",
     "start_time": "2025-06-11T22:07:16.239754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Installation des packages nÃ©cessaires (Ã  exÃ©cuter une seule fois)\n",
    "!pip install requests beautifulsoup4 pandas lxml\n",
    "\n",
    "# Imports essentiels\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Tous les packages sont importÃ©s avec succÃ¨s!\")"
   ],
   "id": "a271a2740be8f1de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (2.32.4)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (4.13.4)\r\n",
      "Collecting pandas\r\n",
      "  Downloading pandas-2.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\r\n",
      "Collecting lxml\r\n",
      "  Downloading lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (2025.4.26)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from beautifulsoup4) (4.14.0)\r\n",
      "Collecting numpy>=1.26.0 (from pandas)\r\n",
      "  Downloading numpy-2.3.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Collecting pytz>=2020.1 (from pandas)\r\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting tzdata>=2022.7 (from pandas)\r\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Downloading pandas-2.3.0-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\r\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m10.7/10.7 MB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl (8.1 MB)\r\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m8.1/8.1 MB\u001B[0m \u001B[31m11.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading numpy-2.3.0-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\r\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m5.1/5.1 MB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\r\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\r\n",
      "Installing collected packages: pytz, tzdata, numpy, lxml, pandas\r\n",
      "\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m5/5\u001B[0m [pandas]2m4/5\u001B[0m [pandas]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed lxml-5.4.0 numpy-2.3.0 pandas-2.3.0 pytz-2025.2 tzdata-2025.2\r\n",
      "âœ… Tous les packages sont importÃ©s avec succÃ¨s!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:09:41.917437Z",
     "start_time": "2025-06-11T22:09:41.405640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Installation des packages nÃ©cessaires (Ã  exÃ©cuter une seule fois)\n",
    "!pip install requests beautifulsoup4 pandas lxml\n",
    "\n",
    "# Imports essentiels\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Tous les packages sont importÃ©s avec succÃ¨s!\")"
   ],
   "id": "7f4529c3eaf8eecb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (2.32.4)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (4.13.4)\r\n",
      "Requirement already satisfied: pandas in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (2.3.0)\r\n",
      "Requirement already satisfied: lxml in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (5.4.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from requests) (2025.4.26)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from beautifulsoup4) (4.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from pandas) (2.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/william/.virtualenvs/.venv1/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "âœ… Tous les packages sont importÃ©s avec succÃ¨s!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:10:03.166260Z",
     "start_time": "2025-06-11T22:10:03.162554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CrÃ©er la structure de dossiers pour notre projet\n",
    "folders = ['data', 'data/raw', 'data/processed', 'outputs', 'cache']\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“ Structure de dossiers crÃ©Ã©e:\")\n",
    "for folder in folders:\n",
    "    print(f\"  â””â”€â”€ {folder}/\")"
   ],
   "id": "e72613283162a4f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Structure de dossiers crÃ©Ã©e:\n",
      "  â””â”€â”€ data/\n",
      "  â””â”€â”€ data/raw/\n",
      "  â””â”€â”€ data/processed/\n",
      "  â””â”€â”€ outputs/\n",
      "  â””â”€â”€ cache/\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "3c67050af12fa072"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Partie 1 : MaÃ®triser les fichiers CSV {#csv}\n",
    "\n",
    "### 2.1 TÃ©lÃ©charger un fichier CSV depuis internet"
   ],
   "id": "f40677b218152f97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:10:52.622557Z",
     "start_time": "2025-06-11T22:10:52.619309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MÃ©thode 1 : Avec requests\n",
    "def download_csv(url, filename):\n",
    "    \"\"\"TÃ©lÃ©charge un fichier CSV depuis une URL\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # VÃ©rifier les erreurs\n",
    "\n",
    "    filepath = f'data/raw/{filename}'\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    print(f\"âœ… Fichier tÃ©lÃ©chargÃ©: {filepath}\")\n",
    "    return filepath"
   ],
   "id": "63591f34b2c5e0fb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:11:12.662393Z",
     "start_time": "2025-06-11T22:11:11.976248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TÃ©lÃ©charger un fichier d'exemple\n",
    "url = 'https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv'\n",
    "csv_file = download_csv(url, 'addresses.csv')"
   ],
   "id": "6916a62ca5786395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fichier tÃ©lÃ©chargÃ©: data/raw/addresses.csv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T22:11:32.551195Z",
     "start_time": "2025-06-11T22:11:32.546887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# AperÃ§u rapide du fichier\n",
    "print(\"ğŸ“„ AperÃ§u du fichier CSV:\")\n",
    "print(\"-\" * 50)\n",
    "with open(csv_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:  # Afficher les 5 premiÃ¨res lignes\n",
    "            print(line.strip())\n",
    "print(\"-\" * 50)"
   ],
   "id": "e7402f78c263e28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ AperÃ§u du fichier CSV:\n",
      "--------------------------------------------------\n",
      "John,Doe,120 jefferson st.,Riverside, NJ, 08075\n",
      "Jack,McGinnis,220 hobo Av.,Phila, PA,09119\n",
      "\"John \"\"Da Man\"\"\",Repici,120 Jefferson St.,Riverside, NJ,08075\n",
      "Stephen,Tyler,\"7452 Terrace \"\"At the Plaza\"\" road\",SomeTown,SD, 91234\n",
      ",Blankman,,SomeTown, SD, 00298\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Lire un CSV sans pandas (mÃ©thode native Python)",
   "id": "fbec9fb50ae34fbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Lecture basique avec le module csv\n",
    "print(\"ğŸ” Lecture avec csv.reader():\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "with open(csv_file, 'r') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i < 3:  # Afficher les 3 premiÃ¨res lignes\n",
    "            print(f\"Ligne {i+1}: {row}\")\n",
    "            print(f\"  â†’ PrÃ©nom: {row[0]}\")\n",
    "            print(f\"  â†’ Nom: {row[1]}\")\n",
    "            print(f\"  â†’ Adresse: {row[2]}\")\n",
    "            print()"
   ],
   "id": "923ff383ebe5aa35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 CSV avec en-tÃªtes",
   "id": "1d2953b76016980a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TÃ©lÃ©charger un CSV avec en-tÃªtes\n",
    "url_with_headers = 'https://people.sc.fsu.edu/~jburkardt/data/csv/biostats.csv'\n",
    "csv_with_headers = download_csv(url_with_headers, 'biostats.csv')\n",
    "\n",
    "# Lire avec DictReader\n",
    "print(\"ğŸ“Š Lecture avec csv.DictReader():\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "with open(csv_with_headers, 'r') as f:\n",
    "    reader = csv.DictReader(f, skipinitialspace=True)\n",
    "\n",
    "    # Afficher les colonnes disponibles\n",
    "    print(f\"Colonnes: {reader.fieldnames}\\n\")\n",
    "\n",
    "    # Lire quelques lignes\n",
    "    for i, row in enumerate(reader):\n",
    "        if i < 3:\n",
    "            print(f\"Personne {i+1}:\")\n",
    "            print(f\"  Nom: {row['Name']}\")\n",
    "            print(f\"  Sexe: {row['Sex']}\")\n",
    "            print(f\"  Ã‚ge: {row['Age']} ans\")\n",
    "            print(f\"  Taille: {row['Height (in)']} pouces\")\n",
    "            print()"
   ],
   "id": "198b60f60087a600"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.4 Analyser et nettoyer les donnÃ©es CSV",
   "id": "77ad231edd98db03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_csv(filepath):\n",
    "    \"\"\"Analyse un fichier CSV et retourne des statistiques\"\"\"\n",
    "    stats = {\n",
    "        'total_rows': 0,\n",
    "        'total_columns': 0,\n",
    "        'empty_cells': 0,\n",
    "        'column_names': [],\n",
    "        'sample_data': []\n",
    "    }\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        reader = csv.DictReader(f, skipinitialspace=True)\n",
    "        stats['column_names'] = reader.fieldnames\n",
    "        stats['total_columns'] = len(reader.fieldnames)\n",
    "\n",
    "        for i, row in enumerate(reader):\n",
    "            stats['total_rows'] += 1\n",
    "\n",
    "            # Compter les cellules vides\n",
    "            for value in row.values():\n",
    "                if not value or value.strip() == '':\n",
    "                    stats['empty_cells'] += 1\n",
    "\n",
    "            # Garder quelques exemples\n",
    "            if i < 3:\n",
    "                stats['sample_data'].append(row)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Analyser notre CSV\n",
    "print(\"ğŸ“ˆ Analyse du fichier CSV:\")\n",
    "print(\"=\" * 50)\n",
    "stats = analyze_csv(csv_with_headers)\n",
    "print(f\"Total de lignes: {stats['total_rows']}\")\n",
    "print(f\"Total de colonnes: {stats['total_columns']}\")\n",
    "print(f\"Cellules vides: {stats['empty_cells']}\")\n",
    "print(f\"Colonnes: {', '.join(stats['column_names'])}\")"
   ],
   "id": "1dde11488a5f4f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.5 Ã‰crire un fichier CSV",
   "id": "974781cfc0d2838e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CrÃ©er des donnÃ©es Ã  sauvegarder\n",
    "beatles = [\n",
    "    {'prÃ©nom': 'John', 'nom': 'Lennon', 'instrument': 'guitare', 'annÃ©e_naissance': 1940},\n",
    "    {'prÃ©nom': 'Paul', 'nom': 'McCartney', 'instrument': 'basse', 'annÃ©e_naissance': 1942},\n",
    "    {'prÃ©nom': 'George', 'nom': 'Harrison', 'instrument': 'guitare', 'annÃ©e_naissance': 1943},\n",
    "    {'prÃ©nom': 'Ringo', 'nom': 'Starr', 'instrument': 'batterie', 'annÃ©e_naissance': 1940}\n",
    "]\n",
    "\n",
    "# Ã‰crire le CSV\n",
    "output_file = 'data/processed/beatles.csv'\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    fieldnames = ['prÃ©nom', 'nom', 'instrument', 'annÃ©e_naissance']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for beatle in beatles:\n",
    "        writer.writerow(beatle)\n",
    "\n",
    "print(f\"âœ… Fichier crÃ©Ã©: {output_file}\")\n",
    "\n",
    "# VÃ©rifier le rÃ©sultat\n",
    "print(\"\\nğŸ“„ Contenu du fichier crÃ©Ã©:\")\n",
    "print(\"-\" * 50)\n",
    "with open(output_file, 'r') as f:\n",
    "    print(f.read())"
   ],
   "id": "fc0d94e04f2e10e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.6 Exercice pratique : Fusionner plusieurs CSV",
   "id": "a1248fdd5c1b4b9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def merge_csv_files(file_list, output_file, key_column=None):\n",
    "    \"\"\"\n",
    "    Fusionne plusieurs fichiers CSV en un seul\n",
    "    Si key_column est spÃ©cifiÃ©, supprime les doublons basÃ©s sur cette colonne\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    all_fieldnames = set()\n",
    "\n",
    "    # Lire tous les fichiers\n",
    "    for filepath in file_list:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            all_fieldnames.update(reader.fieldnames)\n",
    "            all_data.extend(list(reader))\n",
    "\n",
    "    # Supprimer les doublons si nÃ©cessaire\n",
    "    if key_column and key_column in all_fieldnames:\n",
    "        seen = set()\n",
    "        unique_data = []\n",
    "        for row in all_data:\n",
    "            key = row.get(key_column)\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_data.append(row)\n",
    "        all_data = unique_data\n",
    "\n",
    "    # Ã‰crire le fichier fusionnÃ©\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=sorted(all_fieldnames))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_data)\n",
    "\n",
    "    print(f\"âœ… {len(file_list)} fichiers fusionnÃ©s â†’ {output_file}\")\n",
    "    print(f\"   Total: {len(all_data)} lignes, {len(all_fieldnames)} colonnes\")\n",
    "\n",
    "    return all_data"
   ],
   "id": "d0524742d79c1d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test de fusion (crÃ©ons d'abord des fichiers d'exemple)\n",
    "# Vous pouvez tester cette fonction avec vos propres fichiers CSV"
   ],
   "id": "c8a3679df8fd4158"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Partie 2 : APIs - Collecter des donnÃ©es depuis le web {#api}\n",
    "\n",
    "### 3.1 Comprendre les APIs REST"
   ],
   "id": "ab32096f3ac82a02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Anatomie d'une URL d'API\n",
    "def explain_api_url(url):\n",
    "    \"\"\"DÃ©compose et explique une URL d'API\"\"\"\n",
    "    from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    params = parse_qs(parsed.query)\n",
    "\n",
    "    print(\"ğŸ” Analyse de l'URL:\")\n",
    "    print(f\"  ğŸ“ Protocole: {parsed.scheme}\")\n",
    "    print(f\"  ğŸŒ Domaine: {parsed.netloc}\")\n",
    "    print(f\"  ğŸ“‚ Chemin: {parsed.path}\")\n",
    "    if params:\n",
    "        print(f\"  â“ ParamÃ¨tres:\")\n",
    "        for key, values in params.items():\n",
    "            print(f\"     - {key}: {', '.join(values)}\")\n",
    "\n",
    "# Exemple\n",
    "url_example = \"https://api.github.com/users/octocat?type=user&sort=created\"\n",
    "explain_api_url(url_example)"
   ],
   "id": "3d2a9266df2363a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 PremiÃ¨re requÃªte API",
   "id": "3bec08fea4c0596e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# API simple sans authentification : GitHub\n",
    "def get_github_user(username):\n",
    "    \"\"\"RÃ©cupÃ¨re les informations d'un utilisateur GitHub\"\"\"\n",
    "    url = f'https://api.github.com/users/{username}'\n",
    "\n",
    "    print(f\"ğŸ”„ RequÃªte vers: {url}\")\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # VÃ©rifier le statut\n",
    "    print(f\"ğŸ“Š Status code: {response.status_code}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"âŒ Erreur: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Tester avec un utilisateur\n",
    "user_data = get_github_user('torvalds')  # Linus Torvalds\n",
    "\n",
    "if user_data:\n",
    "    print(\"\\nğŸ‘¤ Informations utilisateur:\")\n",
    "    print(f\"  Nom: {user_data['name']}\")\n",
    "    print(f\"  Entreprise: {user_data['company']}\")\n",
    "    print(f\"  Bio: {user_data['bio'][:100]}...\" if user_data['bio'] else \"  Bio: N/A\")\n",
    "    print(f\"  Repos publics: {user_data['public_repos']}\")\n",
    "    print(f\"  Followers: {user_data['followers']}\")"
   ],
   "id": "d08400eaafe6e4ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "### 3.3 API avec paramÃ¨tres",
   "id": "d958f6926495a943"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# API de livres Open Library\n",
    "def search_books(query, limit=5):\n",
    "    \"\"\"Recherche des livres via l'API Open Library\"\"\"\n",
    "    base_url = 'https://openlibrary.org/search.json'\n",
    "\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'limit': limit,\n",
    "        'fields': 'title,author_name,first_publish_year,isbn,number_of_pages_median'\n",
    "    }\n",
    "\n",
    "    print(f\"ğŸ” Recherche de livres: '{query}'\")\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['docs']\n",
    "    else:\n",
    "        print(f\"âŒ Erreur: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Rechercher des livres\n",
    "books = search_books('Python programming', limit=3)\n",
    "\n",
    "print(f\"\\nğŸ“š {len(books)} livres trouvÃ©s:\\n\")\n",
    "for i, book in enumerate(books, 1):\n",
    "    print(f\"{i}. {book.get('title', 'Sans titre')}\")\n",
    "    authors = book.get('author_name', ['Auteur inconnu'])\n",
    "    print(f\"   Auteur(s): {', '.join(authors[:2])}\")\n",
    "    print(f\"   AnnÃ©e: {book.get('first_publish_year', 'N/A')}\")\n",
    "    print(f\"   Pages: {book.get('number_of_pages_median', 'N/A')}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "### 3.4 Gestion avancÃ©e des APIs\n",
    "\n",
    "```python\n",
    "class APIClient:\n",
    "    \"\"\"Client API gÃ©nÃ©rique avec fonctionnalitÃ©s avancÃ©es\"\"\"\n",
    "\n",
    "    def __init__(self, base_url, headers=None, timeout=10):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.headers = headers or {}\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "\n",
    "        # Statistiques\n",
    "        self.stats = {\n",
    "            'requests': 0,\n",
    "            'errors': 0,\n",
    "            'total_time': 0\n",
    "        }\n",
    "\n",
    "    def _make_request(self, method, endpoint, **kwargs):\n",
    "        \"\"\"Effectue une requÃªte avec gestion d'erreurs\"\"\"\n",
    "        url = f\"{self.base_url}/{endpoint.lstrip('/')}\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.stats['requests'] += 1\n",
    "\n",
    "        try:\n",
    "            response = self.session.request(\n",
    "                method, url, timeout=self.timeout, **kwargs\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            self.stats['total_time'] += time.time() - start_time\n",
    "            return response\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.stats['errors'] += 1\n",
    "            print(f\"âŒ Erreur: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get(self, endpoint, params=None):\n",
    "        \"\"\"RequÃªte GET\"\"\"\n",
    "        return self._make_request('GET', endpoint, params=params)\n",
    "\n",
    "    def post(self, endpoint, data=None, json=None):\n",
    "        \"\"\"RequÃªte POST\"\"\"\n",
    "        return self._make_request('POST', endpoint, data=data, json=json)\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Affiche les statistiques\"\"\"\n",
    "        print(\"\\nğŸ“Š Statistiques API:\")\n",
    "        print(f\"  Total requÃªtes: {self.stats['requests']}\")\n",
    "        print(f\"  Erreurs: {self.stats['errors']}\")\n",
    "        if self.stats['requests'] > 0:\n",
    "            avg_time = self.stats['total_time'] / self.stats['requests']\n",
    "            print(f\"  Temps moyen: {avg_time:.2f}s\")\n",
    "            success_rate = (1 - self.stats['errors']/self.stats['requests']) * 100\n",
    "            print(f\"  Taux de succÃ¨s: {success_rate:.1f}%\")\n",
    "\n",
    "# Utilisation du client\n",
    "github_client = APIClient('https://api.github.com')\n",
    "\n",
    "# Faire plusieurs requÃªtes\n",
    "endpoints = ['users/github', 'users/microsoft', 'users/google']\n",
    "for endpoint in endpoints:\n",
    "    response = github_client.get(endpoint)\n",
    "    if response:\n",
    "        data = response.json()\n",
    "        print(f\"âœ… {data['name']}: {data['public_repos']} repos\")\n",
    "\n",
    "github_client.get_stats()"
   ],
   "id": "267dc91f8cf49a02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 API avec pagination",
   "id": "31b0a811d38da1a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_all_pages(base_url, params=None, max_pages=5):\n",
    "    \"\"\"\n",
    "    RÃ©cupÃ¨re toutes les pages d'une API paginÃ©e\n",
    "    Exemple avec l'API GitHub\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= max_pages:\n",
    "        # Ajouter la pagination aux paramÃ¨tres\n",
    "        current_params = params.copy() if params else {}\n",
    "        current_params['page'] = page\n",
    "        current_params['per_page'] = 30  # GitHub default\n",
    "\n",
    "        print(f\"ğŸ“„ RÃ©cupÃ©ration page {page}...\")\n",
    "        response = requests.get(base_url, params=current_params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ Erreur page {page}: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # Si pas de donnÃ©es, on arrÃªte\n",
    "        if not data:\n",
    "            print(f\"âœ… Fin de la pagination Ã  la page {page}\")\n",
    "            break\n",
    "\n",
    "        all_results.extend(data)\n",
    "        page += 1\n",
    "\n",
    "        # Respecter le rate limit\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Exemple : rÃ©cupÃ©rer les repos d'une organisation\n",
    "org_repos = get_all_pages(\n",
    "    'https://api.github.com/orgs/python/repos',\n",
    "    params={'type': 'public', 'sort': 'stars'},\n",
    "    max_pages=3\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“¦ {len(org_repos)} repositories rÃ©cupÃ©rÃ©s\")\n",
    "print(\"\\nTop 5 par Ã©toiles:\")\n",
    "for i, repo in enumerate(org_repos[:5], 1):\n",
    "    print(f\"{i}. {repo['name']} â­ {repo['stargazers_count']}\")"
   ],
   "id": "2b1794514e81fb51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.6 Exercice : CrÃ©er un wrapper d'API complet",
   "id": "97d36a73cd2ef8b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BookAPI:\n",
    "    \"\"\"Wrapper pour l'API Open Library\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_url = 'https://openlibrary.org'\n",
    "        self.cache = {}\n",
    "\n",
    "    def search_books(self, query, **kwargs):\n",
    "        \"\"\"Recherche de livres\"\"\"\n",
    "        endpoint = f\"{self.base_url}/search.json\"\n",
    "        params = {'q': query, **kwargs}\n",
    "\n",
    "        # Simple cache en mÃ©moire\n",
    "        cache_key = f\"search_{query}_{str(kwargs)}\"\n",
    "        if cache_key in self.cache:\n",
    "            print(\"ğŸ“¦ RÃ©sultat depuis le cache\")\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            self.cache[cache_key] = data\n",
    "            return data\n",
    "        return None\n",
    "\n",
    "    def get_book_by_isbn(self, isbn):\n",
    "        \"\"\"RÃ©cupÃ¨re un livre par ISBN\"\"\"\n",
    "        endpoint = f\"{self.base_url}/api/books\"\n",
    "        params = {\n",
    "            'bibkeys': f'ISBN:{isbn}',\n",
    "            'format': 'json',\n",
    "            'jscmd': 'data'\n",
    "        }\n",
    "\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get(f'ISBN:{isbn}')\n",
    "        return None\n",
    "\n",
    "    def get_author(self, author_id):\n",
    "        \"\"\"RÃ©cupÃ¨re les infos d'un auteur\"\"\"\n",
    "        endpoint = f\"{self.base_url}/authors/{author_id}.json\"\n",
    "        response = requests.get(endpoint)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        return None"
   ],
   "id": "27ba89e64b00147e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Utilisation\n",
    "book_api = BookAPI()\n",
    "\n",
    "# Rechercher\n",
    "results = book_api.search_books('Harry Potter', limit=1)\n",
    "if results and results['docs']:\n",
    "    book = results['docs'][0]\n",
    "    print(f\"ğŸ“– TrouvÃ©: {book.get('title')}\")\n",
    "    print(f\"   Auteur: {book.get('author_name', ['Unknown'])[0]}\")\n",
    "\n",
    "    # RÃ©cupÃ©rer par ISBN si disponible\n",
    "    if 'isbn' in book and book['isbn']:\n",
    "        isbn = book['isbn'][0]\n",
    "        print(f\"\\nğŸ” Recherche dÃ©tails pour ISBN: {isbn}\")\n",
    "        details = book_api.get_book_by_isbn(isbn)\n",
    "        if details:\n",
    "            print(f\"   Ã‰diteur: {details.get('publishers', [{}])[0].get('name', 'N/A')}\")\n",
    "            print(f\"   Pages: {details.get('number_of_pages', 'N/A')}\")"
   ],
   "id": "c5356783f9209c1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "7be95aa85310b29d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Partie 3 : Web Scraping avec BeautifulSoup {#scraping}\n",
    "\n",
    "### 4.1 Introduction au HTML et BeautifulSoup"
   ],
   "id": "fef05ac2181ca1cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# HTML d'exemple pour comprendre la structure\n",
    "html_example = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Ma Page d'Exemple</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1 class=\"main-title\">Bienvenue sur ma page</h1>\n",
    "        <div class=\"content\">\n",
    "            <p id=\"intro\">Ceci est une introduction.</p>\n",
    "            <ul class=\"list\">\n",
    "                <li class=\"item\">Premier Ã©lÃ©ment</li>\n",
    "                <li class=\"item\">DeuxiÃ¨me Ã©lÃ©ment</li>\n",
    "                <li class=\"item special\">Ã‰lÃ©ment spÃ©cial</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div class=\"footer\">\n",
    "            <p>Â© 2024 Mon Site</p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parser avec BeautifulSoup\n",
    "soup = BeautifulSoup(html_example, 'html.parser')\n",
    "\n",
    "print(\"ğŸŒ³ Structure HTML parsÃ©e\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Naviguer dans le HTML\n",
    "print(f\"Titre de la page: {soup.title.string}\")\n",
    "print(f\"H1 principal: {soup.h1.string}\")\n",
    "print(f\"Paragraphe intro: {soup.find(id='intro').string}\")\n",
    "\n",
    "# Trouver tous les Ã©lÃ©ments de liste\n",
    "print(\"\\nÃ‰lÃ©ments de la liste:\")\n",
    "for li in soup.find_all('li'):\n",
    "    classes = li.get('class', [])\n",
    "    print(f\"  - {li.string} (classes: {', '.join(classes)})\")"
   ],
   "id": "6f68b795c42a3e12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Scraping d'un vrai site web",
   "id": "e21b51be964626c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_quotes():\n",
    "    \"\"\"Scrape des citations depuis quotes.toscrape.com\"\"\"\n",
    "    url = 'http://quotes.toscrape.com/'\n",
    "\n",
    "    print(f\"ğŸ•·ï¸ Scraping de {url}\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    quotes_data = []\n",
    "\n",
    "    # Trouver toutes les citations\n",
    "    quotes = soup.find_all('div', class_='quote')\n",
    "\n",
    "    for quote in quotes:\n",
    "        # Extraire le texte\n",
    "        text = quote.find('span', class_='text').text\n",
    "\n",
    "        # Extraire l'auteur\n",
    "        author = quote.find('small', class_='author').text\n",
    "\n",
    "        # Extraire les tags\n",
    "        tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
    "\n",
    "        quotes_data.append({\n",
    "            'text': text,\n",
    "            'author': author,\n",
    "            'tags': tags\n",
    "        })\n",
    "\n",
    "    return quotes_data\n",
    "\n",
    "# Scraper les citations\n",
    "quotes = scrape_quotes()\n",
    "\n",
    "print(f\"\\nğŸ“œ {len(quotes)} citations trouvÃ©es:\\n\")\n",
    "for i, quote in enumerate(quotes[:3], 1):\n",
    "    print(f\"{i}. {quote['text'][:60]}...\")\n",
    "    print(f\"   - {quote['author']}\")\n",
    "    print(f\"   Tags: {', '.join(quote['tags'])}\")\n",
    "    print()"
   ],
   "id": "40544c34113235ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 Scraping avancÃ© : IMDB",
   "id": "525edca28dccbf6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_imdb_movies(num_movies=10):\n",
    "    \"\"\"\n",
    "    Scrape les films populaires d'IMDB\n",
    "    Note: Dans un cas rÃ©el, respectez robots.txt et les conditions d'utilisation\n",
    "    \"\"\"\n",
    "    url = 'https://www.imdb.com/chart/moviemeter/'\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    }\n",
    "\n",
    "    print(f\"ğŸ¬ Scraping IMDB: {url}\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"âŒ Erreur: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    movies = []\n",
    "\n",
    "    # Chercher le tableau des films\n",
    "    movie_table = soup.find('tbody', class_='lister-list')\n",
    "    if not movie_table:\n",
    "        print(\"âŒ Structure de la page a changÃ©\")\n",
    "        return []\n",
    "\n",
    "    movie_rows = movie_table.find_all('tr')[:num_movies]\n",
    "\n",
    "    for row in movie_rows:\n",
    "        try:\n",
    "            # Titre\n",
    "            title_column = row.find('td', class_='titleColumn')\n",
    "            title = title_column.find('a').text.strip()\n",
    "\n",
    "            # AnnÃ©e\n",
    "            year = title_column.find('span', class_='secondaryInfo').text.strip('()')\n",
    "\n",
    "            # Note\n",
    "            rating_column = row.find('td', class_='ratingColumn')\n",
    "            rating = rating_column.find('strong')\n",
    "            rating = rating.text if rating else 'N/A'\n",
    "\n",
    "            # Rang\n",
    "            rank = row.find('td', class_='posterColumn').find('span')['data-value']\n",
    "\n",
    "            movies.append({\n",
    "                'rank': rank,\n",
    "                'title': title,\n",
    "                'year': year,\n",
    "                'rating': rating\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erreur lors du parsing d'un film: {e}\")\n",
    "            continue\n",
    "\n",
    "    return movies\n",
    "\n",
    "# Note: Cette fonction est Ã  des fins Ã©ducatives\n",
    "# Toujours vÃ©rifier robots.txt et respecter les limites\n",
    "print(\"âš ï¸ Note: Ceci est un exemple Ã©ducatif.\")\n",
    "print(\"   En production, utilisez l'API IMDB ou respectez robots.txt\\n\")\n",
    "\n",
    "# Pour l'exemple, nous allons crÃ©er des donnÃ©es simulÃ©es\n",
    "mock_movies = [\n",
    "    {'rank': '1', 'title': 'The Shawshank Redemption', 'year': '1994', 'rating': '9.3'},\n",
    "    {'rank': '2', 'title': 'The Godfather', 'year': '1972', 'rating': '9.2'},\n",
    "    {'rank': '3', 'title': 'The Dark Knight', 'year': '2008', 'rating': '9.0'},\n",
    "]\n",
    "\n",
    "print(\"ğŸ¬ Top 3 films (donnÃ©es simulÃ©es):\")\n",
    "for movie in mock_movies:\n",
    "    print(f\"{movie['rank']}. {movie['title']} ({movie['year']}) - â­ {movie['rating']}\")"
   ],
   "id": "b0109c3a242a76b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.4 Techniques de navigation avancÃ©es",
   "id": "fb57c2996e8bb4c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# HTML complexe pour dÃ©monstration\n",
    "complex_html = \"\"\"\n",
    "<div class=\"container\">\n",
    "    <article class=\"post\" id=\"post1\">\n",
    "        <header>\n",
    "            <h2>Premier Article</h2>\n",
    "            <span class=\"author\">Par Alice</span>\n",
    "            <time>2024-01-15</time>\n",
    "        </header>\n",
    "        <div class=\"content\">\n",
    "            <p>Premier paragraphe de l'article.</p>\n",
    "            <p>DeuxiÃ¨me paragraphe avec <a href=\"/link1\">un lien</a>.</p>\n",
    "        </div>\n",
    "        <footer>\n",
    "            <span class=\"tags\">Python, Web Scraping</span>\n",
    "            <span class=\"comments\">5 commentaires</span>\n",
    "        </footer>\n",
    "    </article>\n",
    "\n",
    "    <article class=\"post\" id=\"post2\">\n",
    "        <header>\n",
    "            <h2>DeuxiÃ¨me Article</h2>\n",
    "            <span class=\"author\">Par Bob</span>\n",
    "            <time>2024-01-16</time>\n",
    "        </header>\n",
    "        <div class=\"content\">\n",
    "            <p>Contenu du deuxiÃ¨me article.</p>\n",
    "        </div>\n",
    "    </article>\n",
    "</div>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(complex_html, 'html.parser')\n",
    "\n",
    "print(\"ğŸ§­ Navigation avancÃ©e dans le HTML\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Naviguer avec les relations parent/enfant\n",
    "article = soup.find('article')\n",
    "header = article.find('header')\n",
    "print(f\"1. Titre de l'article: {header.h2.string}\")\n",
    "print(f\"   Parent du header: {header.parent.name}\")\n",
    "\n",
    "# 2. Naviguer entre siblings\n",
    "print(f\"\\n2. Siblings du header:\")\n",
    "for sibling in header.find_next_siblings():\n",
    "    print(f\"   - {sibling.name}: {sibling.get('class', [])}\")\n",
    "\n",
    "# 3. Recherche avec fonctions personnalisÃ©es\n",
    "def has_multiple_classes(tag):\n",
    "    \"\"\"Trouve les tags avec plusieurs classes\"\"\"\n",
    "    return tag.has_attr('class') and len(tag['class']) > 1\n",
    "\n",
    "tags_with_multiple_classes = soup.find_all(has_multiple_classes)\n",
    "print(f\"\\n3. Tags avec plusieurs classes: {len(tags_with_multiple_classes)}\")\n",
    "\n",
    "# 4. Extraction de donnÃ©es structurÃ©es\n",
    "def extract_article_data(article):\n",
    "    \"\"\"Extrait toutes les donnÃ©es d'un article\"\"\"\n",
    "    return {\n",
    "        'id': article.get('id'),\n",
    "        'title': article.find('h2').string,\n",
    "        'author': article.find('span', class_='author').text.replace('Par ', ''),\n",
    "        'date': article.find('time').string,\n",
    "        'content': ' '.join(p.get_text(strip=True) for p in article.find_all('p')),\n",
    "        'tags': article.find('span', class_='tags').text if article.find('span', class_='tags') else 'N/A'\n",
    "    }\n",
    "\n",
    "print(\"\\n4. DonnÃ©es extraites:\")\n",
    "for article in soup.find_all('article'):\n",
    "    data = extract_article_data(article)\n",
    "    print(f\"\\n   Article: {data['title']}\")\n",
    "    print(f\"   Auteur: {data['author']}\")\n",
    "    print(f\"   Date: {data['date']}\")"
   ],
   "id": "60694d3c6dcceeac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.5 Gestion des erreurs et robustesse",
   "id": "9936d90e6fdd47af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RobustScraper:\n",
    "    \"\"\"Scraper robuste avec gestion d'erreurs\"\"\"\n",
    "\n",
    "    def __init__(self, retry_count=3, delay=1):\n",
    "        self.retry_count = retry_count\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (compatible; Educational Bot)'\n",
    "        })\n",
    "\n",
    "    def fetch_page(self, url):\n",
    "        \"\"\"RÃ©cupÃ¨re une page avec retry\"\"\"\n",
    "        for attempt in range(self.retry_count):\n",
    "            try:\n",
    "                print(f\"ğŸ”„ Tentative {attempt + 1}/{self.retry_count} pour {url}\")\n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"âŒ Erreur: {e}\")\n",
    "                if attempt < self.retry_count - 1:\n",
    "                    print(f\"â³ Attente {self.delay}s avant retry...\")\n",
    "                    time.sleep(self.delay)\n",
    "                else:\n",
    "                    print(\"âŒ Ã‰chec aprÃ¨s toutes les tentatives\")\n",
    "                    return None\n",
    "\n",
    "    def safe_extract(self, soup, selector, attribute=None, default='N/A'):\n",
    "        \"\"\"Extraction sÃ©curisÃ©e d'Ã©lÃ©ments\"\"\"\n",
    "        try:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                if attribute:\n",
    "                    return element.get(attribute, default)\n",
    "                return element.get_text(strip=True)\n",
    "            return default\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erreur d'extraction pour {selector}: {e}\")\n",
    "            return default\n",
    "\n",
    "    def scrape_with_structure(self, url, structure):\n",
    "        \"\"\"\n",
    "        Scrape selon une structure dÃ©finie\n",
    "        structure = {\n",
    "            'title': {'selector': 'h1', 'attribute': None},\n",
    "            'image': {'selector': 'img.main', 'attribute': 'src'}\n",
    "        }\n",
    "        \"\"\"\n",
    "        response = self.fetch_page(url)\n",
    "        if not response:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        data = {}\n",
    "\n",
    "        for field, config in structure.items():\n",
    "            data[field] = self.safe_extract(\n",
    "                soup,\n",
    "                config['selector'],\n",
    "                config.get('attribute'),\n",
    "                config.get('default', 'N/A')\n",
    "            )\n",
    "\n",
    "        return data\n",
    "\n",
    "# Utilisation\n",
    "scraper = RobustScraper()\n",
    "\n",
    "# DÃ©finir la structure Ã  extraire\n",
    "structure = {\n",
    "    'title': {'selector': 'h1.main-title'},\n",
    "    'author': {'selector': 'span.author'},\n",
    "    'date': {'selector': 'time', 'attribute': 'datetime'},\n",
    "    'content': {'selector': 'div.content'}\n",
    "}\n",
    "\n",
    "# Test avec une URL (utiliser une vraie URL ici)\n",
    "# data = scraper.scrape_with_structure('https://example.com', structure)"
   ],
   "id": "12b312d3d6b96a3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "1607555da260d686"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Partie 4 : Projet complet - Pipeline de donnÃ©es {#project}\n",
    "\n",
    "### 5.1 Architecture du pipeline"
   ],
   "id": "bef30cc8fbd3701b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DataSource(ABC):\n",
    "    \"\"\"Classe abstraite pour les sources de donnÃ©es\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def validate(self, data):\n",
    "        pass\n",
    "\n",
    "class CSVSource(DataSource):\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"Extrait les donnÃ©es du CSV\"\"\"\n",
    "        data = []\n",
    "        with open(self.filepath, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                data.append(row)\n",
    "        return data\n",
    "\n",
    "    def validate(self, data):\n",
    "        \"\"\"Valide les donnÃ©es CSV\"\"\"\n",
    "        if not data:\n",
    "            return False\n",
    "        # VÃ©rifier que toutes les lignes ont les mÃªmes clÃ©s\n",
    "        keys = set(data[0].keys())\n",
    "        return all(set(row.keys()) == keys for row in data)\n",
    "\n",
    "class APISource(DataSource):\n",
    "    def __init__(self, url, params=None):\n",
    "        self.url = url\n",
    "        self.params = params or {}\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"Extrait les donnÃ©es de l'API\"\"\"\n",
    "        response = requests.get(self.url, params=self.params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        return []\n",
    "\n",
    "    def validate(self, data):\n",
    "        \"\"\"Valide les donnÃ©es API\"\"\"\n",
    "        return isinstance(data, (list, dict)) and len(str(data)) > 0\n",
    "\n",
    "class WebSource(DataSource):\n",
    "    def __init__(self, url, extractor_func):\n",
    "        self.url = url\n",
    "        self.extractor_func = extractor_func\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"Extrait les donnÃ©es du site web\"\"\"\n",
    "        response = requests.get(self.url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return self.extractor_func(soup)\n",
    "        return []\n",
    "\n",
    "    def validate(self, data):\n",
    "        \"\"\"Valide les donnÃ©es web\"\"\"\n",
    "        return isinstance(data, list) and len(data) > 0\n",
    "\n",
    "print(\"âœ… Classes de base dÃ©finies\")"
   ],
   "id": "401f0abc83093925"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Pipeline de transformation",
   "id": "4ee9f80ddcb776a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"Pipeline complet de traitement de donnÃ©es\"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sources = []\n",
    "        self.transformations = []\n",
    "        self.data = []\n",
    "        self.metadata = {\n",
    "            'created': datetime.now(),\n",
    "            'sources_count': 0,\n",
    "            'records_count': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "\n",
    "    def add_source(self, source):\n",
    "        \"\"\"Ajoute une source de donnÃ©es\"\"\"\n",
    "        self.sources.append(source)\n",
    "        self.metadata['sources_count'] = len(self.sources)\n",
    "        return self\n",
    "\n",
    "    def add_transformation(self, func):\n",
    "        \"\"\"Ajoute une transformation\"\"\"\n",
    "        self.transformations.append(func)\n",
    "        return self\n",
    "\n",
    "    def extract_all(self):\n",
    "        \"\"\"Extrait donnÃ©es de toutes les sources\"\"\"\n",
    "        print(f\"\\nğŸš€ DÃ©marrage pipeline: {self.name}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        all_data = []\n",
    "\n",
    "        for i, source in enumerate(self.sources, 1):\n",
    "            print(f\"\\nğŸ“¥ Source {i}/{len(self.sources)}: {source.__class__.__name__}\")\n",
    "\n",
    "            try:\n",
    "                data = source.extract()\n",
    "\n",
    "                if source.validate(data):\n",
    "                    if isinstance(data, dict):\n",
    "                        data = [data]\n",
    "\n",
    "                    all_data.extend(data)\n",
    "                    print(f\"   âœ… {len(data)} enregistrements extraits\")\n",
    "                else:\n",
    "                    error = f\"Validation Ã©chouÃ©e pour {source.__class__.__name__}\"\n",
    "                    print(f\"   âŒ {error}\")\n",
    "                    self.metadata['errors'].append(error)\n",
    "\n",
    "            except Exception as e:\n",
    "                error = f\"Erreur extraction {source.__class__.__name__}: {str(e)}\"\n",
    "                print(f\"   âŒ {error}\")\n",
    "                self.metadata['errors'].append(error)\n",
    "\n",
    "        self.data = all_data\n",
    "        self.metadata['records_count'] = len(all_data)\n",
    "        return self\n",
    "\n",
    "    def transform_all(self):\n",
    "        \"\"\"Applique toutes les transformations\"\"\"\n",
    "        print(f\"\\nğŸ”„ Application de {len(self.transformations)} transformations\")\n",
    "\n",
    "        for i, transform in enumerate(self.transformations, 1):\n",
    "            print(f\"   ğŸ“ Transformation {i}: {transform.__name__}\")\n",
    "            try:\n",
    "                self.data = transform(self.data)\n",
    "                print(f\"      âœ… SuccÃ¨s ({len(self.data)} enregistrements)\")\n",
    "            except Exception as e:\n",
    "                error = f\"Erreur transformation {transform.__name__}: {str(e)}\"\n",
    "                print(f\"      âŒ {error}\")\n",
    "                self.metadata['errors'].append(error)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def save(self, format='json', output_dir='outputs'):\n",
    "        \"\"\"Sauvegarde les donnÃ©es\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        if format == 'json':\n",
    "            filepath = f\"{output_dir}/{self.name}_{timestamp}.json\"\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'metadata': {k: str(v) for k, v in self.metadata.items()},\n",
    "                    'data': self.data\n",
    "                }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        elif format == 'csv':\n",
    "            filepath = f\"{output_dir}/{self.name}_{timestamp}.csv\"\n",
    "            if self.data:\n",
    "                with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=self.data[0].keys())\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(self.data)\n",
    "\n",
    "        print(f\"\\nğŸ’¾ DonnÃ©es sauvegardÃ©es: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"RÃ©sumÃ© du pipeline\"\"\"\n",
    "        print(f\"\\nğŸ“Š RÃ©sumÃ© du pipeline: {self.name}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Sources: {self.metadata['sources_count']}\")\n",
    "        print(f\"Enregistrements: {self.metadata['records_count']}\")\n",
    "        print(f\"Erreurs: {len(self.metadata['errors'])}\")\n",
    "\n",
    "        if self.metadata['errors']:\n",
    "            print(\"\\nâš ï¸ Erreurs rencontrÃ©es:\")\n",
    "            for error in self.metadata['errors']:\n",
    "                print(f\"   - {error}\")\n",
    "\n",
    "# DÃ©finir des transformations\n",
    "def clean_text(data):\n",
    "    \"\"\"Nettoie les champs texte\"\"\"\n",
    "    for record in data:\n",
    "        for key, value in record.items():\n",
    "            if isinstance(value, str):\n",
    "                record[key] = value.strip()\n",
    "    return data\n",
    "\n",
    "def add_timestamp(data):\n",
    "    \"\"\"Ajoute un timestamp Ã  chaque enregistrement\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    for record in data:\n",
    "        record['processed_at'] = timestamp\n",
    "    return data\n",
    "\n",
    "def remove_duplicates(data):\n",
    "    \"\"\"Supprime les doublons basÃ©s sur un hash\"\"\"\n",
    "    seen = set()\n",
    "    unique_data = []\n",
    "\n",
    "    for record in data:\n",
    "        # CrÃ©er un hash unique pour chaque enregistrement\n",
    "        record_hash = hashlib.md5(\n",
    "            json.dumps(record, sort_keys=True).encode()\n",
    "        ).hexdigest()\n",
    "\n",
    "        if record_hash not in seen:\n",
    "            seen.add(record_hash)\n",
    "            unique_data.append(record)\n",
    "\n",
    "    print(f\"      Doublons supprimÃ©s: {len(data) - len(unique_data)}\")\n",
    "    return unique_data\n",
    "\n",
    "print(\"âœ… Pipeline et transformations dÃ©finis\")"
   ],
   "id": "bd4f94e27a92939b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.3 Exemple d'utilisation complÃ¨te",
   "id": "998acdc397047cd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CrÃ©er un pipeline complet\n",
    "pipeline = DataPipeline(\"donnees_combinees\")\n",
    "\n",
    "# 1. Ajouter une source CSV\n",
    "csv_source = CSVSource('data/processed/beatles.csv')\n",
    "pipeline.add_source(csv_source)\n",
    "\n",
    "# 2. Ajouter une source API (exemple avec une API de test)\n",
    "api_source = APISource(\n",
    "    'https://jsonplaceholder.typicode.com/users',\n",
    "    params={'_limit': 5}\n",
    ")\n",
    "pipeline.add_source(api_source)\n",
    "\n",
    "# 3. Ajouter une source Web\n",
    "def extract_quotes_simple(soup):\n",
    "    \"\"\"Extracteur simple pour les citations\"\"\"\n",
    "    quotes = []\n",
    "    # Simuler l'extraction (en production, utiliser le vrai scraping)\n",
    "    quotes.append({\n",
    "        'text': 'La vie est belle',\n",
    "        'author': 'Anonyme',\n",
    "        'source': 'web'\n",
    "    })\n",
    "    return quotes\n",
    "\n",
    "web_source = WebSource('http://quotes.toscrape.com/', extract_quotes_simple)\n",
    "pipeline.add_source(web_source)\n",
    "\n",
    "# 4. Ajouter des transformations\n",
    "pipeline.add_transformation(clean_text)\n",
    "pipeline.add_transformation(add_timestamp)\n",
    "pipeline.add_transformation(remove_duplicates)\n",
    "\n",
    "# 5. ExÃ©cuter le pipeline\n",
    "pipeline.extract_all().transform_all()\n",
    "\n",
    "# 6. Sauvegarder les rÃ©sultats\n",
    "json_file = pipeline.save(format='json')\n",
    "csv_file = pipeline.save(format='csv')\n",
    "\n",
    "# 7. Afficher le rÃ©sumÃ©\n",
    "pipeline.get_summary()\n",
    "\n",
    "# 8. AperÃ§u des donnÃ©es finales\n",
    "print(\"\\nğŸ‘€ AperÃ§u des donnÃ©es finales:\")\n",
    "for i, record in enumerate(pipeline.data[:3], 1):\n",
    "    print(f\"\\nEnregistrement {i}:\")\n",
    "    for key, value in list(record.items())[:4]:  # Afficher 4 premiers champs\n",
    "        print(f\"  {key}: {value}\")"
   ],
   "id": "2a7a20e785e35863"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.4 Cache et optimisation",
   "id": "ec6b4523571027e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CachedDataSource:\n",
    "    \"\"\"Wrapper pour ajouter du cache Ã  n'importe quelle source\"\"\"\n",
    "\n",
    "    def __init__(self, source, cache_dir='cache', expire_hours=24):\n",
    "        self.source = source\n",
    "        self.cache_dir = cache_dir\n",
    "        self.expire_hours = expire_hours\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    def _get_cache_key(self):\n",
    "        \"\"\"GÃ©nÃ¨re une clÃ© de cache unique\"\"\"\n",
    "        source_str = f\"{self.source.__class__.__name__}_{self.source.__dict__}\"\n",
    "        return hashlib.md5(source_str.encode()).hexdigest()\n",
    "\n",
    "    def _get_cache_path(self):\n",
    "        \"\"\"Chemin du fichier cache\"\"\"\n",
    "        return os.path.join(self.cache_dir, f\"{self._get_cache_key()}.pkl\")\n",
    "\n",
    "    def _is_cache_valid(self, cache_path):\n",
    "        \"\"\"VÃ©rifie si le cache est encore valide\"\"\"\n",
    "        if not os.path.exists(cache_path):\n",
    "            return False\n",
    "\n",
    "        # VÃ©rifier l'Ã¢ge du cache\n",
    "        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))\n",
    "        age = datetime.now() - file_time\n",
    "        return age.total_seconds() < self.expire_hours * 3600\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"Extrait avec mise en cache\"\"\"\n",
    "        cache_path = self._get_cache_path()\n",
    "\n",
    "        # VÃ©rifier le cache\n",
    "        if self._is_cache_valid(cache_path):\n",
    "            print(f\"   ğŸ“¦ Utilisation du cache: {os.path.basename(cache_path)}\")\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "\n",
    "        # Extraire les donnÃ©es\n",
    "        print(f\"   ğŸ”„ Extraction depuis la source...\")\n",
    "        data = self.source.extract()\n",
    "\n",
    "        # Sauvegarder en cache\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def validate(self, data):\n",
    "        \"\"\"DÃ©lÃ¨gue la validation Ã  la source originale\"\"\"\n",
    "        return self.source.validate(data)\n",
    "\n",
    "# Utilisation avec cache\n",
    "print(\"ğŸ§ª Test du systÃ¨me de cache\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Source API avec cache\n",
    "api_source = APISource('https://jsonplaceholder.typicode.com/posts', {'_limit': 10})\n",
    "cached_api = CachedDataSource(api_source, expire_hours=1)\n",
    "\n",
    "# Premier appel - pas de cache\n",
    "print(\"\\n1ï¸âƒ£ Premier appel:\")\n",
    "data1 = cached_api.extract()\n",
    "print(f\"   DonnÃ©es rÃ©cupÃ©rÃ©es: {len(data1)} Ã©lÃ©ments\")\n",
    "\n",
    "# DeuxiÃ¨me appel - depuis le cache\n",
    "print(\"\\n2ï¸âƒ£ DeuxiÃ¨me appel:\")\n",
    "data2 = cached_api.extract()\n",
    "print(f\"   DonnÃ©es rÃ©cupÃ©rÃ©es: {len(data2)} Ã©lÃ©ments\")\n",
    "\n",
    "print(\"\\nâœ… Le cache fonctionne!\")"
   ],
   "id": "4e643e5928a4905c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "33e05953fce4315e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Exercices et dÃ©fis {#exercises}\n",
    "\n",
    "### DÃ©fi 1 : Analyseur CSV avancÃ©"
   ],
   "id": "5a54bde072585819"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: ImplÃ©menter cette classe\n",
    "class AdvancedCSVAnalyzer:\n",
    "    \"\"\"\n",
    "    Votre mission : CrÃ©er un analyseur CSV qui peut :\n",
    "    1. DÃ©tecter automatiquement le dÃ©limiteur (,;|\\t)\n",
    "    2. DÃ©tecter l'encodage du fichier\n",
    "    3. GÃ©rer les fichiers avec ou sans en-tÃªtes\n",
    "    4. Fournir des statistiques sur chaque colonne\n",
    "    5. DÃ©tecter les types de donnÃ©es\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        # TODO: ImplÃ©menter\n",
    "\n",
    "    def detect_delimiter(self):\n",
    "        \"\"\"DÃ©tecte automatiquement le dÃ©limiteur utilisÃ©\"\"\"\n",
    "        # TODO: Lire quelques lignes et analyser\n",
    "        pass\n",
    "\n",
    "    def detect_encoding(self):\n",
    "        \"\"\"DÃ©tecte l'encodage du fichier\"\"\"\n",
    "        # TODO: Essayer diffÃ©rents encodages\n",
    "        pass\n",
    "\n",
    "    def analyze_columns(self):\n",
    "        \"\"\"Analyse chaque colonne (type, valeurs uniques, etc.)\"\"\"\n",
    "        # TODO: Parcourir et analyser\n",
    "        pass\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"GÃ©nÃ¨re un rapport complet sur le fichier\"\"\"\n",
    "        # TODO: Compiler toutes les analyses\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# analyzer = AdvancedCSVAnalyzer('data/votre_fichier.csv')\n",
    "# report = analyzer.generate_report()\n",
    "# print(report)"
   ],
   "id": "9774d0ea83a58129"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DÃ©fi 2 : Multi-API Aggregator",
   "id": "a29ecddad059626b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: ImplÃ©menter\n",
    "class MultiAPIAggregator:\n",
    "    \"\"\"\n",
    "    Votre mission : CrÃ©er un agrÃ©gateur qui peut :\n",
    "    1. Interroger plusieurs APIs en parallÃ¨le\n",
    "    2. Normaliser les formats de rÃ©ponse diffÃ©rents\n",
    "    3. Fusionner les rÃ©sultats intelligemment\n",
    "    4. GÃ©rer les erreurs par API\n",
    "    5. Respecter les rate limits de chaque API\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.apis = {}\n",
    "        # TODO: ImplÃ©menter\n",
    "\n",
    "    def register_api(self, name, config):\n",
    "        \"\"\"Enregistre une nouvelle API\"\"\"\n",
    "        # config = {\n",
    "        #     'base_url': '...',\n",
    "        #     'rate_limit': 60,  # requÃªtes par minute\n",
    "        #     'normalizer': function,  # pour normaliser les rÃ©ponses\n",
    "        # }\n",
    "        pass\n",
    "\n",
    "    def query_all(self, search_term):\n",
    "        \"\"\"Interroge toutes les APIs enregistrÃ©es\"\"\"\n",
    "        # TODO: Utiliser threading ou asyncio\n",
    "        pass\n",
    "\n",
    "    def merge_results(self, results):\n",
    "        \"\"\"Fusionne intelligemment les rÃ©sultats\"\"\"\n",
    "        # TODO: DÃ©dupliquer, scorer, trier\n",
    "        pass\n",
    "\n",
    "# Exemple d'utilisation attendu :\n",
    "# aggregator = MultiAPIAggregator()\n",
    "# aggregator.register_api('github', {...})\n",
    "# aggregator.register_api('gitlab', {...})\n",
    "# results = aggregator.query_all('python scraping')"
   ],
   "id": "c495600c7f7954df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DÃ©fi 3 : Smart Web Scraper",
   "id": "31bf82856375b688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: ImplÃ©menter\n",
    "class SmartWebScraper:\n",
    "    \"\"\"\n",
    "    Votre mission : CrÃ©er un scraper intelligent qui peut :\n",
    "    1. DÃ©tecter automatiquement la structure d'une page\n",
    "    2. Identifier les patterns de donnÃ©es (listes, tableaux, etc.)\n",
    "    3. S'adapter aux changements de structure\n",
    "    4. GÃ©nÃ©rer des sÃ©lecteurs CSS optimaux\n",
    "    5. Exporter dans plusieurs formats\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        # TODO: ImplÃ©menter\n",
    "\n",
    "    def analyze_structure(self):\n",
    "        \"\"\"Analyse la structure de la page\"\"\"\n",
    "        # TODO: Identifier les patterns rÃ©pÃ©titifs\n",
    "        pass\n",
    "\n",
    "    def generate_selectors(self):\n",
    "        \"\"\"GÃ©nÃ¨re automatiquement les meilleurs sÃ©lecteurs\"\"\"\n",
    "        # TODO: Trouver les sÃ©lecteurs les plus stables\n",
    "        pass\n",
    "\n",
    "    def extract_data(self):\n",
    "        \"\"\"Extrait les donnÃ©es identifiÃ©es\"\"\"\n",
    "        # TODO: Utiliser les sÃ©lecteurs gÃ©nÃ©rÃ©s\n",
    "        pass\n",
    "\n",
    "    def monitor_changes(self):\n",
    "        \"\"\"Monitore les changements de structure\"\"\"\n",
    "        # TODO: Comparer avec version prÃ©cÃ©dente\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# scraper = SmartWebScraper('https://example.com/products')\n",
    "# scraper.analyze_structure()\n",
    "# data = scraper.extract_data()"
   ],
   "id": "695b5dadb7d66b45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Challenge Final : Pipeline de veille concurrentielle",
   "id": "16ac5c20863e9c83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "CHALLENGE FINAL : CrÃ©er un systÃ¨me complet de veille concurrentielle\n",
    "\n",
    "Objectif : Surveiller automatiquement les prix/produits de plusieurs sites\n",
    "\n",
    "FonctionnalitÃ©s requises :\n",
    "1. Configuration par fichier YAML/JSON\n",
    "2. Support de multiples sources (API, Web, CSV)\n",
    "3. DÃ©tection automatique des changements\n",
    "4. Alertes (email, webhook, etc.)\n",
    "5. Dashboard de visualisation\n",
    "6. Historique des donnÃ©es\n",
    "7. Export automatique\n",
    "\n",
    "Structure suggÃ©rÃ©e :\n",
    "competitive_intelligence/\n",
    "â”œâ”€â”€ config/\n",
    "â”‚   â””â”€â”€ sources.yaml\n",
    "â”œâ”€â”€ extractors/\n",
    "â”‚   â”œâ”€â”€ api_extractor.py\n",
    "â”‚   â”œâ”€â”€ web_extractor.py\n",
    "â”‚   â””â”€â”€ csv_extractor.py\n",
    "â”œâ”€â”€ processors/\n",
    "â”‚   â”œâ”€â”€ normalizer.py\n",
    "â”‚   â”œâ”€â”€ comparator.py\n",
    "â”‚   â””â”€â”€ alerting.py\n",
    "â”œâ”€â”€ storage/\n",
    "â”‚   â”œâ”€â”€ database.py\n",
    "â”‚   â””â”€â”€ cache.py\n",
    "â”œâ”€â”€ dashboard/\n",
    "â”‚   â””â”€â”€ visualizer.py\n",
    "â””â”€â”€ main.py\n",
    "\n",
    "Bonus :\n",
    "- Interface web (Flask/Streamlit)\n",
    "- Scheduling automatique (cron)\n",
    "- Machine Learning pour prÃ©dictions\n",
    "- API REST pour accÃ©der aux donnÃ©es\n",
    "\"\"\"\n",
    "\n",
    "# Commencez ici :\n",
    "class CompetitiveIntelligence:\n",
    "    def __init__(self, config_file):\n",
    "        self.config = self.load_config(config_file)\n",
    "        self.sources = []\n",
    "        self.processors = []\n",
    "        self.storage = None\n",
    "\n",
    "    def load_config(self, config_file):\n",
    "        \"\"\"Charge la configuration depuis un fichier\"\"\"\n",
    "        # TODO: ImplÃ©menter\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Lance le pipeline complet\"\"\"\n",
    "        # TODO: ImplÃ©menter\n",
    "        pass\n",
    "\n",
    "# Bonne chance ! ğŸš€"
   ],
   "id": "8fe98df3c449a2d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "ebc50a09456cbc38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ğŸ“š Ressources et conseils finaux\n",
    "\n",
    "### Meilleures pratiques Ã  retenir\n",
    "\n",
    "1. **Toujours respecter les sites web**\n",
    "   - VÃ©rifier robots.txt\n",
    "   - Ajouter des dÃ©lais entre requÃªtes\n",
    "   - Utiliser un User-Agent descriptif\n",
    "\n",
    "2. **GÃ©rer les erreurs gracieusement**\n",
    "   - Try/except partout\n",
    "   - Logging dÃ©taillÃ©\n",
    "   - Retry avec backoff exponentiel\n",
    "\n",
    "3. **Optimiser les performances**\n",
    "   - Cache intelligent\n",
    "   - RequÃªtes asynchrones quand possible\n",
    "   - Batch processing\n",
    "\n",
    "4. **Maintenir la qualitÃ© des donnÃ©es**\n",
    "   - Validation Ã  chaque Ã©tape\n",
    "   - Tests unitaires\n",
    "   - Monitoring continu\n",
    "\n",
    "### Prochaines Ã©tapes\n",
    "\n",
    "1. **Approfondir les bases de donnÃ©es**\n",
    "   - SQLite pour le stockage local\n",
    "   - PostgreSQL pour la production\n",
    "   - MongoDB pour les donnÃ©es non-structurÃ©es\n",
    "\n",
    "2. **Explorer les outils avancÃ©s**\n",
    "   - Scrapy pour le web scraping industriel\n",
    "   - Selenium pour les sites JavaScript\n",
    "   - Apache Airflow pour l'orchestration\n",
    "\n",
    "3. **Apprendre l'analyse de donnÃ©es**\n",
    "   - Pandas pour la manipulation\n",
    "   - Matplotlib/Seaborn pour la visualisation\n",
    "   - Scikit-learn pour le ML\n",
    "\n",
    "### Commandes utiles\n"
   ],
   "id": "b9d313ca50e71717"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# VÃ©rifier robots.txt\n",
    "curl https://example.com/robots.txt\n",
    "\n",
    "# Tester une API\n",
    "curl -X GET \"https://api.example.com/endpoint\" -H \"accept: application/json\"\n",
    "\n",
    "# Monitorer les requÃªtes\n",
    "netstat -an | grep :80\n",
    "\n",
    "# Analyser un fichier CSV\n",
    "head -n 10 file.csv | column -t -s ','"
   ],
   "id": "92d26c3a352a1364"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "df8f4f25bd9b862"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ğŸ‰ **FÃ©licitations !** Vous avez maintenant toutes les bases pour devenir un expert en Data Sourcing !\n",
    "\n",
    "N'oubliez pas : la pratique est la clÃ©. Commencez par de petits projets et augmentez progressivement la complexitÃ©.\n",
    "\n",
    "Bon code ! ğŸâœ¨"
   ],
   "id": "b02f520f0dfb461"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
