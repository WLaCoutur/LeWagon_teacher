{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "_w:31252:1748896599\n",
    "Generative AI for Data Science\n",
    "Structure\n",
    "\n",
    "What is Generative AI?\n",
    "How can you use it?\n",
    "Going Beyond ChatGPT: API & Functions\n",
    "Langchain & Beyond: Using LLMs in Applications\n",
    "Shortcomings\n",
    "Further Reading\n",
    "The notebook to replicate this lecture's tutorials can be found in today's \"Challenge\" on Kitt.\n",
    "\n",
    "1. What is Generative AI?\n",
    "\n",
    "\n",
    "Gen AI is an umbrella term\n",
    "\n",
    "Model is trained\n",
    "(Optional) Model fine-tuned\n",
    "Inference is run\n",
    "Images, text, sound\n",
    "\n",
    "Gen AI is an umbrella term\n",
    "\n",
    "Model is trained\n",
    "(Optional) Model fine-tuned\n",
    "Inference is run\n",
    "Images, text, sound\n",
    "\n",
    "\n",
    "2. How can you use it?\n",
    "\n",
    "\n",
    "The simplest (and most widely known) way to interact with high-quality generative AI is through ChatGPT:\n",
    "\n",
    "Trained on a vast amount of data\n",
    "175+ billion trained parameters\n",
    "700,000 dollars inference/ day (on top of 2-5 million dollars estimated cost for each training)\n",
    "\n",
    "\n",
    "Pre-trained vs fine-tuning vs from-scratch\n",
    "\n",
    "\n",
    "\n",
    "Prompt engineering:\n",
    "\n",
    "Some key points:\n",
    "\n",
    "Using role-playing\n",
    "Being specific in the task\n",
    "Highlighting inputs and specifying outputs\n",
    "\"Zero-shot\" vs \"few-shot\"\n",
    "Using Chain-of-thought prompting\n",
    "3. Going beyond ChatGPT\n",
    "The OpenAI API\n",
    "\n",
    "The OpenAI API\n",
    "\n",
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai_api_key = 'api-key-here'\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "openai.api_key = openai_api_key\n",
    "# Prompt for the AI model\n",
    "prompt = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "\n",
    "# Make a request to the API to generate text\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Use the engine of your choice\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens = 50\n",
    ")\n",
    "response.choices[0].message.content\n",
    "System prompts\n",
    "\n",
    "# Prompt for the AI model\n",
    "prompt = \"Give instructions to cook vegetable samosas\"\n",
    "\n",
    "# Make a request to the API to generate text\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Use the engine of your choice\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a sassy culinary instructor that gives sarcastic replies\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens = 50\n",
    ")\n",
    "response.choices[0].message.content\n",
    "Function calling: Imagine a function we might write\n",
    "\n",
    "Function calling: Imagine a function we might write\n",
    "\n",
    "def get_current_weather(location, unit):\n",
    "    ### A request is made to an API with a specific format\n",
    "    ### returns some result\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"I'm interested in the weather in Bozeman. I'm old-school so like it in F?\"}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city with its accompanying state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\",\n",
    "                         \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "],\n",
    "function_call=\"auto\",\n",
    ")\n",
    "completion.choices[0].message.function_call.arguments\n",
    "A practical example\n",
    "\n",
    "A practical example\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/deep_learning_datasets/results.csv\")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me about matches that took place in Italy between 1980 up until the end of the 20th century\"}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"get_matches\",\n",
    "        \"description\": \"Return the rows in a DataFrame about women's football games which satisfy the criteria\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"country\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the country the matches took place e.g. France or China\",\n",
    "                },\n",
    "                \"start_year\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The year to begin filtering from e.g. 1956\",\n",
    "                },\n",
    "                \"end_year\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The year to end filtering on e.g. 2005\"}\n",
    "            },\n",
    "            \"required\": [\"location\", \"start_year\", \"end_year\"],\n",
    "        },\n",
    "    }\n",
    "],\n",
    "function_call=\"auto\",\n",
    ")\n",
    "args = json.loads(completion.choices[0].message.function_call.arguments)\n",
    "\n",
    "print(args)\n",
    "def matches_finder(country: str, start_year: int, end_year: int):\n",
    "    return df.loc[\n",
    "        (df[\"country\"] == country) &\n",
    "        (start_year <= df[\"date\"].dt.year) &\n",
    "        (df[\"date\"].dt.year <= end_year)\n",
    "    ]\n",
    "def matches_finder(country: str, start_year: int, end_year: int):\n",
    "    return df.loc[\n",
    "        (df[\"country\"] == country) &\n",
    "        (start_year <= df[\"date\"].dt.year) &\n",
    "        (df[\"date\"].dt.year <= end_year)\n",
    "    ]\n",
    "matches_finder(**args)\n",
    "4. Langchain and Beyond:\n",
    "\n",
    "\n",
    "How can I work with larger amounts of data?\n",
    "\n",
    "\n",
    "How can I work with larger amounts of data?\n",
    "\n",
    "We saw in the Transformers lecture how tricky it is to have large context windows (a.k.a. sequence length)\n",
    "ChatGPT and other models have ~32k tokens max\n",
    "Does that mean that we can only ever work with documents <32k tokens ðŸ¥º?\n",
    "\n",
    "We can use a Vector DataBase to store our embeddings ðŸ’ª\n",
    "\n",
    "\n",
    "\n",
    "We can use services like Open AI's embeddings API to convert large documents into vector representations and then store them ðŸ’ª\n",
    "\n",
    "\n",
    "We can use services like Open AI's embeddings API to convert large documents into vector representations and then store them ðŸ’ª\n",
    "\n",
    "model = \"text-embedding-ada-002\"\n",
    "\n",
    "embedding = openai.embeddings.create(input=[\"\"\"This is a simple embedding of a sentence\"\"\"],\n",
    "                                     model=model)\n",
    "\n",
    "We can use services like Open AI's embeddings API to convert large documents into vector representations and then store them ðŸ’ª\n",
    "\n",
    "model = \"text-embedding-ada-002\"\n",
    "\n",
    "embedding = openai.embeddings.create(input=[\"\"\"This is a simple embedding of a sentence\"\"\"],\n",
    "                                     model=model)\n",
    "How large are the embeddings we got?\n",
    "\n",
    "We can use services like Open AI's embeddings API to convert large documents into vector representations and then store them ðŸ’ª\n",
    "\n",
    "model = \"text-embedding-ada-002\"\n",
    "\n",
    "embedding = openai.embeddings.create(input=[\"\"\"This is a simple embedding of a sentence\"\"\"],\n",
    "                                     model=model)\n",
    "How large are the embeddings we got?\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.array(embedding[\"data\"][0][\"embedding\"]).shape\n",
    "How can we go about tackling larger documents?\n",
    "\n",
    "How can we go about tackling larger documents?\n",
    "\n",
    "! wget -O book.pdf \"https://greenteapress.com/thinkpython2/thinkpython2.pdf\"\n",
    "How can we go about tackling larger documents?\n",
    "\n",
    "! wget -O book.pdf \"https://greenteapress.com/thinkpython2/thinkpython2.pdf\"\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"book.pdf\")\n",
    "\n",
    "data = loader.load()\n",
    "How can we go about tackling larger documents?\n",
    "\n",
    "! wget -O book.pdf \"https://greenteapress.com/thinkpython2/thinkpython2.pdf\"\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"book.pdf\")\n",
    "\n",
    "data = loader.load()\n",
    "print (f'You have {len(data)} documents in your data')\n",
    "print (f'''There are ~{np.mean([len(x.page_content) for x in data])} characters per document''')\n",
    "How could we split our documents up?\n",
    "\n",
    "How could we split our documents up?\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=400)\n",
    "\n",
    "texts = text_splitter.split_documents(data)\n",
    "Storing them in a Vector DataBase\n",
    "\n",
    "Storing them in a Vector DataBase\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "Storing them in a Vector DataBase\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "vector_db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query = \"How do I establish a Class?\"\n",
    "docs = vector_db.similarity_search(query, k = 5)\n",
    "Can we go even further?\n",
    "\n",
    "\n",
    "Can we go even further?\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "Can we go even further?\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=api_key)\n",
    "chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "Can we go even further?\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=api_key)\n",
    "chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
    "ðŸ”Ž A note on temperature and on \"map_reduce\"!\n",
    "\n",
    "query = \"How does the author recommend I keep studying after the book?\"\n",
    "docs = vector_db.similarity_search(query, k=1)\n",
    "query = \"How does the author recommend I keep studying after the book?\"\n",
    "docs = vector_db.similarity_search(query, k=1)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "Running LLMs locally/ privately\n",
    "Why might you need to do this?\n",
    "\n",
    "Data privacy\n",
    "Fine-tuning on specific datasets\n",
    "We can even download quantized (reduced) versions of very large models from HuggingFace ðŸ˜®\n",
    "\n",
    "Why Quantize?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Why Quantize?\n",
    "\n",
    "Assuming weights are stored in 32-bit float format:\n",
    "\n",
    "1 model parameter = 4 bytes\n",
    "\n",
    "1 billion parameters = 4 x 1,000,000,000 bytes = 4 GB (not even counting optimizer, gradient and activation info)\n",
    "\n",
    "Many cutting edge models (Falcon, Llama, GPT 4) easily break 70 billion trainable parameters ðŸ¤¯\n",
    "\n",
    "output = llm(\"Q: How large is the earth's diameter? A: \",\n",
    "             max_tokens=200,\n",
    "             echo=True)\n",
    "output[\"choices\"][0][\"text\"]\n",
    "Running multi-modal models yourself (Colab recommended)\n",
    "\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\") # For use w/ a GPU in colab\n",
    "prompt = \"A Renaissance painting of the Eiffel tower\"\n",
    "pipeline(prompt, num_inference_steps=30).images[0]\n",
    "5. Shortcomings\n",
    "Bias in the model\n",
    "Reliance on LLMs for labelling\n",
    "Reliability (even with the Functions API)\n",
    "Recency of data\n",
    "Confidence intervals (or lack thereof)\n",
    "More in Ethics & AI!\n",
    "6. Further Reading\n",
    "OpenAI API Docs: Filled with code examples to use\n",
    "Andrew Ng's Prompt Engineering for Developers: Excellent, free 1-hour course\n",
    "Full list of Deeplearning.ai courses: Build on many of the use cases mentioned in this lecture\n",
    "RSS Data Science and AI Newsletter: Monthly updates on latest tools\n",
    "HuggingFace Blog Post on QLora"
   ],
   "id": "4583de0ced5c128f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
