{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Explainable AI\n",
    "What we will cover today:\n",
    "XAI or Explainable AI\n",
    "Integration in Machine Learning\n",
    "Intuition about Shapley Values\n",
    "Shapley values using the SHAP package\n",
    "Local explainability\n",
    "Global explainability\n",
    "Explaining image classification\n",
    "1Ô∏è‚É£ XAI or Explainable AI\n",
    "Why do we need XAI?\n",
    "Try explaining to non-data-scientists...\n",
    "... a linear regression ü§î\n",
    "\n",
    "^\n",
    "y\n",
    "=\n",
    "Œ≤\n",
    "0\n",
    "+\n",
    "Œ≤\n",
    "1\n",
    "x\n",
    "1\n",
    "+\n",
    "Œ≤\n",
    "2\n",
    "x\n",
    "2\n",
    "+\n",
    "‚ãØ\n",
    "... a decision tree üò¨\n",
    "\n",
    "... a Random Forest model üòµ‚Äçüí´\n",
    "\n",
    "... a deep learning model ü§Ø\n",
    "\n",
    "Where do we need XAI?\n",
    "Can I get a loan?\n",
    "\n",
    "What drives predictions?\n",
    "\n",
    "\n",
    "Examples of application areas\n",
    "Model validation\n",
    "Model debugging\n",
    "Knowledge discovery\n",
    "Legal requirements:\n",
    "üá™üá∫ GDPR limitations on automated decision making based on personal data without human intervention\n",
    "Interpretability vs explainability\n",
    "Interpretability\n",
    "Understand exactly why and how the model is generating predictions\n",
    "You need to observe the inner mechanics of the model\n",
    "Explainability\n",
    "Explain the behavior of the model in human terms\n",
    "With complex models you cannot fully understand what lead to the prediction\n",
    "But you can use techniques to discover meaning between input data and model outputs\n",
    "üîó Interpretability versus explainability - AWS\n",
    "2Ô∏è‚É£ Integration in ML\n",
    "Where does XAI fit in the ML solution?\n",
    "\n",
    "\n",
    "Approaches\n",
    "Intrinsic explainable models\n",
    "The model parameters directly explain how the features contribute to the prediction\n",
    "Typical examples: linear regression, decision tree, KNN\n",
    "Agnostic approach:\n",
    "The ML model is a black box: XAI does not assume any knowledge of the model used\n",
    "It only relies on inputs and predictions of the model\n",
    "Typical use: random forest, RNN\n",
    "Model-dependent approach\n",
    "Knowledge of the ML model is used to produce explanations\n",
    "Typical examples: an agnostic approach being optimized (for speed) for certain ML models\n",
    "Global vs local explanation\n",
    "\n",
    "Source: Model Explainability ‚Äî How to choose the right tool? ‚Äî Mateusz Garbacz\n",
    "3Ô∏è‚É£ Shapley values ‚Äî intuition\n",
    "Foundations in game theory\n",
    "Game theory is the study of mathematical models of strategic interactions among rational agents\n",
    "Game theory\tNon-cooperative games\tCooperative games\n",
    "\n",
    "John von Neumann\n",
    "(1903-1957)\tJohn Nash\n",
    "(1928-2015)\tLloyd Shapley\n",
    "(1923-2016)\n",
    "On the Theory of Games of Strategy (1928)\tNon-cooperative games (1950)\tNotes on the n-Person\n",
    "Game -- II: The Value\n",
    "of an n-Person Game (1950)\n",
    "Nobel Prize in Economic Sciences (1994)\tNobel Prize in Economic Sciences (2012)\n",
    "Cooperative games\n",
    "\n",
    "From game theory to machine learning\n",
    "\n",
    "\n",
    "* Actually the difference between the prediction and the average prediction\n",
    "Intuition\n",
    "\n",
    "Shapley values\n",
    "Imagine a dataset with\n",
    "n\n",
    "features\n",
    "x\n",
    "0\n",
    ",\n",
    "x\n",
    "1\n",
    ",\n",
    "‚Ä¶\n",
    ",\n",
    "x\n",
    "j\n",
    ",\n",
    "‚Ä¶\n",
    ",\n",
    "x\n",
    "n\n",
    "Now, take a particular instance (i.e. one row in your data)\n",
    "Each feature\n",
    "x\n",
    "j\n",
    "contributes to the prediction of this particular instance\n",
    "The Shapley value\n",
    "œï\n",
    "j\n",
    "(\n",
    "i\n",
    "n\n",
    "s\n",
    "t\n",
    "a\n",
    "n\n",
    "c\n",
    "e\n",
    ")\n",
    "=\n",
    "the contribution of feature value j for this instance\n",
    "For a specific instance, there are as many Shapley values as there are features\n",
    "Shapley values are specific for an instance\n",
    "A different instance, will have different Shapley values\n",
    "Prediction for x\n",
    "= Average prediction\n",
    "+ sum of Shapley values for each contribution\n",
    "How are Shapley values calculated?\n",
    "Let's calculate the Shapley value for one feature\n",
    "j\n",
    "for one particular instance\n",
    "\n",
    "Let's build all the coalitions of features, and look at feature\n",
    "j\n",
    "'s contribution to the prediction\n",
    "The simplest coalition, is the empty one ...\n",
    "\n",
    "We compare the coalition with and without feature\n",
    "j\n",
    "We calculate both predictions and take the difference\n",
    "... followed by a coalition of only one feature ...\n",
    "\n",
    "Again, we compare the coalition with and without feature\n",
    "j\n",
    "Again, we calculate both predictions and take the difference\n",
    "... and so on\n",
    "\n",
    "Each time we compare the coalition with and without feature\n",
    "j\n",
    "Each time we calculate both predictions and take the difference\n",
    "... and finally we calculate average of these differences = the Shapley value\n",
    "ü§Ø  Wait a minute...\n",
    "We can't just drop a feature from our model, right‚ùì\n",
    "ü™Ñ We need to simulate the exclusion of a feature\n",
    "üí° To do so we make multiple predictions\n",
    "üëâ Each time we replace the excluded feature by a randomly drawn value üé≤\n",
    "ü§ì Want to know more? üîó\n",
    "4Ô∏è‚É£ Shapley in practice with the SHAP package\n",
    "SHAP\n",
    "\n",
    "SHAP or Shapley Additive exPlanations is a Python package\n",
    "Basically, it generates a local explaining model for a chosen instance\n",
    "It explains a single prediction through a linear combination (üëâ additive)\n",
    "of the underlying Shapley values\n",
    "SHAP also offers global explainability extensions\n",
    "üîó Documentation\n",
    "Let's put it into practice with üè†üè†üè† and XGBoost üöÄ\n",
    "# Our classic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost\n",
    "# !pip install shap\n",
    "import shap\n",
    "# Run the following command. Needed for some visualizations.\n",
    "shap.initjs();\n",
    "\n",
    "Fit a model\n",
    "We are using California house prices from 1990, in $100.000\n",
    "# Load the data\n",
    "X, y = shap.datasets.california()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# Fit our model\n",
    "model_tree = xgboost.XGBRegressor(n_estimators=100, max_depth=2)\n",
    "model_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_test_pred = model_tree.predict(X_test)\n",
    "Let's have a look at the predictions\n",
    "# The average house price (in $100.000)\n",
    "print(f\"The average house price in the train set is {y_train.mean():.3f}\")\n",
    "\n",
    "\n",
    "# Have a look at the mean squared error on the test set\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"The Mean Squared Error on the test set is   {mse:.3f}\")\n",
    "The average house price in the train set is 2.067\n",
    "The Mean Squared Error on the test set is   0.292\n",
    "# Let's check our predictions on the test set visually\n",
    "plt.scatter(y_test, y_test_pred)\n",
    "plt.title(\"Prediction vs true value\")\n",
    "plt.xlabel(\"y_test\"); plt.ylabel(\"y_test_predict\");\n",
    "\n",
    "5Ô∏è‚É£ Local explainability\n",
    "Select an instance, and calculate the prediction\n",
    "row_to_show = 24  # Select one instance to explain\n",
    "feature_values = X_test.iloc[[row_to_show]]\n",
    "prediction = model_tree.predict(feature_values)\n",
    "print(f\"{'The prediction for this instance:':<35}{prediction[0]:>7.3f}\")\n",
    "The prediction for this instance:    1.558\n",
    "And calculate the SHAP values\n",
    "# Step 1: Create a SHAP Explainer\n",
    "explainer = shap.Explainer(model_tree)\n",
    "# Step 2: Calculate the SHAP values\n",
    "# Using the explainer we just created\n",
    "# and giving as input the feature values for our instance\n",
    "shap_values_one = explainer(feature_values)\n",
    "# Have a look at these values\n",
    "print(f\"{'Base value:':<35}{shap_values_one.base_values[0]:>7.3f}\")\n",
    "print(f\"{'Sum of SHAP values:':<35}{shap_values_one.values.sum():>7.3f}\")\n",
    "print(f\"{'The prediction for this instance:':<35}{prediction[0]:>7.3f}\")\n",
    "Base value:                          2.067\n",
    "Sum of SHAP values:                 -0.509\n",
    "The prediction for this instance:    1.558\n",
    "Now that we have our SHAP values, let's visualize them\n",
    "shap.plots.bar(shap_values_one[0])\n",
    "\n",
    "shap.plots.waterfall(shap_values_one[0])\n",
    "\n",
    "shap.plots.force(shap_values_one[0])\n",
    "0.6674\n",
    "0.8674\n",
    "1.067\n",
    "1.267\n",
    "1.467\n",
    "1.667\n",
    "1.867\n",
    "2.067\n",
    "2.267\n",
    "2.467\n",
    "2.667\n",
    "2.867\n",
    "3.067\n",
    "3.267\n",
    "3.467\n",
    "AveBedrms = 1.104\n",
    "Latitude = 33.95\n",
    "MedInc = 2.203\n",
    "AveOccup = 4.035\n",
    "AveRooms = 4.413\n",
    "Longitude = -118.3\n",
    "base value\n",
    "1.56\n",
    "1.56\n",
    "higher\n",
    "‚Üí\n",
    "f(x)\n",
    "‚Üê\n",
    "lower\n",
    "6Ô∏è‚É£ Global explainability\n",
    "# Let's calculate SHAP values for our whole test population\n",
    "# We can use the same explainer\n",
    "# This time we feed it our whole test population\n",
    "\n",
    "shap_values = explainer(X_test)\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "Or as a beeswarm plot\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "The SHAP package has even more visualisations\n",
    "üëâ Have a look at the üîó docs\n",
    "7Ô∏è‚É£ What about images?\n",
    "SHAP also works for images ü§©\n",
    "Preparing a model\n",
    "Let's use a pretrained ResNet50 model\n",
    "# !pip install opencv-python\n",
    "import requests\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "# Loading the class names from ImageNet 1000\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "class_names = requests.get(url).json().values()\n",
    "class_names = [value[1] for value in class_names]\n",
    "# Load the pre-trained model and some sample data\n",
    "model_deep = ResNet50(weights='imagenet')\n",
    "X, y = shap.datasets.imagenet50()\n",
    "X = X.astype(int)\n",
    "Obtaining the SHAP values\n",
    "# Function to preprocess the data and get the model output\n",
    "# We will use this as input for the SHAP explainer\n",
    "def model(X):\n",
    "    X_copy = X.copy()\n",
    "    X_copy = preprocess_input(X_copy)\n",
    "    return model_deep(X_copy)\n",
    "\n",
    "# A masker that will mask out partitions of the input image\n",
    "masker = shap.maskers.Image(\"blur(128,128)\", X[0].shape)\n",
    "\n",
    "# Finally create the explainer\n",
    "explainer = shap.Explainer(model, masker, output_names=class_names)\n",
    "\n",
    "# Explain some images using 500 evaluations of the model\n",
    "# to estimate the SHAP values\n",
    "shap_values = explainer(X[1:5], max_evals=500, batch_size=50,\n",
    "                        outputs=shap.Explanation.argsort.flip[:4],\n",
    "                        silent=True)\n",
    "shap.image_plot(shap_values, pixel_values=X[1:5])\n",
    "\n",
    "Further reading üìö:\n",
    "Explainable AI with Python ‚Äî Leonida Gianfagna, Antonio Di Cecco\n",
    "\n",
    "Interpretable Machine Learning ‚Äî A Guide for Making Black Box Models Explainable ‚Äî Christoph Molnar\n",
    "\n",
    "SHAP package documentation\n"
   ],
   "id": "74f3162ca2b0de8c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
