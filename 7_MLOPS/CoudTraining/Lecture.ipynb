{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Training in the Cloud\n",
    "Plan for the lecture\n",
    "1ï¸âƒ£ Reminders\n",
    "2ï¸âƒ£ Objective\n",
    "3ï¸âƒ£ Cloud platform\n",
    "4ï¸âƒ£ Application parameters\n",
    "5ï¸âƒ£ Model in the cloud\n",
    "6ï¸âƒ£ Data in the cloud\n",
    "7ï¸âƒ£ Training in the cloud\n",
    "1ï¸âƒ£ Reminders\n",
    "Minimal package structure\n",
    "\n",
    "\n",
    "Package installation\n",
    "\n",
    "2ï¸âƒ£ Objective\n",
    "Where are we in our journey?\n",
    "\n",
    "ğŸ”­ Build the WagonCab app ğŸš— from the notebook provided by the Data Science team ğŸ§‘â€ğŸ”¬\n",
    "\n",
    "âœ… Analyze ğŸ”¬ the notebook\n",
    "âœ… Convert the notebook into a Python package ğŸ“¦ to make its code operable â™»ï¸\n",
    "âœ… Scale ğŸ—» the code to train the package on the full dataset\n",
    "\n",
    "What's the next step?\n",
    "\n",
    "ğŸ¯ We want our package ğŸ“¦ to work in the cloud:\n",
    "Allow team members to collaborate\n",
    "Stop monopolizing our machine during training\n",
    "Plug â™»ï¸ constantly evolving, real world data and production processes\n",
    "Train on a virtual machine ğŸ§® with a GPU\n",
    "\n",
    "Transition\n",
    "\n",
    "â“ How to decouple the package from our machine ğŸ’»\n",
    "Change the output: save the trained model to the cloud\n",
    "Change the input: train from data in the cloud\n",
    "Change the execution: run the training on a virtual machine in the cloud\n",
    "\n",
    "\n",
    "Updated Package structure\n",
    ".\n",
    "â”œâ”€â”€ .env                            # âš™ï¸ Single source of config variables\n",
    "â”œâ”€â”€ .envrc                          # ğŸ¬ .env loader (used by direnv)\n",
    "â”œâ”€â”€ Makefile                        # New commands \"run_train\", \"run_process\", etc..\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â”œâ”€â”€ setup.py\n",
    "â”œâ”€â”€ taxifare\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ interface\n",
    "â”‚   â”‚   â””â”€â”€ main_local.py           # ğŸšª (OLD) entry point\n",
    "â”‚   â”‚   â””â”€â”€ main.py                 # ğŸšª (NEW) entry point: No more chunk!\n",
    "â”‚   â”œâ”€â”€ ml_logic\n",
    "â”‚       â”œâ”€â”€ data.py                 # (UPDATED) Loading and storing data from/to Big Query !\n",
    "â”‚       â”œâ”€â”€ registry.py             # (UPDATED) Loading and storing model weights from/to Cloud Storage!\n",
    "â”‚       â”œâ”€â”€ ...\n",
    "â”‚   â”œâ”€â”€ params.py                   # Simply load all .env variables into python objects\n",
    "â”‚   â””â”€â”€ utils.py\n",
    "â””â”€â”€ tests\n",
    "3ï¸âƒ£ Cloud Platform\n",
    "Google Cloud Platform\n",
    "\n",
    "ğŸ‘‰ On-demand computing resources\n",
    "ğŸ‘‰ Resources are elastic (scale up and down)\n",
    "Service layers\n",
    "\n",
    "On-premise\n",
    "Computer with a virtualization layer\n",
    "Manage the location, the hardware, the operating system, and the code environment\n",
    "IaaS: infrastructure as a service (Google Compute Engine)\n",
    "Virtual machine in the cloud\n",
    "Choose the location and the hardware, pay for what you allocate\n",
    "Manage the operating system ğŸŒ and the environment ğŸŒ´\n",
    "PaaS: platform as a service (Cloud Run)\n",
    "Choose the environment for your code, pay for what you use\n",
    "Manage the package ğŸ“¦\n",
    "SaaS: software as a service (Google Big Query)\n",
    "Choose the software ğŸ\n",
    "Platform\n",
    "\n",
    "Google Cloud Platform\n",
    "A product for everything\n",
    "User-friendly\n",
    "Fast learning curve\n",
    "20% cheaper than Microsoft Azure\n",
    "\n",
    "Cloud provider\tStorage\tDatabase\tCompute\tProducts\n",
    "Amazon\tS3 (Simple Storage Service)\tAthena/Redshift/Redshift Spectrum\tEC2 (Elastic Compute Cloud)\tAWS Cloud products\n",
    "Microsoft\tAzure Blob Storage\tAzure Synapse Analytics\tAzure Virtual Machines\tAzure products\n",
    "Google\tCloud Storage\tBig Query\tCompute Engine\tGoogle Cloud products\n",
    "\n",
    "AWS vs Azure vs GCP product comparison\n",
    "Interface Rule of Thumb\n",
    "\n",
    "ğŸŒ web console\n",
    "Great for exploration, but quite slow\n",
    "For non-repetitive and precise operations\n",
    "ğŸ’» CLI (gcloud, gsutil, bq)\n",
    "Steep learning curve but faster\n",
    "Great for precise operations\n",
    "ğŸ§¬ code\n",
    "For the behaviors that need to be automated\n",
    "Livecode ğŸš§\n",
    "\n",
    "ğŸ¯ Checkout the webconsole and intoduce gcp and the services for today\n",
    "ğŸ’» Show GOOGLE_APPLICATION_CREDENTIALS that we setup on the very first day!\n",
    "ğŸ’» Show how to authenticate code and the cli\n",
    "\n",
    "\n",
    "Projects\n",
    "\n",
    "Projects: organize GCP resources\n",
    "Organizational nodes & folders: organize projects\n",
    "Regions and zones: deployment areas for resources\n",
    "\n",
    "CLI\n",
    "gcloud projects list                    # list projects\n",
    "\n",
    "gcloud compute regions list             # list compute regions\n",
    "gcloud compute zones list               # list compute zones\n",
    "Accounts\n",
    "\n",
    "Accounts: user identifier\n",
    "Service account: application identifier\n",
    "IAM: identity and access management for account organization, groups, roles & audits\n",
    "\n",
    "CLI\n",
    "gcloud init                             # setup CLI authentication\n",
    "\n",
    "gcloud config configurations list       # list CLI configurations\n",
    "\n",
    "gcloud auth application-default login   # setup code authentication\n",
    "\n",
    "gcloud services list --enabled          # list enabled service APIs\n",
    "\n",
    "More commands in the Google Cloud Platform and Service Account cheatsheets\n",
    "Budget Alert ğŸš§\n",
    "\n",
    "ğŸ¯ Setup a budget alert to follow your resource spendings on Google Cloud Platform\n",
    "Shortcuts\n",
    "\n",
    "Shortcuts\n",
    "âŒ Ctrl + C: cancel / stop a running command\n",
    "âœï¸ Ctrl + U: erase current line\n",
    "â¬…ï¸ Ctrl + A: move the cursor to the beginning of the line\n",
    "â¡ï¸ Ctrl + E: move the cursor to the end of the line\n",
    "â© Opt/Alt (orCtrlon WSL) + â¬…ï¸/â¡ï¸: move the cursor a word to the left / right\n",
    "4ï¸âƒ£ Application Parameters\n",
    "Livecode ğŸš§\n",
    "\n",
    "ğŸ¯ Change the behavior of the package ğŸ“¦ depending on the execution context:\n",
    "between collaborators\n",
    "local vs cloud\n",
    "Setup a .env file\n",
    "\n",
    "ğŸ’» Create a .env file\n",
    "Add a MODEL_TARGET variable with value local\n",
    "\n",
    "ğŸ’» Load the .env variables\n",
    "Create a .envrc file with the content dotenv\n",
    "Load the variables into the environment with direnv allow .\n",
    "Environment variable demonstration\n",
    "\n",
    "ğŸ’» Use the value in taxifare\n",
    "Change MODEL_TARGET in params.py to load the environment variable\n",
    "At the end of registry.py add a __main__ block\n",
    "Show how we can alter the flow of python using our .env and params.py\n",
    "Shift MODEL_TARGET between local and gcs\n",
    "\n",
    "ğŸ•µï¸â€â™€ï¸ Checkout how we utilize MODEL_TARGET in registry.py\n",
    "Well done! ğŸ‰\n",
    "\n",
    "The package ğŸ“¦ now adapts its behavior to its execution context, thanks to the .env file\n",
    "Theory ğŸ”­\n",
    "ğŸ“– What we saw:\n",
    ".env file\n",
    "direnv loader\n",
    "Code refactoring with flow control\n",
    "ğŸ‘‰ Separate configuration from code\n",
    ".env Project Configuration File\n",
    "\n",
    "Defines application properties that vary depending on the execution context\n",
    "\n",
    "ğŸ§­ Define behaviors\n",
    "Change the model target or data source type\n",
    "âš™ï¸ Store resources\n",
    "Local file paths, database URIs (e.g. postgresql://user:pwd@hostname:port/db)\n",
    "ğŸ”‘ Store credentials\n",
    "Account credentials, API tokens, application secrets\n",
    "\n",
    "ğŸš¨   ğŸš¨   ğŸš¨   Do NOT track this file with Git (check your .gitignore)   ğŸš¨   ğŸš¨   ğŸš¨\n",
    "direnv Configuration Loader\n",
    "\n",
    "Exposes the .env variables in the code, in the command line or in a Makefile as environment variables\n",
    "Install the direnv command line script\n",
    "\n",
    "Create a .envrc loader file next to the .env file:\n",
    "dotenv\n",
    "\n",
    "Allow direnv to load the .envrc file:\n",
    "direnv allow .                # path to the `.env` file\n",
    "\n",
    "ğŸ‘‰ direnv reload direnv status\n",
    "Environment Variables\n",
    "\n",
    "In the code (we do this all in params.py to simplify code elsewhere!):\n",
    "import os\n",
    "\n",
    "data_source = os.environ.get(\"DATA_SOURCE\")  # if a default value is ok\n",
    "data_source = os.environ[\"DATA_SOURCE\"]      # to fail when the conf is missing\n",
    "\n",
    "In the command line:\n",
    "echo $DATA_SOURCE\n",
    "\n",
    "In a Makefile:\n",
    "print_data_source:\n",
    "    echo $DATA_SOURCE                   # âŒ\n",
    "    echo $DATA_SOURCE/data.csv          # âŒ\n",
    "    echo ${DATA_SOURCE}                 # âœ…\n",
    "    echo ${DATA_SOURCE}/data.csv        # âœ…\n",
    "5ï¸âƒ£ Model in the Cloud\n",
    "\n",
    "\n",
    "Google Cloud Storage\n",
    "\n",
    "ğŸ‘‰ Static storage for unstructured data\n",
    "Livecode ğŸš§\n",
    "\n",
    "ğŸ¯ Push the trained model to the cloud on every training\n",
    "Save the Model to the Cloud\n",
    "ğŸ’» Create a bucket to save the model to!\n",
    "Create a bucket using the ui\n",
    "Upload one model manually!\n",
    "ğŸ’» Configure the .env\n",
    "Change MODEL_TARGET to gcs\n",
    "Add BUCKET_NAME to params and .env\n",
    "\n",
    "ğŸ•µï¸â€â™€ï¸ Checkout the code\n",
    "In taxifare.registry.py checkout the save_model function\n",
    "Run a training (make run_train)!\n",
    "Checkout your bucket\n",
    "\n",
    "ğŸ’» Verify that the model was uploaded\n",
    "Using the web console ğŸŒ\n",
    "Once we are confident with the web console ğŸŒ, let's speed up things with the CLI\n",
    "ğŸ’» Download the model\n",
    "Using the web console ğŸŒ\n",
    "With the CLI\n",
    "Buckets\n",
    "\n",
    "Containers for blobs, âŒ no tree structure\n",
    "Worldwide unique kebab-case naming\n",
    "\n",
    "CLI\n",
    "\n",
    "REGION=europe-west1\n",
    "PROJECT=project-id\n",
    "BUCKET=bucket-name\n",
    "\n",
    "gsutil ls                               # list buckets\n",
    "\n",
    "gsutil mb \\\n",
    "    -l $REGION \\\n",
    "    -p $PROJECT \\\n",
    "    gs://$BUCKET                        # create bucket\n",
    "Blobs\n",
    "\n",
    "Immutable data storage\n",
    "Best for unstructured data (text, images, videos, music)\n",
    "Blob URI: gs://bucket-name/blob/name (URI vs URL)\n",
    "\n",
    "CLI\n",
    "gsutil ls gs://$BUCKET             # list blobs at the root of the bucket\n",
    "gsutil ls -r gs://$BUCKET          # recursively list all blobs\n",
    "\n",
    "gsutil cp *.csv gs://$BUCKET/      # copy all CSVs in the cwd to the bucket's root\n",
    "gsutil cp gs://$BUCKET/*.csv .     # copy all CSVs at the bucket's root to the cwd\n",
    "\n",
    "More commands in the Cloud Storage cheatsheet\n",
    "CODE\n",
    "Download blob\n",
    "from google.cloud import storage\n",
    "\n",
    "BUCKET_NAME = \"my-bucket\"\n",
    "\n",
    "storage_filename = \"models/xgboost_model.joblib\"\n",
    "local_filename = \"model.joblib\"\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(storage_filename)\n",
    "blob.download_to_filename(local_filename)\n",
    "\n",
    "Upload blob\n",
    "storage_filename = \"models/random_forest_model.joblib\"\n",
    "local_filename = \"model.joblib\"\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(storage_filename)\n",
    "blob.upload_from_filename(local_filename)\n",
    "\n",
    "More features in the Cloud Storage API\n",
    "Theory ğŸ”­\n",
    "ğŸ“– What we saw:\n",
    "Buckets act as containers for our data\n",
    "Each blob stores one file\n",
    "\n",
    "6ï¸âƒ£ Data in the Cloud\n",
    "\n",
    "\n",
    "Google Big Query\n",
    "\n",
    "ğŸ‘‰ Data warehouse in the cloud\n",
    "ğŸ‘‰ Handles Petabytes worth of data\n",
    "ğŸ‘‰ Built-in ML capabilities (out of scope)\n",
    "CODE\n",
    "\n",
    "Load table\n",
    "from google.cloud import bigquery\n",
    "\n",
    "PROJECT = \"my-project\"\n",
    "DATASET = \"taxifare_dataset\"\n",
    "TABLE = \"processed_1k\"\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {PROJECT}.{DATASET}.{TABLE}\n",
    "    \"\"\"\n",
    "\n",
    "client = bigquery.Client(project=gcp_project)\n",
    "query_job = client.query(query)\n",
    "result = query_job.result()\n",
    "df = result.to_dataframe()\n",
    "\n",
    "More features in the Big Query API\n",
    "Upload dataframe\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT = \"my-project\"\n",
    "DATASET = \"taxifare_lecture\"\n",
    "TABLE = \"lecture_data\"\n",
    "\n",
    "table = f\"{PROJECT}.{DATASET}.{TABLE}\"\n",
    "\n",
    "df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "write_mode = \"WRITE_TRUNCATE\" # or \"WRITE_APPEND\"\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=write_mode)\n",
    "\n",
    "job = client.load_table_from_dataframe(df, table, job_config=job_config)\n",
    "result = job.result()\n",
    "Livecode ğŸš§\n",
    "\n",
    "ğŸ¯ Load a dataframe into to big query table\n",
    "Training from the cloud\n",
    "  ğŸ’» Upload a dataframe to BQ\n",
    "Create a dataset\n",
    "Create a new python script\n",
    "Use the upload dataframe code\n",
    "\n",
    "ğŸ’» Verify that the data is uploaded\n",
    "Let's use the the web console ğŸŒ and then the cli\n",
    "Then try changing the write_mode\n",
    "\n",
    "Theory ğŸ”­\n",
    "ğŸ“– What we saw:\n",
    "Data warehouse in the cloud\n",
    "datasets as regular databases\n",
    "tables store data\n",
    "Datasets & Tables\n",
    "\n",
    "Database in the cloud\n",
    "Best for structured/relational data (tabular)\n",
    "\n",
    "CLI\n",
    "DATASET=taxifare_lecture\n",
    "TABLE=lecture_data\n",
    "\n",
    "bq ls                              # list datasets\n",
    "bq ls $DATASET                     # list dataset tables\n",
    "\n",
    "bq show $DATASET.$TABLE            # show table format\n",
    "Imports & Queries\n",
    "\n",
    "CLI\n",
    "SOURCE=processed_1k.csv\n",
    "\n",
    "# load data to dataset table\n",
    "bq load --autodetect $DATASET.$TABLE $SOURCE\n",
    "\n",
    "Legacy SQL vs custom SQL:\n",
    "# show table's first rows\n",
    "bq query \"SELECT * FROM $DATASET.$TABLE LIMIT 5\"\n",
    "\n",
    "# show table usage\n",
    "bq query \\\n",
    "    --nouse_legacy_sql \\\n",
    "    \"SELECT * FROM $DATASET.INFORMATION_SCHEMA.PARTITIONS\"\n",
    "\n",
    "More commands in the Big Query cheatsheet\n",
    "Query Evaluation ğŸ’µ\n",
    "\n",
    "Cost per scanned data, âŒ not per returned row\n",
    "In contrast, managed cloud database services can bill dedicated instances per number of allocated CPUs and amount of memory used, or bill shared instances per uptime, amount of stored data and number of concurrent connections to the database\n",
    "Use the CLI's --dry_run flag to evaluate the cost of a query\n",
    "Use the web console ğŸŒ preview to estimate the amount of data scanned\n",
    "Only select required columns (columnar storage)\n",
    "Use partitions\n",
    "Optimize the queries on partitions\n",
    "âŒ LIMIT does not impact the pricing\n",
    "More on cost best practices\n",
    "7ï¸âƒ£ Training in the Cloud\n",
    "\n",
    "\n",
    "Livecode ğŸš§\n",
    "\n",
    "ğŸ¯ Run the package training in the cloud\n",
    "Create a Virtual Machine\n",
    "\n",
    "ğŸ’» Create a virtual machine from the web console ğŸŒ\n",
    "Overview of region, zone, hardware, price, operating system\n",
    "Select the Ubuntu operating system (boot disk section)\n",
    "Connect to the Virtual Machine\n",
    "\n",
    "ğŸ’» Connect to the virtual machine\n",
    "Start the VM\n",
    "Connect to the VM using ssh\n",
    "Explore the empty VM (no/bad python version, no git auth)\n",
    "Run the Package in the Cloud\n",
    "ğŸ’» Connect to an already configured VM\n",
    "Connect to an already setup vm\n",
    "scp across the lecture livecode directory\n",
    "ğŸ’» Train the model\n",
    "ğŸ’» Stop the virtual machine\n",
    "Theory ğŸ”­\n",
    "ğŸ“– What we saw:\n",
    "virtual machines as computers in the cloud âŒ without a screen\n",
    "VMs are switched on and off remotely\n",
    "ssh allows remote connections to a shell running in a VM, similarly to when a new tab is created in the Terminal\n",
    "Google Compute Engine\n",
    "\n",
    "ğŸ‘‰ Virtual machine in the cloud\n",
    "ğŸ‘‰ Custom hardware (GPU) and operating system (Ubuntu)\n",
    "ğŸ‘‰ Basic building block of many Google Cloud Platform products\n",
    "Virtual Machine\n",
    "\n",
    "CLI\n",
    "INSTANCE=my-instance\n",
    "\n",
    "gcloud compute instances list                     # list virtual machine's status\n",
    "\n",
    "gcloud compute instances start $INSTANCE          # start instance\n",
    "gcloud compute instances stop $INSTANCE           # stop instance\n",
    "\n",
    "ğŸš¨   ğŸš¨   ğŸš¨   Save the planet ğŸŒ±, schedule an auto shutdown for your VM   ğŸš¨   ğŸš¨   ğŸš¨\n",
    "Remote Operation\n",
    "\n",
    "Interact with a remote machine:\n",
    "ssh to run commands in a shell on the remote machine through the SSH protocol\n",
    "scp to copy files from and to the machine\n",
    "\n",
    "CLI\n",
    "# interactive ssh to remote instance\n",
    "gcloud compute ssh $INSTANCE\n",
    "\n",
    "# run remote commands on remote instance\n",
    "gcloud compute ssh $INSTANCE --command \"ls -la\"\n",
    "\n",
    "# recursively copy home directory on instance to local directory\n",
    "gcloud compute scp --recurse $INSTANCE:~/ .\n",
    "\n",
    "More commands in the Compute Engine cheatsheet\n",
    "Your turn ğŸš€\n"
   ],
   "id": "9fcae7139d2704d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3e4f0f107d639294"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d00e4d9212b1632d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "530f5036b51b73"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
